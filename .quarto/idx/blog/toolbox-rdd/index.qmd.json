{"title":"The policy evaluator's toolbox: Presenting your next regression discontinuity analysis","markdown":{"yaml":{"title":"The policy evaluator's toolbox: Presenting your next regression discontinuity analysis","author":"Paw Hansen","date":"2023-06-01","categories":["statistical analysis","policy evaluation"],"image":"featured.jpg","description":"When evaluating public policies, regression discontinuity designs are impossible to live without. Here is how to do one.","editor_options":{"chunk_output_type":"console"}},"headingText":"Plot 1: Showing the raw data","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = F, message = F)\n```\n\nIn the previous post of my [“policy evaluator’s toolbox” series](https://www.pawhansen.org/blog#category=policy%20evaluation), I showed you two basic ways of plotting results from randomized experiments. A randomized experiment should be the design you are going for whenever possible since it allows for easy estimation and clean causal inference.\n\nIn practice, though, randomized experiments may not be feasible. Often policymakers are unwilling to assign policies at random, so the question becomes what we can do in those cases if we still want to evaluate if a policy had an effect.\n\nToday, we’ll have a look at one widely-used approach: the **regression discontinuity design**. Here is the definition I got when asking ChatGPT:\n\n> “A regression discontinuity design (RDD) is a quasi-experimental research design used to estimate causal effects by taking advantage of a discontinuity in the treatment assignment process. The basic logic behind RDD is that when a certain threshold or cutoff point is used to determine whether an individual receives a treatment or not, individuals on either side of the cutoff are expected to be similar in all other relevant aspects, except for the treatment itself. By comparing the outcomes of individuals just above and just below the cutoff, researchers can attribute any differences to the treatment rather than other confounding factors.”\n\nThis is not a bad definition but I do want to emphasize one core assumptions: the assignment variable should be _continuous_ or \"smooth\" around the cutoff. \n\nIn my field of public welfare, many such cutoff variables exist:\n\n* *Age*. For example, everybody younger than 21 years old might receive free dental care\n* *Income*. For example, everybody earning above some level might lose eligibility for housing benefits\n* *Time*. For example, a new tutoring program may be implemented on January 1st making it possible to estimate its effects by comparing units just before and just after the cutoff date \n\nFinally, a popular threshold is *test scores*, which we will work with today. Specifically, we’ll use the `Chile Schools` data taken from (the highly recommendable) _Regression and Other Stories_. Let’s get our packages and load up the data:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Packages used in this post\"\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggsci)\nlibrary(readr)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\nchile <- read_csv(\"chile.csv\")\n\n```\n\nThe `Chile Schools` data is a good example of something you will often encounter when doing policy evaluations. In 1992, the Chilean government wanted to improve poor-performing schools. To do so, the government assigned extra resources (teacher training and after-school tutoring) to all schools scoring lower than a predefined cutoff on the average fourth-grade test score. This assignment is clearly not random. In fact, we can be quite certain that the schools that received the extra resources are different from those that did not (that is why those schools received extra help!). \n\nHowever, as we move closer to the cutoff, it seems more and more likely that the schools falling on either side are not _that_ different. For example, say the cutoff was 60 points on a 100-point scale. Two schools with an average score of 30 and 70 are clearly different, and it makes no sense to compare those. But what about two schools scoring 58 and 62? Do we really believe that those schools differ fundamentally from each other? The key assumption we make in a regression discontinuity design is that the units we look at on either side of the cutoff are as-if random.\n\n\nA good place to start is to show the raw data on either side of the cutoff. In the @fig-peak, all units above the threshold (scaled to zero) received extra help, while those below did not.\n\n```{r}\n#| label: fig-peak\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. The assignment variable is centered reading test scores in 1988. Having a score less than 0 meant that schools would be included in the program.\"\n\n# Make a variable saying if schools received the treatment or not\nchile <-\n  chile %>% \n  mutate(condition = ifelse(rule2 < 0, \"Treatment\", \"Control\")) \n\n\nchile %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n```\n\nHowever, since we wanted schools to be \"alike\" (in terms of potential outcomes), we should probably focus the graph on those units that are close to the threshold. Let's define a cutoff range from -5 through 5 and make a new plot (@fig-cutoff):\n\n```{r}\n#| label: fig-cutoff\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. In this version of the graph, only schools close to the cutoff (-5 through 5) are plotted.\"\n\nchile_sub <- \n  chile %>% \n  filter(rule2 < 5, rule2 > -5) \n\nchile_sub %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  geom_point(aes(color = condition)) + \n scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n```\n\nAnd we can even make this plot a bit clearer if we use binned averages instead of showing all data points:\n\n```{r}\n#| label: fig-better\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. In this version of the graph, schools are plotted using binned averages on the assignment variable.\"\n\nchile_sub %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  stat_summary_bin(aes(color = condition), \n                   fun = \"mean\", \n                   size = 2, \n                   geom = \"point\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n```\n\n### Plot 2: Adding the model \n\nShowing the data (to others but also yourself) is always a good place to start and a key part of of [exploratory data analysis](https://r4ds.had.co.nz/exploratory-data-analysis.html). However, to answer the question of interest (did assigning extra help to schools improve reading?), we need a model. \n\nLuckily, the modeling part of a regression discontinuity analysis is straightforward: We simply include the condition and the assignment variable as predictors in our regression model: \n\n```{r}\n#| label: tbl-effect\n#| tbl-cap: \"Effect of being in policy program\"\n\nchile_rs <- \n  lm(read92 ~ condition + rule2, data = chile_sub) \n\nchile_rs %>% \n  tidy(conf.int=T,) %>% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %>% \n  mutate(term = c(\"Intercept\", \"In program\", \"Assignment variable\")) %>% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      digits = 2)\n\n```\n\nThus, the estimated effect of being in the program is about `r round(coef(chile_rs)[2], 2)` points. \n\nTo understand this result better, and to present it in an intuitive way to stakeholders, we can plot the model on either side of the cutoff (i.e. the regression line):\n\n```{r}\n#| label: fig-final\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. In this version of the graph, the fitted model on either side of the threshold is overlaid on the raw data.\"\n\nchile_sub %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  geom_smooth(\n    data = chile_sub %>%  filter(rule2 < 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  geom_smooth(\n    data = chile_sub %>%  filter(rule2 > 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  annotate(geom=\"text\", \n           x  = -2.5, \n           y = 33, \n           color = \"#DF8F44FF\", \n           label = \"Included in program\") + \n    annotate(geom=\"text\", \n           x  = 2.5, \n           y = 33, \n           color = \"#374E55FF\", \n           label = \"Not included in program\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n```\n\n\nThe estimated effect of `r round(coef(chile_rs)[2], 2)` points corresponds to the difference (the \"kink\") between the two lines.\n\n### Building the final model\n\nFinally, in most real-world applications, it makes sense to include pre-treatment variables that strongly predict the outcome since this can make our model estimates more precise. In our example, we have schoolchildren’s math and reading scores in 1988, which is probably useful when trying to predict reading scores in 1992. Let’s include those:\n\n```{r}\n#| label: tbl-final-model\n#| tbl-cap: \"Effect of being in policy program, covariate adjusted\"\n#| \nchile_rs_final <- \n  lm(read92 ~ condition + rule2 + read88 + math88, data = chile_sub) \n\nchile_rs_final %>% \n  tidy(conf.int=T,) %>% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %>% \n  mutate(term = c(\"Intercept\", \n                  \"In program\", \n                  \"Assignment variable\",\n                  \"Reading score in 1988\",\n                  \"Math score in 1988\")) %>% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      digits = 2) \n\n```\n\nAs can be seen, our confidence intervals are now slightly narrower, reflecting the extra information we have included in our analysis. \n\n### Final thoughts: plotting your next regression discontinuity analysis\nRegression discontinuity designs (RDD) are a popular way of evaluating policies when randomization b y the researcher is not an option. When doing RDD, we look for naturally occurring cutoffs that split units into a treatment and a control group. \n\nCutoff variables should be continuous. Finally, be aware that RDD is _quasi_-experimental. It is _not_ a randomized experiment, and it is on _you_ to prove that the assumptions hold.    \n\n### Cool! Where can I learn more?\n\n* Gelman, A., Hill, J., & Vehtari, A. (2020). _Regression and other stories_. Cambridge University Press.\n* Cunningham, S. (2021). _Causal inference: The mixtape_. Yale university press. \n \n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = F, message = F)\n```\n\nIn the previous post of my [“policy evaluator’s toolbox” series](https://www.pawhansen.org/blog#category=policy%20evaluation), I showed you two basic ways of plotting results from randomized experiments. A randomized experiment should be the design you are going for whenever possible since it allows for easy estimation and clean causal inference.\n\nIn practice, though, randomized experiments may not be feasible. Often policymakers are unwilling to assign policies at random, so the question becomes what we can do in those cases if we still want to evaluate if a policy had an effect.\n\nToday, we’ll have a look at one widely-used approach: the **regression discontinuity design**. Here is the definition I got when asking ChatGPT:\n\n> “A regression discontinuity design (RDD) is a quasi-experimental research design used to estimate causal effects by taking advantage of a discontinuity in the treatment assignment process. The basic logic behind RDD is that when a certain threshold or cutoff point is used to determine whether an individual receives a treatment or not, individuals on either side of the cutoff are expected to be similar in all other relevant aspects, except for the treatment itself. By comparing the outcomes of individuals just above and just below the cutoff, researchers can attribute any differences to the treatment rather than other confounding factors.”\n\nThis is not a bad definition but I do want to emphasize one core assumptions: the assignment variable should be _continuous_ or \"smooth\" around the cutoff. \n\nIn my field of public welfare, many such cutoff variables exist:\n\n* *Age*. For example, everybody younger than 21 years old might receive free dental care\n* *Income*. For example, everybody earning above some level might lose eligibility for housing benefits\n* *Time*. For example, a new tutoring program may be implemented on January 1st making it possible to estimate its effects by comparing units just before and just after the cutoff date \n\nFinally, a popular threshold is *test scores*, which we will work with today. Specifically, we’ll use the `Chile Schools` data taken from (the highly recommendable) _Regression and Other Stories_. Let’s get our packages and load up the data:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Packages used in this post\"\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggsci)\nlibrary(readr)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\nchile <- read_csv(\"chile.csv\")\n\n```\n\nThe `Chile Schools` data is a good example of something you will often encounter when doing policy evaluations. In 1992, the Chilean government wanted to improve poor-performing schools. To do so, the government assigned extra resources (teacher training and after-school tutoring) to all schools scoring lower than a predefined cutoff on the average fourth-grade test score. This assignment is clearly not random. In fact, we can be quite certain that the schools that received the extra resources are different from those that did not (that is why those schools received extra help!). \n\nHowever, as we move closer to the cutoff, it seems more and more likely that the schools falling on either side are not _that_ different. For example, say the cutoff was 60 points on a 100-point scale. Two schools with an average score of 30 and 70 are clearly different, and it makes no sense to compare those. But what about two schools scoring 58 and 62? Do we really believe that those schools differ fundamentally from each other? The key assumption we make in a regression discontinuity design is that the units we look at on either side of the cutoff are as-if random.\n\n### Plot 1: Showing the raw data\n\nA good place to start is to show the raw data on either side of the cutoff. In the @fig-peak, all units above the threshold (scaled to zero) received extra help, while those below did not.\n\n```{r}\n#| label: fig-peak\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. The assignment variable is centered reading test scores in 1988. Having a score less than 0 meant that schools would be included in the program.\"\n\n# Make a variable saying if schools received the treatment or not\nchile <-\n  chile %>% \n  mutate(condition = ifelse(rule2 < 0, \"Treatment\", \"Control\")) \n\n\nchile %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n```\n\nHowever, since we wanted schools to be \"alike\" (in terms of potential outcomes), we should probably focus the graph on those units that are close to the threshold. Let's define a cutoff range from -5 through 5 and make a new plot (@fig-cutoff):\n\n```{r}\n#| label: fig-cutoff\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. In this version of the graph, only schools close to the cutoff (-5 through 5) are plotted.\"\n\nchile_sub <- \n  chile %>% \n  filter(rule2 < 5, rule2 > -5) \n\nchile_sub %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  geom_point(aes(color = condition)) + \n scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n```\n\nAnd we can even make this plot a bit clearer if we use binned averages instead of showing all data points:\n\n```{r}\n#| label: fig-better\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. In this version of the graph, schools are plotted using binned averages on the assignment variable.\"\n\nchile_sub %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  stat_summary_bin(aes(color = condition), \n                   fun = \"mean\", \n                   size = 2, \n                   geom = \"point\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n```\n\n### Plot 2: Adding the model \n\nShowing the data (to others but also yourself) is always a good place to start and a key part of of [exploratory data analysis](https://r4ds.had.co.nz/exploratory-data-analysis.html). However, to answer the question of interest (did assigning extra help to schools improve reading?), we need a model. \n\nLuckily, the modeling part of a regression discontinuity analysis is straightforward: We simply include the condition and the assignment variable as predictors in our regression model: \n\n```{r}\n#| label: tbl-effect\n#| tbl-cap: \"Effect of being in policy program\"\n\nchile_rs <- \n  lm(read92 ~ condition + rule2, data = chile_sub) \n\nchile_rs %>% \n  tidy(conf.int=T,) %>% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %>% \n  mutate(term = c(\"Intercept\", \"In program\", \"Assignment variable\")) %>% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      digits = 2)\n\n```\n\nThus, the estimated effect of being in the program is about `r round(coef(chile_rs)[2], 2)` points. \n\nTo understand this result better, and to present it in an intuitive way to stakeholders, we can plot the model on either side of the cutoff (i.e. the regression line):\n\n```{r}\n#| label: fig-final\n#| fig-cap: \"Reading test scores in 1992 by assignment variable. In this version of the graph, the fitted model on either side of the threshold is overlaid on the raw data.\"\n\nchile_sub %>% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  geom_smooth(\n    data = chile_sub %>%  filter(rule2 < 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  geom_smooth(\n    data = chile_sub %>%  filter(rule2 > 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  annotate(geom=\"text\", \n           x  = -2.5, \n           y = 33, \n           color = \"#DF8F44FF\", \n           label = \"Included in program\") + \n    annotate(geom=\"text\", \n           x  = 2.5, \n           y = 33, \n           color = \"#374E55FF\", \n           label = \"Not included in program\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n```\n\n\nThe estimated effect of `r round(coef(chile_rs)[2], 2)` points corresponds to the difference (the \"kink\") between the two lines.\n\n### Building the final model\n\nFinally, in most real-world applications, it makes sense to include pre-treatment variables that strongly predict the outcome since this can make our model estimates more precise. In our example, we have schoolchildren’s math and reading scores in 1988, which is probably useful when trying to predict reading scores in 1992. Let’s include those:\n\n```{r}\n#| label: tbl-final-model\n#| tbl-cap: \"Effect of being in policy program, covariate adjusted\"\n#| \nchile_rs_final <- \n  lm(read92 ~ condition + rule2 + read88 + math88, data = chile_sub) \n\nchile_rs_final %>% \n  tidy(conf.int=T,) %>% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %>% \n  mutate(term = c(\"Intercept\", \n                  \"In program\", \n                  \"Assignment variable\",\n                  \"Reading score in 1988\",\n                  \"Math score in 1988\")) %>% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      digits = 2) \n\n```\n\nAs can be seen, our confidence intervals are now slightly narrower, reflecting the extra information we have included in our analysis. \n\n### Final thoughts: plotting your next regression discontinuity analysis\nRegression discontinuity designs (RDD) are a popular way of evaluating policies when randomization b y the researcher is not an option. When doing RDD, we look for naturally occurring cutoffs that split units into a treatment and a control group. \n\nCutoff variables should be continuous. Finally, be aware that RDD is _quasi_-experimental. It is _not_ a randomized experiment, and it is on _you_ to prove that the assumptions hold.    \n\n### Cool! Where can I learn more?\n\n* Gelman, A., Hill, J., & Vehtari, A. (2020). _Regression and other stories_. Cambridge University Press.\n* Cunningham, S. (2021). _Causal inference: The mixtape_. Yale university press. \n \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","code-summary":"Code","theme":{"light":["litera","../../custom.css"],"dark":["slate","custom.scss"]},"smooth-scroll":true,"title-block-banner":true,"title":"The policy evaluator's toolbox: Presenting your next regression discontinuity analysis","author":"Paw Hansen","date":"2023-06-01","categories":["statistical analysis","policy evaluation"],"image":"featured.jpg","description":"When evaluating public policies, regression discontinuity designs are impossible to live without. Here is how to do one.","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}