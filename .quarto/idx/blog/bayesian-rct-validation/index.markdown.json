{"title":"Bayesian analysis of a randomized controlled trial II: Defining and validating the model","markdown":{"yaml":{"title":"Bayesian analysis of a randomized controlled trial II: Defining and validating the model","author":"","date":"2023-07-02","slug":[],"categories":["statistical analysis"],"tags":["Bayesian analysis"],"subtitle":"Part two of my three-part series on Bayesian analysis of randomized controlled trials. You've built a model but is it any good?","excerpt":"Part two of my three-part series on Bayesian analysis of randomized controlled trials. You've built a model but is it any good?","draft":"no","series":null,"layout":"single","editor_options":{"chunk_output_type":"console"}},"headingText":"MCMC trace, density, & autocorrelation plots - weakly informative prior","containsRefs":false,"markdown":"\n<script src=\"{{< blogdown/postref >}}index_files/kePrint/kePrint.js\"></script>\n<link href=\"{{< blogdown/postref >}}index_files/lightable/lightable.css\" rel=\"stylesheet\" />\n\n\n\nThe aim of this post is to _validate_ our models. Basically, we'll be asking two questions:\n\n1. How well does the model fit the data?\n2. How well does the model fit _new_ data?\n\nFor the first question, we will do posterior predictive simulation to see if data simulated from the model has features similar to the original data (which was used to fit the model). \n\nFor the second question, we will \n\nThe packages we'll be using: \n\n\n```r\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesrules)\nlibrary(tidybayes)\nlibrary(bayesplot)\nlibrary(kableExtra)\nlibrary(patchwork)\n\ntheme_set(theme_minimal(base_size = 12))\n```\n\n\n\nRecall our two models from my previous post:\n\n* In one model, I used a _weakly informative prior_ \n* In another, model I used an _evidence-based prior_\n\n\nBefore moving on, we should check the stability of our simulations. \n\nFor the model using a weakly informative prior...\n\n\n```r\np1 <- mcmc_trace(fail_model_weakinf) + scale_x_continuous(breaks = c(0, 5000))\n\np2 <- mcmc_dens_overlay(fail_model_weakinf)\np3 <- mcmc_acf(fail_model_weakinf)\n\np1 / p2 / p3 + plot_annotation(tag_levels = 'A')\n```\n\n<div class=\"figure\">\n<img src=\"{{< blogdown/postref >}}index_files/figure-html/plot-diag-weak-1.png\" alt=\"Diagnostic plots for the stability of our simulation results concerning the model using weakly informative priors. All looks good.\" width=\"672\" />\n<p class=\"caption\"><span id=\"fig:plot-diag-weak\"></span>Figure 1: Diagnostic plots for the stability of our simulation results concerning the model using weakly informative priors. All looks good.</p>\n</div>\n\nAll looks good. For the evidence based model...\n\n\n```r\n# MCMC trace, density, & autocorrelation plots - evidence based model\np4 <- mcmc_trace(fail_model_evidence) + scale_x_continuous(breaks = c(0, 5000))\np5 <- mcmc_dens_overlay(fail_model_evidence)\np6 <- mcmc_acf(fail_model_evidence)\n\np1 / p2 / p3 + plot_annotation(tag_levels = 'A')\n```\n\n<div class=\"figure\">\n<img src=\"{{< blogdown/postref >}}index_files/figure-html/plot-diag-evidence-1.png\" alt=\"Diagnostic plots for the stability of our simulation results concerning the evidence-based model. All looks good.\" width=\"672\" />\n<p class=\"caption\"><span id=\"fig:plot-diag-evidence\"></span>Figure 2: Diagnostic plots for the stability of our simulation results concerning the evidence-based model. All looks good.</p>\n</div>\n\nsame. \n\nAll set, let's get on with model validation!\n\n### Check #1: How well does the model fit the data?\n\nTo see this, we simulate 100 data sets from our posterior distribution. For each data set, we then calculate the number of failed tests to see if this matches up with that of the original data.  \n\n\n```r\ncalc_prop_fail <- function(x) {mean(x == 1)\n}\n```\n\n\n```r\npp_check_model_weakinf <- \n  pp_check(fail_model_weakinf, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_evidence <-\n  pp_check(fail_model_evidence, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_weakinf / pp_check_model_evidence + plot_annotation(tag_levels = 'A')\n```\n\n<div class=\"figure\">\n<img src=\"{{< blogdown/postref >}}index_files/figure-html/pp-checks-1.png\" alt=\"Posterior predictve checks of the two models. Histograms show the proportion of failed students in a number of simulated datasets based on each model, whereas the dark blue lines mark the real proportion of failed students in the data. (a) With a weakly informative prior, the simulated data sets are unwilling to say much about the share of failed reading tests. (b) In contrast, using an evidence-based prior results in posterior simulations that resemble the original data well.\" width=\"672\" />\n<p class=\"caption\"><span id=\"fig:pp-checks\"></span>Figure 3: Posterior predictve checks of the two models. Histograms show the proportion of failed students in a number of simulated datasets based on each model, whereas the dark blue lines mark the real proportion of failed students in the data. (a) With a weakly informative prior, the simulated data sets are unwilling to say much about the share of failed reading tests. (b) In contrast, using an evidence-based prior results in posterior simulations that resemble the original data well.</p>\n</div>\n\n### Check #2: How well does the model fit _new_ data?\n\nBecause we are working with a categorical outcome, we can be _either_ right or wrong. The question is how often are we right? \n\n\n```r\nset.seed(0407)\n\nclass_sum_weakinf <- \n  classification_summary(model = fail_model_weakinf, data = fake, cutoff = 0.5) \n\nclass_sum_evidence <- \n  classification_summary(model = fail_model_evidence, data = fake, cutoff = 0.5) \n```\n\n* **overall accuracy** captures the proportion of all Y observations that are accurately classified\n* **sensitivity (true positive rate)** captures the proportion of Y = 1 observations that are accurately classified \n* **specificity (true negative rate)** the proportion of Y = 0 observations that are accurately classified:\n\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption><span id=\"tab:tab-perf-measures\"></span>Table 1: How well do the two models fit the data?</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Measure </th>\n   <th style=\"text-align:right;\"> Weakly informative priors </th>\n   <th style=\"text-align:right;\"> Evidence-based priors </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Sensitivity (true positive rate) </td>\n   <td style=\"text-align:right;\"> 0.57 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Specificity (true negative rate) </td>\n   <td style=\"text-align:right;\"> 0.43 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Overall accuracy </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n</tbody>\n</table>\n\n","srcMarkdownNoYaml":"\n<script src=\"{{< blogdown/postref >}}index_files/kePrint/kePrint.js\"></script>\n<link href=\"{{< blogdown/postref >}}index_files/lightable/lightable.css\" rel=\"stylesheet\" />\n\n\n\nThe aim of this post is to _validate_ our models. Basically, we'll be asking two questions:\n\n1. How well does the model fit the data?\n2. How well does the model fit _new_ data?\n\nFor the first question, we will do posterior predictive simulation to see if data simulated from the model has features similar to the original data (which was used to fit the model). \n\nFor the second question, we will \n\nThe packages we'll be using: \n\n\n```r\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesrules)\nlibrary(tidybayes)\nlibrary(bayesplot)\nlibrary(kableExtra)\nlibrary(patchwork)\n\ntheme_set(theme_minimal(base_size = 12))\n```\n\n\n\nRecall our two models from my previous post:\n\n* In one model, I used a _weakly informative prior_ \n* In another, model I used an _evidence-based prior_\n\n\nBefore moving on, we should check the stability of our simulations. \n\nFor the model using a weakly informative prior...\n\n\n```r\n# MCMC trace, density, & autocorrelation plots - weakly informative prior \np1 <- mcmc_trace(fail_model_weakinf) + scale_x_continuous(breaks = c(0, 5000))\n\np2 <- mcmc_dens_overlay(fail_model_weakinf)\np3 <- mcmc_acf(fail_model_weakinf)\n\np1 / p2 / p3 + plot_annotation(tag_levels = 'A')\n```\n\n<div class=\"figure\">\n<img src=\"{{< blogdown/postref >}}index_files/figure-html/plot-diag-weak-1.png\" alt=\"Diagnostic plots for the stability of our simulation results concerning the model using weakly informative priors. All looks good.\" width=\"672\" />\n<p class=\"caption\"><span id=\"fig:plot-diag-weak\"></span>Figure 1: Diagnostic plots for the stability of our simulation results concerning the model using weakly informative priors. All looks good.</p>\n</div>\n\nAll looks good. For the evidence based model...\n\n\n```r\n# MCMC trace, density, & autocorrelation plots - evidence based model\np4 <- mcmc_trace(fail_model_evidence) + scale_x_continuous(breaks = c(0, 5000))\np5 <- mcmc_dens_overlay(fail_model_evidence)\np6 <- mcmc_acf(fail_model_evidence)\n\np1 / p2 / p3 + plot_annotation(tag_levels = 'A')\n```\n\n<div class=\"figure\">\n<img src=\"{{< blogdown/postref >}}index_files/figure-html/plot-diag-evidence-1.png\" alt=\"Diagnostic plots for the stability of our simulation results concerning the evidence-based model. All looks good.\" width=\"672\" />\n<p class=\"caption\"><span id=\"fig:plot-diag-evidence\"></span>Figure 2: Diagnostic plots for the stability of our simulation results concerning the evidence-based model. All looks good.</p>\n</div>\n\nsame. \n\nAll set, let's get on with model validation!\n\n### Check #1: How well does the model fit the data?\n\nTo see this, we simulate 100 data sets from our posterior distribution. For each data set, we then calculate the number of failed tests to see if this matches up with that of the original data.  \n\n\n```r\ncalc_prop_fail <- function(x) {mean(x == 1)\n}\n```\n\n\n```r\npp_check_model_weakinf <- \n  pp_check(fail_model_weakinf, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_evidence <-\n  pp_check(fail_model_evidence, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_weakinf / pp_check_model_evidence + plot_annotation(tag_levels = 'A')\n```\n\n<div class=\"figure\">\n<img src=\"{{< blogdown/postref >}}index_files/figure-html/pp-checks-1.png\" alt=\"Posterior predictve checks of the two models. Histograms show the proportion of failed students in a number of simulated datasets based on each model, whereas the dark blue lines mark the real proportion of failed students in the data. (a) With a weakly informative prior, the simulated data sets are unwilling to say much about the share of failed reading tests. (b) In contrast, using an evidence-based prior results in posterior simulations that resemble the original data well.\" width=\"672\" />\n<p class=\"caption\"><span id=\"fig:pp-checks\"></span>Figure 3: Posterior predictve checks of the two models. Histograms show the proportion of failed students in a number of simulated datasets based on each model, whereas the dark blue lines mark the real proportion of failed students in the data. (a) With a weakly informative prior, the simulated data sets are unwilling to say much about the share of failed reading tests. (b) In contrast, using an evidence-based prior results in posterior simulations that resemble the original data well.</p>\n</div>\n\n### Check #2: How well does the model fit _new_ data?\n\nBecause we are working with a categorical outcome, we can be _either_ right or wrong. The question is how often are we right? \n\n\n```r\nset.seed(0407)\n\nclass_sum_weakinf <- \n  classification_summary(model = fail_model_weakinf, data = fake, cutoff = 0.5) \n\nclass_sum_evidence <- \n  classification_summary(model = fail_model_evidence, data = fake, cutoff = 0.5) \n```\n\n* **overall accuracy** captures the proportion of all Y observations that are accurately classified\n* **sensitivity (true positive rate)** captures the proportion of Y = 1 observations that are accurately classified \n* **specificity (true negative rate)** the proportion of Y = 0 observations that are accurately classified:\n\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption><span id=\"tab:tab-perf-measures\"></span>Table 1: How well do the two models fit the data?</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Measure </th>\n   <th style=\"text-align:right;\"> Weakly informative priors </th>\n   <th style=\"text-align:right;\"> Evidence-based priors </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Sensitivity (true positive rate) </td>\n   <td style=\"text-align:right;\"> 0.57 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Specificity (true negative rate) </td>\n   <td style=\"text-align:right;\"> 0.43 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Overall accuracy </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n</tbody>\n</table>\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","theme":{"light":["litera","../../custom.css"],"dark":["slate","custom.scss"]},"smooth-scroll":true,"title-block-banner":true,"title":"Bayesian analysis of a randomized controlled trial II: Defining and validating the model","author":"","date":"2023-07-02","slug":[],"categories":["statistical analysis"],"tags":["Bayesian analysis"],"subtitle":"Part two of my three-part series on Bayesian analysis of randomized controlled trials. You've built a model but is it any good?","excerpt":"Part two of my three-part series on Bayesian analysis of randomized controlled trials. You've built a model but is it any good?","draft":"no","series":null,"layout":"single","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}