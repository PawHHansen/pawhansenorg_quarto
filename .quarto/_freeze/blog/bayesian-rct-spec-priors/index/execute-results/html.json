{
  "hash": "edc84d1d651be6bc5303727f0d0d20c9",
  "result": {
    "markdown": "---\ntitle: 'Bayesian analysis of a randomized controlled trial I: Specifying the priors'\nauthor: \"Paw Hansen\"\ndate: '2023-07-02'\ndescription: 'Kicking off my three-part series on doing Bayesian analysis of randomized controlled trials. First up: specifying your priors.'\nimage: \"featured.jpg\"\ncategories: [Bayesian modeling]\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nWelcome to the first part of my three-part series on how to analyze randomized controlled trials (RCTs) using Bayesian thinking and methods. For this first post, I assume you have some basic understanding about what it means to be \"working like a Bayesian\". If you are entirely new to Bayesian methods, check out the references at the bottom of the post.\n\nThe packages we'll be using today:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(modelsummary)\nlibrary(kableExtra)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(janitor)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### Simulation: A hypothetical experiment on dyslexia\n\nTo get us going, let's begin by simulating some fake data to analyse. The code below simulates data for at hypothetical experiment in which students were randomly divided into either a treatment or control group. For context, let us imagine that the treatment is an intensive tutoring program. The outcome is dichotomous: did the students fail a reading test after the program ended or not? In our case, we'll imagine that failing the test indicates dyslexia. [^1]\n\n[^1]: Notice that although this is well-suited for our purpose, it actually represents a sub-optimal research design because we have discretized a variable for which it is fair to assume we would have a continuous measure, for example a test score for each student. By working with a dichotomous outcome, we are throwing away valuable information. Table \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed for reproducibility\nset.seed(1407)\n\n# Number of participants in each group\nn_control <- 450\nn_treat <- 450\n\n# True probabilities of failing the test for each group\np_control <- 0.55\np_treatment <- 0.45\n\n# Simulate data\nfake <- tibble(\n  condition = rep(c(\"Control\", \"Treatment\"), \n                  c(n_control, n_treat))) |> \n  mutate(fail_test = ifelse(condition == \"Control\", \n                     rbinom(n_control, 1, p_control), \n                     rbinom(n_treat, 1, p_treatment))) \n```\n:::\n\n::: {#tbl-head-fake .cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Peak at the first five rows in the data</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> condition </th>\n   <th style=\"text-align:right;\"> fail_test </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### Specifying a weakly informative prior\n\nA core idea in Bayesian thinking is that we should formalize and incorporate our *prior beliefs* into our analyses. That is, prior to seeing _any_ data, what do we belief the treatment effect and other parameters of interest to look like? \n\nPrior beliefs could come from multiple sources, including previous studies, expert knowledge, historical data, and many other. What really Bayesian modelling, _Bayesian_ is that we explicitly specify these beliefs ahead of seeing the data and then update our prior beliefs based on the data. \n\nSince our data has a dichotomous outcome, it makes sense to apply logistic regression. Hence, we have two parameters for which we need to consider our prior beliefs: the _intercept_ and the _treatment effect_. \n\nSometimes, we only have a fuzzy sense about what the data might look like because little or no prior information is available. In those cases, we don't want over prior beliefs to overshadow the data. \n\nSay that was the case with our experiment on dyslexia. We would then rely on the default priors in `rstanarm`. These are **weakly-informative**; serving a soft constraints while leaving most of the leverage to the data. Because both the intercept and the treatment effect can take on any value, Normal priors are appropriate for both.\n\nIn code below, I 'hard-code' the default priors in `rstanarm` to make them explicit but you could also type in `prior_intercept = default_prior_intercept()` and `prior = default_prior_coef()` for the intercept and the treatment effect respectively. Further, To allow `rstanarm` to tweak our scales in accordance with the data, we set `autoscale = TRUE`.\n\nLet's run the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfail_model_weakinf <- \n  stan_glm(fail_test ~ condition,\n           data = fake, family = binomial,\n           prior = normal(location = 0, \n                          scale = 2.5, \n                          autoscale = T),\n           prior_intercept = normal(location = 0, \n                                    scale = 2.5, \n                                    autoscale = T),\n           chains = 4, iter = 5000*2, seed = 1407,\n           prior_PD = TRUE,\n           refresh = 0)\n```\n:::\n\n\nWe can call `prior_summary()` to check that our priors have been specified correctly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(fail_model_weakinf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'fail_model_weakinf' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 5)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n:::\n\n\nNotice that because we set `autoscale = T` `rstanarm` adjusted the prior for the treatment effect to have scale = 5.\n\nWe can also plot these distributions to do a visual inspection, for example using histograms:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fail_model_weakinf, \"hist\")\n```\n\n::: {.cell-output-display}\n![Distribution of the two simulated parameters for our weakly-informative priors](index_files/figure-html/fig-weak-coef-1.png){#fig-weak-coef width=672}\n:::\n:::\n\n\nMore generally, we should always set `PD = TRUE` in our first call, because `rstanarm` will then simulated data based on our prior beliefs. That way, we can check if the simulated data reflects our prior understanding.\n\nI'll show you two examples of how to use the simulated data to check your priors.  \n\nFirst, we can simulate 1,000 new data sets based on our priors. For each, we then calculate the proportion of students who failed the reading test. Recall that our prior understanding was fuzzy: we believed that it would center around 0 but this could vary quite a bit.\n\nLet us make a histogram and see if the simulated data reflects this prior understanding:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1407)\n\nfake |> \n  add_predicted_draws(fail_model_weakinf, ndraws = 1000) |>\n  group_by(.draw) |> \n  summarize(proportion_fail = mean(.prediction == 1)) |> \n  ggplot(aes(x = proportion_fail)) + \n  geom_histogram(fill = \"lightblue\", color = \"white\") + \n  geom_vline(xintercept = .5, color = \"firebrick\", lty = 2) + \n  labs(y = \"Count\",\n       x = \"Proportion of failed test\")\n```\n\n::: {.cell-output-display}\n![Checking the intercept. Simulated proportion of failed reading tests, given a weakly informative prior. The distribution is roughly uniform; reflecting that we have little idea about how many students in the control group would fail the reading test.](index_files/figure-html/fig-weak-prior-hist-1.png){#fig-weak-prior-hist width=672}\n:::\n:::\n\n\nLooks right! The histogram *is* centered around .5 but also has a lot of variation.\n\nSecond, we can simulate 100 treatment effects and see if these match our prior understanding. Recall, that we believed the treatment effect would center around 0 but that it could have *a lot* of variation to it (sd = 5).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1407)\n\nfake |> \n  add_epred_draws(fail_model_weakinf, ndraws = 100) |> \n  ggplot(aes(x = condition, y = .epred, group = .draw )) + \n  geom_line(linewidth = .3, color = \"lightblue\") + \n  scale_y_continuous(labels = scales::percent_format()) + \n  labs(x = NULL,\n       y = \"Proportion of failed tests\")\n```\n\n::: {.cell-output-display}\n![Checking the treatment effect. Simulated treatment effects from 100 simulated datasets, given a weakly-informative prior. Lines go in all directions; reflecting our weak prior beliefs about what the treatment effect could be. The proportion of failed students could be higher or lower in the treatment group. We don't really know.](index_files/figure-html/fig-weak-prior-lines-1.png){#fig-weak-prior-lines width=672}\n:::\n:::\n\n\nThese lines are all over the place, reflecting our weak prior understanding of what the treatment effect should look like. In other words, this looks exactly as we should expect.\n\n### Specifying an evidence-based prior\n\nThe default priors in `rstanarm` are always a good place to start. Sometimes, though, we do have additional knowledge at our disposal. For example, let us pretend our intervention is not new. In fact, we have several previous studies on the topic and these have have tested quite similar interventions. How can we take this prior knowledge into account in our modeling?\n\nBecause the model is the same as before, we define priors for the same to parameters: the _intercept_ and the _treatment_ effect.\n\nConcerning the intercept, suppose that previous studies have estimated the overall risk of failing the test from about 40 to 60 percent (because only students at-risk are asked to do the test, perhaps). We can specify our priors using the formula below: \n\n$$ log(odds) = log(\\frac{\\pi}{1-\\pi}) $$ \n\nLet us apply the formula to our 40 and 60 percent range: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the range\nlower_prob <- 0.4\nupper_prob <- 0.6\n\n# Calculate the log-odds\nlower_logodds <- log(lower_prob / (1 - lower_prob))\nupper_logodds <- log(upper_prob / (1 - upper_prob))\n\n# Specify the prior - location will be in the middle of lower and upper logodds\nprior_intercept <- \n  normal(location = (lower_logodds + upper_logodds) / 2, \n         scale = abs((upper_logodds - lower_logodds) / 4))\n```\n:::\n\n\nThen our location for the intercept will be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(prior_intercept$location, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nAnd the scale will be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(prior_intercept$scale, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2\n```\n:::\n:::\n\n\nSecond, we need to consider the treatment effect. Say we think that the chance of failing the reading test could be anywhere from 15 percent lower for the treatment group to no difference at all. Expressed in odds, these differences are .85 and 1 respectively. Let's use that range to specify a prior for the treatment effect:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the range\nlower_effect_odds <- 1\nupper_effect_odds <- .85\n\n# Convert to log-odds\nlower_effect_logodds <- log(lower_effect_odds)\nupper_effect_logodds <- log(upper_effect_odds)\n\n# Specify the prior for the treatment effect coefficient\nprior_treat <- \n  normal(location = (lower_effect_logodds + upper_effect_logodds) / 2, \n         scale = abs((upper_effect_logodds - lower_effect_logodds) / 4)\n         )\n\n# Print location and scale for the treatment effect\nround(prior_treat$location, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.08\n```\n:::\n\n```{.r .cell-code}\nround(prior_treat$scale, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04\n```\n:::\n:::\n\n\nAgain, we should simulate data using our priors to make sure that the data reflects our prior understanding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfail_model_evidence <- \n  stan_glm(fail_test ~ condition,\n           data = fake, family = binomial,\n           prior_intercept = prior_intercept,\n           prior = prior_treat,\n           chains = 4, iter = 5000*2, seed = 1407,\n           prior_PD = TRUE,\n           refresh = 0)\n```\n:::\n\n\nFirst we check that we specified the priors correctly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(fail_model_evidence) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'fail_model_evidence' \n------\nIntercept (after predictors centered)\n ~ normal(location = -2.8e-17, scale = 0.2)\n\nCoefficients\n ~ normal(location = -0.081, scale = 0.041)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fail_model_evidence, \"hist\")\n```\n\n::: {.cell-output-display}\n![Distribution of the two simulated parameters for our evidence-based priors](index_files/figure-html/fig-evidence-coef-1.png){#fig-evidence-coef width=672}\n:::\n:::\n\n\nAnd then we can simulate 1,000 data sets using our prior distribution. For each draw, we calculate the share of students who failed the reading test (`.prediction == 1`) and plot those in a histogram:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1407)\n\nfake |> \n  add_predicted_draws(fail_model_evidence, ndraws = 1000) |> \n  group_by(.draw) |> \n  summarize(proportion_fail = mean(.prediction == 1)) |> \n  ggplot(aes(x = proportion_fail)) + \n  geom_histogram(fill = \"lightblue\", \n                 color = \"white\") + \n  geom_vline(xintercept = .5, color = \"firebrick\", lty = 2) + \n  labs(y = \"Count\",\n       x = \"Proportion of failed students\")\n```\n\n::: {.cell-output-display}\n![Checking the intercept. Simulated proportion of failed reading tests for the full sample, given an evidence-based prior. The proportion centers more narrowly around .5; reflecting that we have stronger beliefs.](index_files/figure-html/fig-prior-evidence-1.png){#fig-prior-evidence width=672}\n:::\n:::\n\n\nOur histogram reflects our prior understanding since we believed the proportion of failed test would range between 40 and 60 percent with about 50 percent being our best guess. \n\nSecond, for checking the prior treatment effect, we can make the same plot as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1407)\n\nfake |> \n  add_epred_draws(fail_model_evidence, ndraws = 100) |> \n  ggplot(aes(x = condition, y = fail_test, group = .draw )) + \n  geom_line(aes(y = .epred), \n            linewidth = .3, color = \"lightblue\") + \n  scale_y_continuous(labels = scales::percent_format()) + \n  labs(x = NULL,\n       y = \"Probability of failing the reading test\")\n```\n\n::: {.cell-output-display}\n![Checking the treatment effect. Simulated treatment effects from 100 simulated datasets, given an evidence-based prior. Most lines slope downward; reflecting our belief that fewer students in the treatment group will fail the test.](index_files/figure-html/fig-prior-treat-1.png){#fig-prior-treat width=672}\n:::\n:::\n\n\nMost of these lines slope downwards, indicating that the expected fail rate is lower in the treatment group than the control group. \n\n### Final thoughts: Bayesian analysis of a randomized controlled trial - specifying the priors\n\nOne of the incredibly powerful aspects of Bayesian analysis is the ability to incorporate prior information into the analysis. In this post, I have shown how to specify an evidence-based and a weakly-informative prior as well as how to use simulation to check if these priors adequately reflects prior knowledge.\n\n### Cool! Where can I learn more?\n\n-   Johnson, A. A., Ott, M. Q., & Dogucu, M. (2022). *Bayes rules!: An introduction to applied Bayesian modeling*. CRC Press.\n\n-   Gelman, A., Hill, J., & Vehtari, A. (2020). *Regression and other stories*. Cambridge University Press.\n\n-   Ryan, E. G., Harrison, E. M., Pearse, R. M., & Gates, S. (2019). Perioperative haemodynamic therapy for major gastrointestinal surgery: the effect of a Bayesian approach to interpreting the findings of a randomised controlled trial. *BMJ open, 9(3)*.\n\n-   Goodrich, B., Gabry, J., Ali, I., & Brilleman, S. (2020). *rstanarm: Bayesian applied regression modeling via Stan*. Find many of their useful vignettes [here](https://mc-stan.org/rstanarm/).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}