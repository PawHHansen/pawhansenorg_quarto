[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paw Hansen",
    "section": "",
    "text": "Hi, I’m Paw! It’s great to have you here.\nI’m a researcher at the Danish Center for Social Science Research, where I focus on experimental behavioral science research related to the public sector. Currently, I’m studying how frontline workers make professional judgments and what impacts these might have on clients. You can explore some examples of my research right here.\nMy research interests also include data visualization, Bayesian modeling, and finding effective ways to communicate statistics and evidence to practitioners in a compelling and intuitive manner. I occasionally write blog posts on these subjects.\nWhen I’m not in the office, I find joy in sourdough baking, sharing coffee moments with friends, and immersing myself in somber novels during the evenings. I reside in Copenhagen along with my wife and our two children."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Paw Hansen",
    "section": "",
    "text": "Hi, I’m Paw! Good to see you.\nI’m a social science researcher doing experimental behavioral science research related to the public sector. Currently, I study how frontline workers make professional judgements, and how these affect clients.\nMy research interests include data visualization, Bayesian modeling, and how to communicate statistics and evidence to practitioners in compelling and intuitive ways.\nWhen I’m out of office, I enjoy sourdough baking, having coffee with friends, and reading gloomy novels in the evenings. I live in Copenhagen with my wife and our baby boy, Jakob."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Summer Simulations: Solving the Marriage Problem Using Tidy Simulation\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nUse tidy simulation to solve a version the marriage problem!\n\n\n\n\n\n\nJun 12, 2024\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nSummer Simulations: Exploring Bertrand’s Ballot Problem with a Simulation Study\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nUse tidy simulation to solve a version of Bertrand’s classic ballot problem\n\n\n\n\n\n\nJun 11, 2024\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nSummer Simulations: Optimizing the Number of Factory Workers in Erewhon\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nUse tidy simulation to solve an optimization puzzle in R!\n\n\n\n\n\n\nJun 11, 2024\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nThe policy evaluator’s toolbox: Presenting your next instrumental variable analysis\n\n\n\n\n\n\n\nstatistical analysis\n\n\npolicy evaluation\n\n\n\n\nAll your econ friends are doing it. Get started with instrumental variables\n\n\n\n\n\n\nNov 3, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nUsing simulation for design analysis part II: Iterating over multiple values\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nAnalyze the properties of your design, varying multiple features at once.\n\n\n\n\n\n\nSep 12, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nUsing simulation for design analysis: A field guide\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nHow simulation can help you make better design decisions ahead of collecting data.\n\n\n\n\n\n\nAug 26, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nWe better test that: Writing tests to ensure data quality\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nNo one likes to be surprised when it comes to data quality. Writing tests will help you varify your data is what you think it is\n\n\n\n\n\n\nAug 23, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nBayesian analysis of a randomized controlled trial III: Interpretation and presentation\n\n\n\n\n\n\n\nBayesian modeling\n\n\npresentation\n\n\n\n\nFinal part of my three-part series on Bayesian analysis of randomized controlled trials. Get ready to interpret and present your results!\n\n\n\n\n\n\nJul 13, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nBayesian analysis of a randomized controlled trial II: Defining and validating the model\n\n\n\n\n\n\n\nBayesian modeling\n\n\n\n\nPart two of my three-part series on Bayesian analysis of randomized controlled trials. You’ve built a model but is it any good?\n\n\n\n\n\n\nJul 5, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nBayesian analysis of a randomized controlled trial I: Specifying the priors\n\n\n\n\n\n\n\nBayesian modeling\n\n\n\n\nKicking off my three-part series on doing Bayesian analysis of randomized controlled trials. First up: specifying your priors.\n\n\n\n\n\n\nJul 2, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nDot-dot-dot: Three dotplots to stake your life on\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nCelebrating the beauty of dots and points in plotting data.\n\n\n\n\n\n\nJun 9, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nThe policy evaluator’s toolbox: Presenting your next regression discontinuity analysis\n\n\n\n\n\n\n\nstatistical analysis\n\n\npolicy evaluation\n\n\n\n\nWhen evaluating public policies, regression discontinuity designs are impossible to live without. Here is how to do one.\n\n\n\n\n\n\nJun 1, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nThe policy evaluator’s toolbox: Presenting results from randomized experiments\n\n\n\n\n\n\n\nstatistical analysis\n\n\npolicy evaluation\n\n\n\n\nSensible out-of-the-box tables and plots for your next randomized controlled trial\n\n\n\n\n\n\nMay 24, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nThe Human Scale Principle for presenting statistical results\n\n\n\n\n\n\n\nstatistical analysis\n\n\npresentation\n\n\n\n\nWhen presenting statistical results, don’t rely on the default computer output. Do this instead.\n\n\n\n\n\n\nMay 17, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nShow the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Invisible frontline work: How client characteristics cause extra-role behaviours in public service\n\n\n\n\n\n\n\nfrontline work\n\n\n\n\nFrontline workers will happily go beyond the call of duty for their students. But they are more willing to accommodate some students than others.\n\n\n\n\n\n\nOct 26, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nMaking public service employees aware of their positive impact\n\n\n\n\n\n\n\nfrontline work\n\n\n\n\nA nagging question: How well do MTurk-validated interventions fare among real-world samples?\n\n\n\n\n\n\nFeb 14, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nRule bending on the frontlines of public service delivery\n\n\n\n\n\n\n\nfrontline work\n\n\n\n\nWhen and why do caseworkers bend the rules in favor of their clients?\n\n\n\n\n\n\nNov 19, 2022\n\n\nPaw Hansen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/replication-motivation/index.html",
    "href": "research/replication-motivation/index.html",
    "title": "Making public service employees aware of their positive impact",
    "section": "",
    "text": "I have a new article out in Public Administration.\nStudies show that asking frontline workers (teachers, nurses, police officers) to reflect on the positive difference they make in citizens’ lives will enhance their motivation. This idea makes sense: People like to help each other out, and when making one’s positive impact more obvious, we should expect public servants to be happier about their jobs.\nHowever, previous studies have often relied on paid survey respondents, such as those recruited via Amazon MTurk. The result is an unfortunate mismatch between test sample and target population. Essentially, we should probably expect people who sign up to participate in a paid survey to be motivated by different outcomes than frontline workers in general.\nTo test how well reflection tasks do among real-world public servants, I did a replication of a recent study by Vogel and Willems. My replication study was set among 412 Danish caseworkers working with unemployed clients.\nFigure 1 is the key graph from the paper. The control group was not asked to do any reflection task but simply answered a few questions about job satisfaction and turnover intention. The prosocial treatment group reflected on a recent impact they had made on a specific client, whereas the societal treatment group reflected on a recent impact on society more generally.\nAs you can see from the graph, there is really no difference between any of the groups, and this indifference holds for both outcomes. Thus, my key claim was that the effect sizes of reflection tasks are likely smaller when deployed “in the wild.” And so we should probably be critical when we consider how much we can get from low-cost reflection tasks.\n\n\n\nFigure 1: Outcomes across experimental conditions\n\n\nThe published version of the article is Open Access. You can find it here."
  },
  {
    "objectID": "research/rule-bending-caseworkers/index.html",
    "href": "research/rule-bending-caseworkers/index.html",
    "title": "Rule bending on the frontlines of public service delivery",
    "section": "",
    "text": "My first article is out in the International Public Management Journal!\nHere’s what it’s about: When facing a disobedient client, caseworkers are often required to impose sanctions. But even when sanctions are explicitly required by law, research shows that caseworkers may turn a blind eye. Why so?\nThe study asks how certain clients can merit themselves to avoid sanctioning.\nDrawing from existing literature, I propose that clients can avoid sanctions if they seem (1) needy, (2) hard working, or (3) resourceful.\nTo test these expectations, I ran a conjoint vignette experiment among Danish unemployment caseworkers. In the experiment, I presented each of 407 caseworkers with 3 fictive client descriptions and asked if the caseworker would sanction that client. Within each client description, I randomly assigned several pieces of information concerning the three types of deserving clients above.\nImportantly, according to policy rules, all clients should have been sanctioned, yet for many of the client profiles caseworkers were unwilling to impose sanctions.\nInterestingly, caseworkers tended to favor stronger clients when bending the rules: Clients who appeared motivated, had not been sanctioned in the past, or had years of previous job experience were all less likely to be sanctioned.\nThis is a bit of paradox: Although welfare usually targets clients in need, avoiding welfare sanctions seems based on client resources. Consequently, caseworker rule-bending can have unintended distributional consequences since stronger clients are those who can get away with disobedience.\nHere is key figure from the paper:\n\n\n\nFigure 1: Causal effect of various client attributes on caseworker sanctioning\n\n\nYou can download a preprint of the article here."
  },
  {
    "objectID": "blog/human-scale-principle/index.html",
    "href": "blog/human-scale-principle/index.html",
    "title": "The Human Scale Principle for presenting statistical results",
    "section": "",
    "text": "Computer software, such as R and Python, makes it easy to build complex statistical models. And with a few lines of code, you can get all sorts of outputs summarizing the model parameters.\nHowever, the default computer output for most statistical models is rarely the most intuitive or compelling way to present your results to stakeholders.\nIn this post, I introduce a basic principle for how to present statistical results in a way that is easy to interpret, compelling to readers, and requires no special knowledge to understand. Applying these guidelines will ensure that your next data analysis actually makes the impact on stakeholders you intend it to make.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(socviz)\nlibrary(broom)\nlibrary(scales)\nlibrary(modelsummary)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\n\nThe problem with accepting the default computer output\nConsider the following example using survey data from the General Social Survey. Using respondents’ age (in years) and vote in the 2016 election (Obama(not Obama), we want to predict support of marihuana legalization (yes/no). That is, our data looks as in Table 1.\n\n\nCode\ndata(\"gss_sm\")\n\n# Recode\ngss_support &lt;- \n  gss_sm %&gt;%\n  drop_na(grass, obama) %&gt;%\n  mutate(grass = ifelse(grass == \"Legal\", 1, 0),\n         obama = factor(ifelse(obama == 1, \"Obama\", \"Not Obama\"))) |&gt; \n  select(c(grass, obama, age))\n\nhead(gss_support) |&gt; \n  kbl()\n\n\n\n\nTable 1: A peak at the data for our example\n\n\ngrass\nobama\nage\n\n\n\n\n1\nObama\n61\n\n\n0\nNot Obama\n72\n\n\n1\nObama\n55\n\n\n1\nObama\n53\n\n\n0\nNot Obama\n71\n\n\n1\nObama\n32\n\n\n\n\n\n\n\n\nLet’s fit a basic logistic regression model predicting support for legalization from age and whether respondents voted for Obama in 2016. Table 2 shows the results.\n\n\nCode\n# Fit model\nlog_mod &lt;- \n  glm(grass ~ age + obama, data = gss_support, family = \"binomial\")\n\n# Show model\nmodelsummary(models = list(\"Support for legalization\" = log_mod),\n             stars = TRUE, fmt = 2, \n             coef_rename =c(\"age\" = \"Age (years)\",\n                            \"obamaObama\" = \"Voted Obama in 2016\") ) \n\n\n\n\nTable 2: Predicting support for marihuana legalization\n\n\n\nSupport for legalization\n\n\n\n\n(Intercept)\n1.17***\n\n\n\n(0.25)\n\n\nAge (years)\n−0.03***\n\n\n\n(0.00)\n\n\nVoted Obama in 2016\n1.10***\n\n\n\n(0.13)\n\n\nNum.Obs.\n1115\n\n\nAIC\n1364.0\n\n\nBIC\n1379.1\n\n\nLog.Lik.\n−679.014\n\n\nRMSE\n0.46\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nTable 2 contains several numbers, some of which have asterisks next to them. If this makes you happy, beware.\nBefore going further, quiz yourself and see if you can answer the following question without using any statistical jargon: what can we say about the relationship between age and support for legalization?\nIf your answer begins “There is a statistically significant relationship between…”, then you are going off track. If your answer begins “The coefficient reveals that the log(odds) of age…”, all the worse.\nBeing a reader of this blog post, you likely have some training in statistical modeling. But your stakeholders might not. If you have a hard time communicating the conclusions from the output in plain language, the default output will probably make little sense to your stakeholders.\n\n\nWhat to do instead: Make predictions on a ‘human scale’\nThe general rule of thumb I propose here is that you always try to present your results ‘on a human scale’. Thinking of someone with no formal training in statistics, what would make sense to that person? What kind of questions would someone caring about the substance (but not the methods) be interested in?\nFor example, log(odds) do not buy you anything down at the supermarket. They have no value or interpretation outside the realms of statistics. Neither do standard deviations, z-scores, or p-values. Without statistical training, these numbers convey no information (and even people with statistical training often get them wrong).\nInstead, use your model to make predictions on a human scale. When deciding on what predictions to make, list all the different questions that someone with no statistical training would want to know the answer to. In the case of our logistic regression, such questions might be:\n\nHow likely is it that an average Republican will support legalization?\n\nHow much more/less is that than the expected support from an average Democrat?\nDoes the support stay the same over the course of a lifetime, or does it drop? By how much does it change?\nHow certain are we of these conclusions?\n\n\n\nTaking a sad presentation and making it better\nFor our example, I use crossing() to set up a dataframe with the values we want to see predictions for. Specifically, I use a range of age going from 18 through 80 years and people who voted for/did not vote for Barack Obama.\n\n\nCode\nnew_dat &lt;- \n  crossing(\n    age = 18:80, \n    obama = c(\"Obama\", \"Not Obama\")\n  )\n\n\nWith that in place, we can calculate our predicted probabilities using augment() from the broom package:\n\n\nCode\npreds &lt;-\n  augment(log_mod, newdata = new_dat, type.predict = \"response\", se_fit = T) %&gt;%\n  mutate(lwr = .fitted - 1.96 * .se.fit,\n         upr = .fitted + 1.96 * .se.fit) \n\n\nNow we have a large amount of predicted probabilities of substantive interest. A good idea would be to graph them. This can be done in several ways, depending on what you want to focus your analysis on. Figure 1 is my suggestion.\n\n\nCode\npreds %&gt;%\n  ggplot(aes(age, .fitted, color = obama)) + \n  geom_line(linewidth = .6) +\n  geom_line(aes(x = age, y = lwr, group = obama), alpha = .7, linetype = \"dashed\") + \n  geom_line(aes(x = age, y = upr, group = obama),  alpha = .7, linetype = \"dashed\") + \n  scale_y_continuous(label = percent_format()) + \n  scale_color_brewer(type = \"qual\", palette = \"Set1\") + \n  labs(x = \"Age (years)\",\n       y = \"Probability of support\",\n       color = \"Vote in 2016 election\") + \n  theme(legend.position = \"top\")\n\n\n\n\n\nFigure 1: Probability of supporting marihuana legalization, by age\n\n\n\n\nWhen writing up the analysis, carefully go over the graph. What do we see?\nA useful structure might be:\n\nPresent the graph. What is on the axes? What do the lines represent?\nPresent general findings. What are some trends visible from the graph?\nUse a few of the calculated predictions to provide readers with some concrete examples.\n\nConcerning 2) and 3), one example, I often use as a template is the following quote from King, Tomz, and Wittenberg (2000, 347):\n\n“Other things being equal, an additional year of education would increase your annual income by $1,500 on average, plus or minus about $500.”\n\nAs a brief example, consider the following paragraph:\n\nFigure 1 shows the results. Solid lines illustrate the predicted probability of supporting marihuana legalization across respondents’ age for people who voted/did not for Obama in the 2016 election. Dashed lines are the 95 percent uncertainty intervals.\n\n\nFirst, we found that political support was a strong predictor of support for legalization. For example, at the median age of 49 years, the predicted probability of support among Obama voters was 74 percent (+/- about 1.7 pct. points), whereas a Republican had only a 49 percent probability of support (+/- 2.5 pct. points).\n\n\nSecond, we found that support for legalization drops with age. For example, at age 74, a Republican is only about half as likely to support legalization as is an 18-year old.\n\nSome final advice: Make sure that you round your estimates instead of just throwing useless decimals at your reader. And always provide readers with a sense of the uncertainty around your estimates.\n\n\nFinal thoughts: Presenting results ‘on a human scale’\nAs a data analyst, you should not just “get the numbers right”, but also “get the right numbers”. To do so, you must work on a human scale; thinking about the questions that would be compelling to someone who cares about the substance but not the methods. Use your model to make predictions that address those questions. Then use those predictions to make a compelling graph and provide a few illustrative examples. Done.\n\n\nCool! Where can I learn more?\n\nKing, G., Tomz, M., and Wittenberg, J. (2000). Making the most of statistical analyses: Improving interpretation and presentation. American journal of political science, 347-361.\nHealy, K. (2018). Data visualization: a practical introduction. Princeton University Press.\n\n\n\n\n\n\nReferences\n\nKing, Gary, Michael Tomz, and Jason Wittenberg. 2000. “Making the Most of Statistical Analyses: Improving Interpretation and Presentation.” American Journal of Political Science 44 (2): 347. https://doi.org/10.2307/2669316."
  },
  {
    "objectID": "blog/bayesian-rct-presentation/index.html",
    "href": "blog/bayesian-rct-presentation/index.html",
    "title": "Bayesian analysis of a randomized controlled trial III: Interpretation and presentation",
    "section": "",
    "text": "Welcome to the last part of my three-part series on Bayesian analysis of randomized controlled trial. So far, we have specified priors to take previous knowledge into account and we have fit and validated two different models.\nIn this final post, we’ll look at how we could go about presenting and interpreting our results. In a report or scientific article, this is the part that should go into the Results section.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\nIn my view, presenting results is where Bayesian analysis really gets to shine. As I hope you will see, taking a Bayesian approach makes it easy to present meaningful and intuitive quantities of interest and gives us incredible flexibility when it comes to answering question of real-world importance.\nRecall that in our hypothetical experiment, students were randomly assigned to either a treatment or a control group. The outcome is dichotomous: did the students fail the reading test (indicating dyslexia) or not.\nBefore we go into any calculations, let’s look at four core quantities of interest commonly used for controlled trial settings:\n\nabsolute risk\nabsolute risk reduction\nrelative risk\nnumbers needed to treat\n\nWhat do these numbers mean? The absolute risk is simply the risk of an event happening given the exposure.\n\nabs_risk &lt;- \n  fake |&gt; \n  summarize(abs_risk = mean(fail_test, na.rm = T), \n            .by = condition)\n\nabs_risk \n\n# A tibble: 2 × 2\n  condition abs_risk\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Control      0.520\n2 Treatment    0.407\n\n\nAt first sight, it seems the treatment worked! The absolute risk of failing the reading test was 0.52 in the control group but only 0.41 in the treatment group.\nSecond, the absolute risk reduction is the difference between the observed risks the treatment and the control group:\n\nround(abs_risk$abs_risk[abs_risk$condition == \"Control\"] \n      - abs_risk$abs_risk[abs_risk$condition == \"Treatment\"], 2)\n\n[1] 0.11\n\n\nFor an individual, the risk reduction expresses the estimated difference in the probability of experiencing the event. In our case the reduction is 11 percent.\nThird, relative risk is the risk of an event occurring in the exposed group divided by the risk of the event occurring in the non-exposed group:\n\nround(abs_risk$abs_risk[abs_risk$condition == \"Treatment\"] \n      / abs_risk$abs_risk[abs_risk$condition == \"Control\"], 2)\n\n[1] 0.78\n\n\nThus, the relative risk of failing the reading test for students in the treatment group was only 0.78 as compared to students in the control group.\nFinally, the number needed to treat is the number of individuals you need to treat to prevent one additional bad outcome. For calculating the number needed to treat, we simply take 1 and divide it by the absolute risk reduction:\n\nround(1/ (abs_risk$abs_risk[abs_risk$condition == \"Control\"] \n          - abs_risk$abs_risk[abs_risk$condition == \"Treatment\"]), 0)\n\n[1] 9\n\n\nOur results suggests that for every 9 students placed in the program, we could prevent 1 student from failing the reading test.\nHopefully, calculating each of these quantities “by hand”, has given us some intuition about what each number conveys. In practice, though, we will never want to do it this way ever again. For one thing, the above approach did not give us any uncertainty estimates, which we should always include as part of any interpretation. Moreover, recalling formulas can be tedious and for more complicated models, many of these will break down anyway. Instead, we will use our posterior simulations.\n\nAnalysis using rstanarm\nThe core principle for presenting results is that we don’t want to just show a list of model parameters. These have little interest in and off themselves. Instead, we’ll use the model to make predictions, i.e. show the implications of the model.\nTo this end, we set up a data frame containing the values of the predictors we intend to predict from (aka a “predictor matrix”). In our context of a controlled trial, these are simply the treatment and the control group:\n\npred_dat &lt;- \n  crossing(\n    condition = c(\"Control\", \"Treatment\")\n    )\n\nWe can then use this data frame to estimate the absolute risk for each group:\n\nmodel_preds &lt;- \n  add_epred_draws(newdata = pred_dat, mod_rstan)\n\nWith our predictions in place, we can plot distributions of each of the four quantities of interest, for example for including in a presentation. The following code chunk shows how to plot each of the four quantities from before:\n\n\nCode\n# abs risk of passing reading test\nmodel_preds |&gt; \n  ggplot(aes(.epred, fill = condition)) + \n  geom_density(alpha = .4) + \n  annotate(geom = \"text\", x = .41, y = 2, label =\"Treatment\") + \n  annotate(geom = \"text\", x = .52, y = 2, label =\"Control\") + \n  scale_x_continuous(labels = scales::label_percent()) + \n  scale_y_continuous(labels = NULL) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Absolute risk\",\n       y = NULL, \n       fill = \"Condition\") + \n  theme(legend.position = \"none\")\n\n# rel risk of failing\nrel_risk &lt;- \n  as_tibble(posterior_epred(mod_rstan, newdata = pred_dat)) |&gt; \n  mutate(control = `1`, treatment = `2`) |&gt; \n  mutate(relrisk = treatment / control) \n\nrel_risk |&gt; \n  ggplot(aes(relrisk)) + \n  geom_histogram(fill = \"lightblue\", color = \"white\") + \n  geom_vline(xintercept = median(rel_risk$relrisk), \n             lty = 2,\n             color = \"firebrick\") + \n  scale_y_continuous(labels = NULL) +\n  labs(x = \"Relative risk\", \n       y = NULL)\n\n# abs risk reduction \nabs_rr &lt;- \n  as_tibble(posterior_epred(mod_rstan, newdata = pred_dat)) |&gt; \n  mutate(control = `1`, treatment = `2`) |&gt; \n  mutate(abs_rr = control - treatment) \n\nabs_rr |&gt; \n  ggplot(aes(abs_rr)) + \n  geom_histogram(fill = \"lightblue\", color = \"white\") +\n  geom_vline(xintercept = median(abs_rr$abs_rr), \n             lty = 2,\n             color = \"firebrick\") + \n  scale_y_continuous(labels = NULL) +\n  labs(x = \"Absoulte risk reduction\",\n       y = NULL)\n\n# Numbers needed to treat \nnntt &lt;- \n  abs_rr |&gt; \n  mutate(nn_treat = 1/abs_rr) \n\nnntt |&gt; \n  ggplot(aes(nn_treat)) +\n  geom_histogram(fill = \"lightblue\", color = \"white\") +\n  geom_vline(xintercept = median(nntt$nn_treat), \n             lty = 2,\n             color = \"firebrick\") + \n  scale_y_continuous(labels = NULL) +\n  xlim(0, 25) + \n  labs(x = \"Numbers needed to treat\", \n       y = NULL)\n\n\n\n\n\n\n\n\n(a) Absolute risk of failing reading test.\n\n\n\n\n\n\n\n(b) Relative risk of failing reading test when given treatment as opposed to control.\n\n\n\n\n\n\n\n\n\n(c) Absolute risk reduction in failing reading test when given treatment\n\n\n\n\n\n\n\n(d) Numbers needed to treat: How many students should be placed in the program to prevent one student from failing the reading test?\n\n\n\n\nFigure 1: Quantities of interest with uncertainties, calculated using rstanarm.\n\n\n\nWhen writing up our analysis, we could include one or more of these plots and then use the predictions to make meaningful statements about the model implications and uncertainties. Taking the number needed to treat as an example, we might want to calculate the 80 percent prediction interval:\n\nround(quantile(nntt$nn_treat, c(.1, .5, .9)))\n\n10% 50% 90% \n  6   9  14 \n\n\nFrom that, we could write something like: “Using our Bayesian model, we estimated the number needed to treat. We found that the intervention should be applied to 9 students to prevent one additional case of dyslexia, plus or minus about 4 students.”\n\n\nTaking the posterior out for a spin: probability of a practically significant effect\nSo far we have covered most the standard quantities of interest for analyzing randomized controlled trials. But Bayesian analysis has more to offer.\nWhen working within a Bayesian framework, a lot of the hard work lies in specifying a sensible prior and then fitting a proper model. But once we have our posterior distribution(s) at hand, we can calculate any quantity that might be of interest.\nTo give a brief example, suppose a policy maker is only interested in scaling up the intervention if the absolute risk reduction is at least 15 percent (perhaps out of political or cost-effectiveness considerations).\nLooking at the histogram in Figure 1 (c), we see that some of the distribution is above .15. It is not impossible that the treatment effect could be .15 or more. But how likely is that? Put another way, given what we know (model, data), what is the probability that the absolute risk reduction is above 15 percent?\nWe can approach this graphically by drawing the empirical cumulative distribution, which shows the share of observations being lower than the specified value on the x-axis.\nFigure 2 (a) shows a distribution of plausible risk reductions given the data and model. The shaded region starting at 15 percent is what we care about. This region is an incredibly powerful concept because it allows us to think about effect sizes in terms of what is practically, as opposed to statistically, significant.\nWhat is the probability that our risk reduction lies within this region? Figure Figure 2 (b), shows a so-called quantile function, plotting the potential risk reductions on the x-axis and the inverse cumulative probability on the y-axis. Substantially, the inverse cumulative probability describes the probability of the risk reduction being greater than the number on the x-axis.\nLooking at the figure, we see that the probability is about 13 percent. Is this a lot or a little? Well, that depends…\nOf course 13 percent, or about 1-in-8, does not sound like a lot. But suppose that running the intervention is really cheap. Then, policy makers may want to “roll the dice” and see what happens. After all, the probability that the effect size is greater than 15 percent is small, but the probability that the effect size is at least 5 percent is 98 percent!\nThe point is that when doing Bayesian analysis, we do not bake decision-making into the analysis, such as saying that the treatment “worked” because the p-value happened to be &lt;0.05. Instead, we live with the uncertainty; estimating the effect size and uncertainties the best we can, and then leave the rest up to stakeholders.\n\n\nCode\n# Density curve \nabs_rr |&gt; \n  ggplot(aes(abs_rr)) + \n  geom_density() + \n  geom_vline(xintercept = .15, \n             linetype = \"dashed\", \n             color = \"steelblue\") + \n  geom_area(data = abs_rr_density_tibble, \n            aes(x = abs_rr, y = density), \n            fill = \"lightblue\",\n            alpha = .4) + \n  scale_x_continuous(labels = scales::label_percent()) + \n  scale_y_continuous(labels = NULL) + \n  labs(x = \"Absolute risk reduction\",\n       y = NULL)\n\n# ECDF\nabs_rr |&gt; \n  ggplot(aes(abs_rr)) +\n  geom_vline(xintercept = .15, color = \"steelblue\", linetype = \"dashed\") + \n  stat_ecdf(aes(y = 1 - ..y..)) + \n  geom_point(data = secret_dot, aes(abs_rr, ecdf), color = \"steelblue\") + \n  annotate(geom = \"text\", x = .21, y = .14, label = str_c(\"Pr(ARR &gt; 15 pct.): \\n\", secret_prob, \" pct.\"), color = \"steelblue\", size = 4) + \n  scale_x_continuous(labels = scales::label_percent(),  limits = c(0, .25)) + \n  scale_y_continuous(labels = scales::label_percent()) + \n  labs(x = \"Absolute risk reduction\", \n       y = \"Inverse cumulative probability\")\n\n\n\n\n\n\n\n\n(a) Distribution of the plausable risk reductions given the data and model. Policy maker only cares about the blue area.\n\n\n\n\n\n\n\n(b) Quantile fundction showing the distribution of plausable risk reductions. The graph shows the probability of the absoulute risk reduction being lower than the specified value on the x-axis.\n\n\n\n\nFigure 2: What is the probability that the risk reduction is above 15 percent?\n\n\n\nOf course, in addition to eyeballing the graph, we can ask R to extract the exact probability for us:\n\n\nCode\nmean(abs_rr$abs_rr &gt; .15)\n\n\n[1] 0.12675\n\n\nAs a final tip, interpreting a quantile function may be tricky to non-technical audiences. If I were advising, say, a politician, I might prefer to present some meaningful risk reductions and then simply present these in a small table such as Table 1:\n\n\n\n\nTable 1: Posterior probabilities of various minimum effect sizes\n\n\nMinimum risk reduction\nProbability\n\n\n\n\n0.05\n0.98\n\n\n0.10\n0.68\n\n\n0.15\n0.13\n\n\n0.20\n0.00\n\n\n\n\n\n\n\n\n\n\nFinal thoughts: Bayesian analysis of a randomized controlled trial - interpretation and presentation\nBayesian analysis has a lot to offer when conducting and interpreting statistical analyses of randomized controlled trials. It allows analysts to interpret results in a way that is usually far more intuitive and useful to policy makers than standard reports relying on statistical significance tests. Rather than discretizing results (the treatment either “worked” or “failed”), Bayesian analysis makes it possible to make more nuanced statements (“the probability that the treatment effect is above the desired 15 percentage points is 97 percent”). Taken together, I hope this post and the two previous ones in the three-part series has encouraged you to go and do your first Bayesian analysis!\n\n\nCool! Where can I learn more?\n\nJohnson, A. A., Ott, M. Q., & Dogucu, M. (2022). Bayes rules!: An introduction to applied Bayesian modeling. CRC Press.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.\nRyan, E. G., Harrison, E. M., Pearse, R. M., & Gates, S. (2019). Perioperative haemodynamic therapy for major gastrointestinal surgery: the effect of a Bayesian approach to interpreting the findings of a randomised controlled trial. BMJ open, 9(3).\nGoodrich, B., Gabry, J., Ali, I., & Brilleman, S. (2020). rstanarm: Bayesian applied regression modeling via Stan. Find many of their useful vignettes here."
  },
  {
    "objectID": "blog/toolbox-rdd/index.html",
    "href": "blog/toolbox-rdd/index.html",
    "title": "The policy evaluator’s toolbox: Presenting your next regression discontinuity analysis",
    "section": "",
    "text": "In the previous post of my “policy evaluator’s toolbox” series, I showed you two basic ways of plotting results from randomized experiments. A randomized experiment should be the design you are going for whenever possible since it allows for easy estimation and clean causal inference.\nIn practice, though, randomized experiments may not be feasible. Often policymakers are unwilling to assign policies at random, so the question becomes what we can do in those cases if we still want to evaluate if a policy had an effect.\nToday, we’ll have a look at one widely-used approach: the regression discontinuity design. Here is the definition I got when asking ChatGPT:\n\n“A regression discontinuity design (RDD) is a quasi-experimental research design used to estimate causal effects by taking advantage of a discontinuity in the treatment assignment process. The basic logic behind RDD is that when a certain threshold or cutoff point is used to determine whether an individual receives a treatment or not, individuals on either side of the cutoff are expected to be similar in all other relevant aspects, except for the treatment itself. By comparing the outcomes of individuals just above and just below the cutoff, researchers can attribute any differences to the treatment rather than other confounding factors.”\n\nThis is not a bad definition but I do want to emphasize one core assumptions: the assignment variable should be continuous or “smooth” around the cutoff.\nIn my field of public welfare, many such cutoff variables exist:\n\nAge. For example, everybody younger than 21 years old might receive free dental care\nIncome. For example, everybody earning above some level might lose eligibility for housing benefits\nTime. For example, a new tutoring program may be implemented on January 1st making it possible to estimate its effects by comparing units just before and just after the cutoff date\n\nFinally, a popular threshold is test scores, which we will work with today. Specifically, we’ll use the Chile Schools data taken from (the highly recommendable) Regression and Other Stories. Let’s get our packages and load up the data:\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggsci)\nlibrary(readr)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\nchile &lt;- read_csv(\"chile.csv\")\n\n\nThe Chile Schools data is a good example of something you will often encounter when doing policy evaluations. In 1992, the Chilean government wanted to improve poor-performing schools. To do so, the government assigned extra resources (teacher training and after-school tutoring) to all schools scoring lower than a predefined cutoff on the average fourth-grade test score. This assignment is clearly not random. In fact, we can be quite certain that the schools that received the extra resources are different from those that did not (that is why those schools received extra help!).\nHowever, as we move closer to the cutoff, it seems more and more likely that the schools falling on either side are not that different. For example, say the cutoff was 60 points on a 100-point scale. Two schools with an average score of 30 and 70 are clearly different, and it makes no sense to compare those. But what about two schools scoring 58 and 62? Do we really believe that those schools differ fundamentally from each other? The key assumption we make in a regression discontinuity design is that the units we look at on either side of the cutoff are as-if random.\n\nPlot 1: Showing the raw data\nA good place to start is to show the raw data on either side of the cutoff. In the Figure 1, all units above the threshold (scaled to zero) received extra help, while those below did not.\n\n\nCode\n# Make a variable saying if schools received the treatment or not\nchile &lt;-\n  chile %&gt;% \n  mutate(condition = ifelse(rule2 &lt; 0, \"Treatment\", \"Control\")) \n\n\nchile %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 1: Reading test scores in 1992 by assignment variable. The assignment variable is centered reading test scores in 1988. Having a score less than 0 meant that schools would be included in the program.\n\n\n\n\nHowever, since we wanted schools to be “alike” (in terms of potential outcomes), we should probably focus the graph on those units that are close to the threshold. Let’s define a cutoff range from -5 through 5 and make a new plot (Figure 2):\n\n\nCode\nchile_sub &lt;- \n  chile %&gt;% \n  filter(rule2 &lt; 5, rule2 &gt; -5) \n\nchile_sub %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  geom_point(aes(color = condition)) + \n scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 2: Reading test scores in 1992 by assignment variable. In this version of the graph, only schools close to the cutoff (-5 through 5) are plotted.\n\n\n\n\nAnd we can even make this plot a bit clearer if we use binned averages instead of showing all data points:\n\n\nCode\nchile_sub %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  stat_summary_bin(aes(color = condition), \n                   fun = \"mean\", \n                   size = 2, \n                   geom = \"point\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 3: Reading test scores in 1992 by assignment variable. In this version of the graph, schools are plotted using binned averages on the assignment variable.\n\n\n\n\n\n\nPlot 2: Adding the model\nShowing the data (to others but also yourself) is always a good place to start and a key part of of exploratory data analysis. However, to answer the question of interest (did assigning extra help to schools improve reading?), we need a model.\nLuckily, the modeling part of a regression discontinuity analysis is straightforward: We simply include the condition and the assignment variable as predictors in our regression model:\n\n\nCode\nchile_rs &lt;- \n  lm(read92 ~ condition + rule2, data = chile_sub) \n\nchile_rs %&gt;% \n  tidy(conf.int=T,) %&gt;% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %&gt;% \n  mutate(term = c(\"Intercept\", \"In program\", \"Assignment variable\")) %&gt;% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      digits = 2)\n\n\n\n\nTable 1: Effect of being in policy program\n\n\nTerm\nEstimate\nLower bound\nUpper bound\n\n\n\n\nIntercept\n59.83\n58.80\n60.85\n\n\nIn program\n2.15\n0.27\n4.04\n\n\nAssignment variable\n0.95\n0.61\n1.28\n\n\n\n\n\n\n\n\nThus, the estimated effect of being in the program is about 2.15 points.\nTo understand this result better, and to present it in an intuitive way to stakeholders, we can plot the model on either side of the cutoff (i.e. the regression line):\n\n\nCode\nchile_sub %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  geom_smooth(\n    data = chile_sub %&gt;%  filter(rule2 &lt; 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  geom_smooth(\n    data = chile_sub %&gt;%  filter(rule2 &gt; 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  annotate(geom=\"text\", \n           x  = -2.5, \n           y = 33, \n           color = \"#DF8F44FF\", \n           label = \"Included in program\") + \n    annotate(geom=\"text\", \n           x  = 2.5, \n           y = 33, \n           color = \"#374E55FF\", \n           label = \"Not included in program\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 4: Reading test scores in 1992 by assignment variable. In this version of the graph, the fitted model on either side of the threshold is overlaid on the raw data.\n\n\n\n\nThe estimated effect of 2.15 points corresponds to the difference (the “kink”) between the two lines.\n\n\nBuilding the final model\nFinally, in most real-world applications, it makes sense to include pre-treatment variables that strongly predict the outcome since this can make our model estimates more precise. In our example, we have schoolchildren’s math and reading scores in 1988, which is probably useful when trying to predict reading scores in 1992. Let’s include those:\n\n\nCode\nchile_rs_final &lt;- \n  lm(read92 ~ condition + rule2 + read88 + math88, data = chile_sub) \n\nchile_rs_final %&gt;% \n  tidy(conf.int=T,) %&gt;% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %&gt;% \n  mutate(term = c(\"Intercept\", \n                  \"In program\", \n                  \"Assignment variable\",\n                  \"Reading score in 1988\",\n                  \"Math score in 1988\")) %&gt;% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      digits = 2) \n\n\n\n\nTable 2: Effect of being in policy program, covariate adjusted\n\n\nTerm\nEstimate\nLower bound\nUpper bound\n\n\n\n\nIntercept\n23.35\n14.84\n31.85\n\n\nIn program\n2.08\n0.28\n3.88\n\n\nAssignment variable\n0.14\n-0.22\n0.51\n\n\nReading score in 1988\n0.61\n0.46\n0.75\n\n\nMath score in 1988\n0.16\n-0.01\n0.32\n\n\n\n\n\n\n\n\nAs can be seen, our confidence intervals are now slightly narrower, reflecting the extra information we have included in our analysis.\n\n\nFinal thoughts: plotting your next regression discontinuity analysis\nRegression discontinuity designs (RDD) are a popular way of evaluating policies when randomization b y the researcher is not an option. When doing RDD, we look for naturally occurring cutoffs that split units into a treatment and a control group.\nCutoff variables should be continuous. Finally, be aware that RDD is quasi-experimental. It is not a randomized experiment, and it is on you to prove that the assumptions hold.\n\n\nCool! Where can I learn more?\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press."
  },
  {
    "objectID": "blog/toolbox-rct/index.html",
    "href": "blog/toolbox-rct/index.html",
    "title": "The policy evaluator’s toolbox: Presenting results from randomized experiments",
    "section": "",
    "text": "In this series of blog posts, I go over some of the most common experimental designs, including natural experiments such as difference-in-difference, regression discontinuity designs, and so on.\nThe goal is to provide you with some sensible default plots and tables you can use when writing up your next experimental analysis. For more detail on the technicalities of the individual designs, check out the sources I link to at the bottom of this post.\n\n\nPackages used in this post\n# Packages we'll be using today\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(kableExtra)\nlibrary(broom)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\nToday: RCTs. What better way to kick off a series on experimental designs than with a truly randomized experiment? To get started, I’ll simulate some fake data we can work with. Table 1 shows a peak of the simulated data.\n\n\nCode\nset.seed(2707)\n\nnum_people &lt;- 1000\ntreat_effect &lt;- 4.5\n\ndat &lt;- \n  tibble(\n  age = sample(18:75, num_people, replace = T), \n  female = sample(c(0, 1), num_people, replace = T), \n  non_western = sample(c(0, 1), num_people, \n                       replace = T, prob = c(.85, .15)), \n  condition = sample(0:1, size = num_people, \n                     replace = TRUE),\n  mu = 10 + .5 * age + 8 * female - 10 * non_western + \n    treat_effect * condition + .25 * (age*condition),\n  outcome = rnorm(num_people, mu, 2.5)\n) |&gt; \n  select(-mu)\n\n# Recode into factors \ndat &lt;- \n  dat |&gt; \n  mutate(condition = ifelse(condition == 1, \"Treatment\", \"Control\")) |&gt; \n  mutate(female = ifelse(female == 1, \"Female\", \"Male\")) |&gt;\n  mutate(non_western = ifelse(non_western == 1, \"Non-western\", \"Western\"))\n\nhead(dat) |&gt;\n  kbl(digits = 2)\n\n\n\n\nTable 1: A peak at the simulated data\n\n\nage\nfemale\nnon_western\ncondition\noutcome\n\n\n\n\n47\nFemale\nWestern\nControl\n43.60\n\n\n47\nMale\nWestern\nControl\n32.64\n\n\n44\nFemale\nWestern\nTreatment\n49.03\n\n\n44\nFemale\nWestern\nControl\n40.30\n\n\n63\nMale\nWestern\nControl\n35.46\n\n\n51\nFemale\nWestern\nTreatment\n60.32\n\n\n\n\n\n\n\n\n\nPresenting descriptives\nFirst, let’s make a table to see if our two groups are balanced on average. See Table 2. This would be your typical “Table 1” if you were writing a research paper.\n\n\nCode\ndat_summary &lt;- \n  dat |&gt; \n  select(\n    Age = age,\n    Female = female,\n    'Non-Western' = non_western,\n    condition,\n    -outcome,\n  )\n  \ndatasummary_balance(~condition, \n                    data = dat_summary)\n\n\n\n\nTable 2: Means of covariates across experimental conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl (N=485)\n\n\nTreatment (N=515)\n\n\n\n\n\n\nMean\nStd. Dev.\nMean\nStd. Dev.\nDiff. in Means\nStd. Error\n\n\n\n\nAge\n\n46.6\n16.7\n47.1\n16.6\n0.6\n1.1\n\n\n\n\nN\nPct.\nN\nPct.\n\n\n\n\nFemale\nFemale\n237\n48.9\n248\n48.2\n\n\n\n\n\nMale\n248\n51.1\n267\n51.8\n\n\n\n\nNon-Western\nNon-western\n72\n14.8\n83\n16.1\n\n\n\n\n\nWestern\n413\n85.2\n432\n83.9\n\n\n\n\n\n\n\n\n\n\nAlternatively, you could reserve that table for an appendix and instead present a table showing descriptives of the full sample. Table 3 shows an example.\n\n\nCode\ndat_summary &lt;- \n  dat |&gt; \n  select(\n    Age = age,\n    Female = female,\n    'Non-Western' = non_western,\n    Condition = condition,\n    -outcome,\n  )\n\ndatasummary_balance(~1, \n                    data = dat_summary)\n\n\n\n\nTable 3: Means of covariates for the full sample\n\n\n\n\nMean\nStd. Dev.\n\n\n\n\nAge\n\n46.9\n16.6\n\n\n\n\nN\nPct.\n\n\nFemale\nFemale\n485\n48.5\n\n\n\nMale\n515\n51.5\n\n\nNon-Western\nNon-western\n155\n15.5\n\n\n\nWestern\n845\n84.5\n\n\nCondition\nControl\n485\n48.5\n\n\n\nTreatment\n515\n51.5\n\n\n\n\n\n\n\n\n\n\nPresenting results\nAfter presenting evidence that your randomization was successful, you should move on to present your results. This could be either as a regression table (Table 4) or as a figure (Figure 1).\n\n\nCode\nmodel_basic &lt;-\n  lm(outcome ~ condition, data = dat)\n\nmodel_cov_adj &lt;-\n  lm(outcome ~ . + age*condition, data = dat)\n\nmodelsummary(models = list(\n  \"Unadjusted\" = model_basic,\n  \"Covariate adjusted\" = model_cov_adj),\n  coef_rename = c(\n    \"age\" = \"Age (in years)\",\n    \"femaleMale\" = \"Sex: Male\",\n    \"conditionTreatment\" = \"Condition: Treatment\",\n    \"non_westernWestern\" = \"Origin: Western\",\n    \"age:conditionTreatment\" = \"Age x Treatment\")\n  )\n\n\n\n\nTable 4: Outcome across experimental conditions (OLS estimates)\n\n\n\nUnadjusted\nCovariate adjusted\n\n\n\n\n(Intercept)\n35.641\n8.141\n\n\n\n(0.553)\n(0.394)\n\n\nCondition: Treatment\n16.548\n4.899\n\n\n\n(0.770)\n(0.479)\n\n\nAge (in years)\n\n0.501\n\n\n\n\n(0.007)\n\n\nSex: Male\n\n−7.975\n\n\n\n\n(0.160)\n\n\nOrigin: Western\n\n9.660\n\n\n\n\n(0.221)\n\n\nAge x Treatment\n\n0.245\n\n\n\n\n(0.010)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.316\n0.971\n\n\nR2 Adj.\n0.315\n0.971\n\n\nAIC\n7840.9\n4697.8\n\n\nBIC\n7855.6\n4732.1\n\n\nLog.Lik.\n−3917.440\n−2341.879\n\n\nRMSE\n12.16\n2.52\n\n\n\n\n\n\n\n\nI personally prefer Figure 1 because in addition to showing the model (i.e. the two group averages), it also conveys information about the full distribution of data for both groups.\n\n\nCode\nmeans &lt;-\n  dat %&gt;%\n  group_by(condition) %&gt;%\n  summarise(avg = mean(outcome),\n            sd = sd(outcome)) \n\navg_control &lt;-\n  means %&gt;%\n  filter(condition == \"Control\") %&gt;%\n  select(avg)\n\ndat %&gt;%\n  ggplot(aes(condition, outcome)) +\n  geom_jitter(aes(color = condition), alpha = .4) +\n  geom_hline(yintercept = avg_control$avg, linetype = \"dashed\", color = \"grey50\") +\n  geom_pointrange(data = means, aes(x = condition, \n                                    y = avg, \n                                    ymin = avg + sd, \n                                    ymax = avg - sd)) +\n  labs(color = \"Condition\", \n       x = NULL, \n       y = \"Outcome\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 1: Outcome across experimental conditions\n\n\n\n\n\n\nPresenting interactions\nFinally, we might be interested in plotting some type of interaction with our treatment. The basic recipe is to define some values for each of the predictors and then use those to predict the outcome. We can use augment() from the broom package to do so.\n\n\nCode\npred_matrix &lt;- \n  crossing(\n    condition = c(\"Control\", \"Treatment\"),\n    age = 15:75,\n    female = \"Female\",\n    non_western = \"Western\"\n  )\n\npreds &lt;-\n  augment(model_cov_adj, newdata = pred_matrix, se_fit = TRUE) |&gt; \n  mutate(.lwr = .fitted - 1.96 *.se.fit,\n         .upr = .fitted + 1.96 *.se.fit)\n\n\nWith our predictions, we can make a basic graph as in Figure 2.\n\n\nCode\npreds |&gt; \n  ggplot(aes(age, .fitted, color = condition)) + \n  geom_line() +\n  geom_line(aes(y = .lwr), lty = \"dashed\", linewidth = .25) + \n  geom_line(aes(y = .upr), lty = \"dashed\", linewidth = .25) + \n  scale_color_brewer(palette = \"Set1\") + \n  geom_point(data = dat, aes(age, outcome), alpha = .1) + \n  labs(x = \"Age (in years)\",\n       y = \"Outcome\") + \n  annotate(geom = \"text\", label = \"Treated units\", x = 70, y = 65, color = \"#377eb8\") + \n  annotate(geom = \"text\", label = \"Control units\", x = 70, y = 45, color = \"#e41a1c\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 2: Comparing treatment effects across age for treated and non-treated units\n\n\n\n\nNotice how this graphs the predicted outcome across age but for Western females only. Alternatively, we could average over predictions from different subgroups to calculate the marginal effects. The marginaleffects package has a nice suite of functions to help us do just that.\n\n\nFinal thoughts: Presenting results from randomized experiments\nRCTs are commonly used to test for causal relationships and knowing the basics in terms of presentation is therefore essential. In this post, I have shown how to use R to calculate and present some of the most common tables and figures used for RCTs. Now you’re ready to present your next RCT!\n\n\nCool! Where can I learn more?\n\nCoppock, A. (2021). Visualize as you randomize. Advances in experimental political science.\nHealy, K. (2018). Data visualization: a practical introduction. Princeton University Press."
  },
  {
    "objectID": "blog/bayesian-rct-spec-priors/index.html",
    "href": "blog/bayesian-rct-spec-priors/index.html",
    "title": "Bayesian analysis of a randomized controlled trial I: Specifying the priors",
    "section": "",
    "text": "Welcome to the first part of my three-part series on how to analyze randomized controlled trials (RCTs) using Bayesian thinking and methods. For this first post, I assume you have some basic understanding about what it means to be “working like a Bayesian”. If you are entirely new to Bayesian methods, check out the references at the bottom of the post.\nPackages used in this post\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(modelsummary)\nlibrary(kableExtra)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(janitor)\n\ntheme_set(theme_minimal(base_size = 12))"
  },
  {
    "objectID": "blog/bayesian-rct-spec-priors/index.html#footnotes",
    "href": "blog/bayesian-rct-spec-priors/index.html#footnotes",
    "title": "Bayesian analysis of a randomized controlled trial I: Specifying the priors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice that although this is well-suited for our purpose, it actually represents a sub-optimal research design because we have discretized a variable for which it is fair to assume we would have a continuous measure, for example a test score for each student. By working with a dichotomous outcome, we are throwing away valuable information. Table↩︎"
  },
  {
    "objectID": "blog/dot-dot-dot/index.html",
    "href": "blog/dot-dot-dot/index.html",
    "title": "Dot-dot-dot: Three dotplots to stake your life on",
    "section": "",
    "text": "Here is a quick post on a few basic plots you can make with geom_point. The goal is simply to celebrate the dot as a way of graphing data. Who knows? Maybe someday there will be a post on the line too! For all plots, we will be working with the gapminder data.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(ggsci)\nlibrary(gapminder)\n\ntheme_set(theme_minimal())\ndata(\"gapminder\")\n\n\n\nPlot 1: The Cleveland dotplot\nTo get started, let’s make a classic Cleveland dot plot. These are good for providing at quick glance of summary statistics, here the average life expectancy for each continent:\n\n\nCode\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean = mean(lifeExp, na.rm = T)) %&gt;%\n  mutate(continent = fct_reorder(continent, mean)) %&gt;%\n  ggplot(aes(mean, continent)) + \n  geom_point(size = 4) +\n  labs(x = \"Years\",\n       y = NULL,\n       caption = \"Source: gapminder\")\n\n\n\n\n\nFigure 1: Mean life expectancy across continents\n\n\n\n\nOne small tweak could be to highlight one of the continents. For instance, suppose your analysis was about explaining what’s going on with one particular observation. In particular, when looking at the above plot, Africa, seems to be falling behind the other continents. An easy way to highlight that would be using color:\n\n\nCode\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  summarise(mean = mean(lifeExp, na.rm = T)) %&gt;%\n  mutate(continent = fct_reorder(continent, mean)) %&gt;%\n  ggplot(aes(mean, continent)) + \n  geom_point(aes(color = I(ifelse(continent == \"Africa\", \n                                  \"firebrick\", \n                                  \"black\"))), \n             size = 4) +\n  labs(x = \"Years\",\n       y = NULL, \n       caption = \"Source: gapminder\")\n\n\n\n\n\nFigure 2: What’s the matter with Africa? Mean life expectancy across continents\n\n\n\n\nThis kind of graph is en easy but powerfull way to spark initial curiosity with readers, which is a key part of storytelling with data\n\n\nPlot 2: The dot-and-whisker\nNext up, we have the dot-and-whisker plot. Often, researchers and data analyst will report model estimates using tables. Some scholars, teasingly, call these ‘BUTONs’: Big Ugly Table of Numbers.\nA simple way to improve presentation is to make a dot-and-whisker plot of the model estimates and corresponding uncertainties. Let’s fit a model and make a plot:\n\n# Rescale population and GDP per capita to more meaningfull scale. \ngapminder &lt;- \n  gapminder %&gt;% \n  mutate(population_scaled = pop/100000000,\n         gdpPercap_scaled = gdpPercap/10000)\n\n# The fit model\ngap_model &lt;- \n  lm(lifeExp ~ continent + year +  population_scaled + gdpPercap_scaled, \n     data = gapminder)\n\n\n\nCode\ngap_model %&gt;%\n  tidy(conf.int = T) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = str_replace_all(term, \n                                \"continent\", \n                                \"Continent: \"),\n         term = str_to_title(term),\n         term = fct_reorder(term, \n                            estimate)) %&gt;%\n  ggplot(aes(estimate, \n             term, \n             xmin = conf.low, \n             xmax = conf.high)) + \n  geom_vline(xintercept = 0, \n             linetype = \"dashed\",\n             color = \"grey\") + \n  geom_pointrange() + \n  labs(y = NULL, \n       x = \"Difference in years\")\n\n\n\n\n\nFigure 3: Explaining life expectancy across the world\n\n\n\n\nFor reference, this is what the same results would have looked like if we had used a standard regression table:\n\n\nCode\nmodelsummary(models = list(\"Life expectancy\" = gap_model), \n             fmt = fmt_significant(2),\n             stars = T,\n             gof_map = c(\"nobs\", \"rmse\",\"r.squared\"))\n\n\n\n\nTable 1: Explaining life expectancy\n\n\n\nLife expectancy\n\n\n\n\n(Intercept)\n−518***\n\n\n\n(20)\n\n\ncontinentAmericas\n14.29***\n\n\n\n(0.49)\n\n\ncontinentAsia\n9.38***\n\n\n\n(0.47)\n\n\ncontinentEurope\n19.36***\n\n\n\n(0.52)\n\n\ncontinentOceania\n20.6***\n\n\n\n(1.5)\n\n\nyear\n0.29***\n\n\n\n(0.01)\n\n\npopulation_scaled\n0.18\n\n\n\n(0.16)\n\n\ngdpPercap_scaled\n3.0***\n\n\n\n(0.2)\n\n\nNum.Obs.\n1704\n\n\nRMSE\n6.87\n\n\nR2\n0.717\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nThe dot-and-whisker plot helps readers quickly compare the model estimates and their corresponding uncertainty.\n\n\nPlot 3: The dumbell chart\nFinally, the dumbell chart can be useful when the goal is to highlight differences, especially across groups. Let’s make one to compare the change in mean life expectancy from the 1950s to the 2000s across all five continents:\n\n\nCode\ngapminder |&gt; \n  mutate(decade = factor(year %/% 10 * 10)) |&gt; \n  filter(decade %in% c(1950, 2000)) |&gt; \n  group_by(continent, decade) |&gt; \n  summarize(lifeexp = mean(lifeExp, na.rm = T)) |&gt; \n  mutate(continent = fct_reorder(continent, lifeexp)) |&gt; \n  ggplot(aes(lifeexp, continent, \n             group = decade)) + \n  geom_line(aes(group = continent), \n            color = \"grey80\", \n            linewidth = 1) + \n  geom_point(aes(color = continent), \n             size = 5) + \n  scale_color_brewer(type = \"qual\", palette = \"Set1\") + \n  labs(x = \"Life expectancy (years)\",\n       y = NULL,\n       caption =\"Source: gapminder\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 4: Mean life expectancy in 1950s and 2000s\n\n\n\n\nThe graph makes it easy to see that Oceania has the highest average but Asia has seen the highest growth. This might be a useful “Figure 1”; illustrating a puzzle which your paper then tries to explain.\n\n\nFinal thoughts: Three dotplots to stake your life on\nPoints or dots can be a useful way of plotting data. In this post, I have shown you three different and quite versatile plots you can use in your next analysis.\n\n\nCool! Where can I learn more?\n\nHealy, Kieran. Data visualization: a practical introduction. Princeton University Press, 2018."
  },
  {
    "objectID": "blog/bayesian-rct-validation/index.html",
    "href": "blog/bayesian-rct-validation/index.html",
    "title": "Bayesian analysis of a randomized controlled trial II: Defining and validating the model",
    "section": "",
    "text": "Welcome to the second post in my brief series on getting started with Bayesian modeling in R. In my last post, we covered specifying the priors to take into account any prior knowledge.\nToday, we’ll try to validate the models we built. As we are working with logistic regression, we’ll focus on two questions Johnson, Ott, and Dogucu (2022):\n\nHave our simulation stabilized?\n\nHow wrong is the model?\nHow accurate are the model’s posterior classifications?\n\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesrules)\nlibrary(tidybayes)\nlibrary(bayesplot)\nlibrary(kableExtra)\nlibrary(patchwork)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\nRecall our two models from my previous post:\n\nIn one model, I used a weakly informative prior\nIn another, model I used an evidence-based prior\n\n\nCheck #1: Have our simulations stabilized?\nBefore moving on, we should check the stability of our simulations.\nThis is easy to do using mcmc_trace from the bayesrules packages.\nLet’s first check the model using a weakly informative prior:\n\n\nCode\n# MCMC trace, density, & autocorrelation plots - weakly informative prior \nmcmc_trace(fail_model_weakinf) + scale_x_continuous(breaks = c(0, 5000))\nmcmc_dens_overlay(fail_model_weakinf)\nmcmc_acf(fail_model_weakinf)\n\n\n\n\n\n\n\n\n(a) Traceplot\n\n\n\n\n\n\n\n\n\n(b) Density of posterior simulated coefficients\n\n\n\n\n\n\n\n\n\n(c) Autocorrelation\n\n\n\n\nFigure 1: Diagnostic plots for the stability of our simulation results concerning the model using weakly informative priors. All looks good.\n\n\n\nAll looks good. For the evidence based model…\n\n\nCode\n# MCMC trace, density, & autocorrelation plots - evidence based model\nmcmc_trace(fail_model_evidence) + scale_x_continuous(breaks = c(0, 5000))\nmcmc_dens_overlay(fail_model_evidence)\nmcmc_acf(fail_model_evidence)\n\n\n\n\n\n\n\n\n(a) Traceplot\n\n\n\n\n\n\n\n\n\n(b) Density of posterior simulated coefficients\n\n\n\n\n\n\n\n\n\n(c) Autocorrelation\n\n\n\n\nFigure 2: Diagnostic plots for the stability of our simulation results concerning the model using evidencebased priors. All looks good.\n\n\n\nsame.\nAll set, let’s get on with model validation!\n\n\nCheck #2: How well does the model fit the data?\nTo see this, we simulate 100 data sets from our posterior distribution. For each data set, we then calculate the number of failed tests to see if this matches up with that of the original data.\n\n\nCode\ncalc_prop_fail &lt;- function(x) {mean(x == 1)\n}\n\n\n\n\nCode\npp_check_model_weakinf &lt;- \n  pp_check(fail_model_weakinf, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_evidence &lt;-\n  pp_check(fail_model_evidence, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_weakinf / pp_check_model_evidence + plot_annotation(tag_levels = 'A')\n\n\n\n\n\nPosterior predictve checks of the two models. Histograms show the proportion of failed students in a number of simulated datasets based on each model, whereas the dark blue lines mark the real proportion of failed students in the data. (a) With a weakly informative prior, the simulated data sets are unwilling to say much about the share of failed reading tests. (b) In contrast, using an evidence-based prior results in posterior simulations that resemble the original data well.\n\n\n\n\n\n\nCheck #3: How well does the model fit new data?\nBecause we are working with a categorical outcome, we can be either right or wrong. The question is how often are we right?\n\n\nCode\nset.seed(0407)\n\nclass_sum_weakinf &lt;- \n  classification_summary(model = fail_model_weakinf, data = fake, cutoff = 0.5) \n\nclass_sum_evidence &lt;- \n  classification_summary(model = fail_model_evidence, data = fake, cutoff = 0.5) \n\n\n\noverall accuracy captures the proportion of all Y observations that are accurately classified\nsensitivity (true positive rate) captures the proportion of Y = 1 observations that are accurately classified\nspecificity (true negative rate) the proportion of Y = 0 observations that are accurately classified:\n\n\n\n\nHow well do the two models fit the data?\n\n\nMeasure\nWeakly informative priors\nEvidence-based priors\n\n\n\n\nSensitivity (true positive rate)\n0.57\n0.54\n\n\nSpecificity (true negative rate)\n0.43\n0.54\n\n\nOverall accuracy\n0.50\n0.54\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJohnson, Alicia A., Miles Q. Ott, and Mine Dogucu. 2022. “Logistic Regression.” In, 329–54. Chapman; Hall/CRC. https://doi.org/10.1201/9780429288340-13."
  },
  {
    "objectID": "blog/writing-tests/index.html",
    "href": "blog/writing-tests/index.html",
    "title": "We better test that: Writing tests to ensure data quality",
    "section": "",
    "text": "I am working my through the so-far-wonderful book Telling stories with data by Rohan Alexander. One thing I really like about the book is the heavy emphasis on simulation and all the things you can do with that: present results, validate models, plan research designs, and many, many other common tasks when doing research.\nAnother part to like is the discussion on how you should write tests throughout a data science project. You want to make sure your data is what you think it is. For instance, you might think that all your key variables are of the right class - but you should probably test that!\nThis is one thing I would have liked to know more about when I started out doing statistical analysis. Back then, I would often run into the problem that my code would not run1. Often, this was due to tiny issues in the data: a variable that should have been a factor was perhaps coded as a character. Or, in the process of joining multiple datasets, I had lost some observations without noticing.\nThese ‘little things’, make up a huge part of what makes data science occasionally frustrating. And they can undermine an otherwise well-thought and thoroughly-carried-out analysis.\nPackages used in this post\nlibrary(tidyverse)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 14))"
  },
  {
    "objectID": "blog/writing-tests/index.html#footnotes",
    "href": "blog/writing-tests/index.html#footnotes",
    "title": "We better test that: Writing tests to ensure data quality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat still happens! But at least not as often, and now I know better what to do about it. Also, occasionally my code would run - but give me wrong results.↩︎\nIn a real-life study, this type of error should probably make us consider if our survey had been implemented in such a flawed way that we could not use the data at all.↩︎"
  },
  {
    "objectID": "blog/simulation-design-analysis/index.html",
    "href": "blog/simulation-design-analysis/index.html",
    "title": "Using simulation for design analysis: A field guide",
    "section": "",
    "text": "If you’ve ever done a research project, at some point someone has likely asked you the following question: Is your n large enough? What this person is asking concerns design analysis: analyzing how different properties of your design will affect your potential conclusions.\nNot doing design analysis ahead of the actual data collection will cause you problems. For example, you might run an intervention study with no chance of detecting the treatment effect.\nIn this post, I’ll show you a general way to approach design analysis using simulation. As it turns out, using simulation is a rigorous way to think about your research design, and it is far better than what is usually done such as throwing your proposed n into some online ‘power calculator’.\nWith simulation, you can study all relevant properties of your design; including n, minimum detectable treatment effect size, standard error of the treatment effect, statistical power of the study, and anything else you might think would be nice to know before running your study.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\n\nHow to use simulation for design analysis: four steps\nThe workflow I propose goes something like this:\n\nCarefully consider any prior assumptions about the future data (sample size, treatment effect, etc.),\nsimulate one dataset based on those assumptions,\nturn your simulation into a function and then repeat it a bunch of times to simulate many dataset,\nstudy the properties of those datasets using summary statistics and/or graphs.\n\nSimple as that.\nLet us work our way through a running example. Suppose we were interested in running a randomized experiment with the aim of improving student test scores.\n\n\nStep #1: Specify prior assumptions\nBased on previous studies, we think the treatment effect could be about five. Previous studies have used samples of about 100 students, so we think this could be a good place for us to start. Also, students in general usually have had test scores with a mean of 60 plus or minus about 20 points.\n\n\nStep #2: Simulate one dataset\nLet us simulate one dataset based on those assumptions:\n\nset.seed(2608)\n\nn_subjects &lt;- 100\ntreat_effect &lt;- 5\n\ny_if_control &lt;- rnorm(n_subjects, 60, 20)\ny_if_treatment &lt;- y_if_control + treat_effect\n\ndat &lt;- tibble(\n  condition = sample(x = rep(c(\"Control\", \"Treated\"), n_subjects/2), \n                     size = n_subjects, \n                     replace = TRUE),\n  outcome = ifelse(condition == \"Control\", y_if_control, y_if_treatment)\n  )\n\nWe can fit an initial model to our simulated data:\n\n\nCode\nmod_1 &lt;- \n  summary(lm(outcome ~ condition, data = dat))\n\nmodelsummary(models = list(\n  \"Model 1\" = mod_1),\n  fmt = fmt_significant(2),\n  stars = TRUE,\n  title = \"Estimated treatment effect from a randomized controlled trial with 100 subjects, take 1\")\n\n\n\nEstimated treatment effect from a randomized controlled trial with 100 subjects, take 1\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n62***\n\n\n\n(3)\n\n\nconditionTreated\n1.4\n\n\n\n(4.1)\n\n\nNum.Obs.\n100\n\n\nR2\n0.001\n\n\nR2 Adj.\n−0.009\n\n\nRMSE\n20.26\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nLooking at the results, 100 subjects for the experiment should make us worry. For one thing, the estimated treatment effect is much smaller than the true treatment effect, which we set to 5. In addition, looking at the standard error of 4.1, is seems impossible that we could ever hope to detect a treatment effect of 5.\nWhat would happen if we were to run the simulation again, only this time setting a different seed?\n\nset.seed(3009)\n\ny_if_control &lt;- rnorm(n_subjects, 60, 20)\ny_if_treatment &lt;- y_if_control + treat_effect\n\ndat_2 &lt;- \n  tibble(\n  condition = sample(x = rep(c(\"Control\", \"Treated\"), n_subjects/2), \n                     size = n_subjects, \n                     replace = TRUE),\n  outcome = ifelse(condition == \"Control\", y_if_control, y_if_treatment)\n  )\n\n\n\nCode\nmod_2 &lt;- \n  summary(lm(outcome ~ condition, data = dat_2))\n\nmodelsummary(models = list(\n  \"Model 1\" = mod_1,\n  \"Model 2\" = mod_2),\n  fmt = fmt_significant(2),\n  stars = TRUE,\n  title = \"Estimated treatment effect from a randomized controlled trial with 100 subjects, take 2\")\n\n\n\nEstimated treatment effect from a randomized controlled trial with 100 subjects, take 2\n\n\n\nModel 1\n Model 2\n\n\n\n\n(Intercept)\n62***\n59.9***\n\n\n\n(3)\n(3.4)\n\n\nconditionTreated\n1.4\n−1.1\n\n\n\n(4.1)\n(4.7)\n\n\nNum.Obs.\n100\n100\n\n\nR2\n0.001\n0.001\n\n\nR2 Adj.\n−0.009\n−0.010\n\n\nRMSE\n20.26\n23.50\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nThis time we get a very different estimate of the treatment effect. Clearly, our the results from a 100 subject experiment is not very trustworthy, given our assumptions.\n\n\nStep #3: Turn the simulation into a function and repeat it\nRather than keep making new datasets “by hand”, we can write a function to systematically study the variation across resamples.\nTo set up the simulation for replication, we first turn our initial simulation into a function:\n\nsim_my_data &lt;- function(n_subjects = 100, treat_effect = 5) {\n  \n  y_if_control &lt;- rnorm(n_subjects, 60, 20)\n  y_if_treatment &lt;- y_if_control + treat_effect\n  \n  tibble(\n    condition = sample(x = rep(c(\"Control\", \"Treated\"), n_subjects/2), \n                     size = n_subjects, \n                     replace = TRUE),\n    outcome = ifelse(condition == \"Control\", y_if_control, y_if_treatment)\n  )\n}\n\nLet’s make sure that our function does what we intend it to do:\n\n\nCode\nsim_my_data() |&gt; \n  head()\n\n\n# A tibble: 6 × 2\n  condition outcome\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Control     107. \n2 Treated      52.2\n3 Treated      84.5\n4 Control      85.5\n5 Control      38.8\n6 Treated      64.5\n\n\nLooks right. Now we can use map() to repeat our simulation 1,000 times:\n\nmany_sims &lt;- \n  tibble(\n    trial = 1:1000,\n    sim_dat = map(trial, ~sim_my_data())\n  )\n\nAnd instead of fitting the model to just one simulated dataset, we can fit 1,000 models - one for each dataset. This will help us study the variation in the quantities we are interested in:\n\nmany_sims &lt;- \n  many_sims |&gt; \n  mutate(model = map(sim_dat, ~lm(outcome ~ condition, data =.)))\n\nNow we have a list of simulated datasets with a model fit to each.\n\n\n\nFigure 1: 1,000 dataatses and 1,000 models\n\n\nAs a final step, let’s make two new list colums:\n\none using tidy() which will store our model estimates, and\none using glance which will store information about each model, such as R^2 etc.\n\n\nmany_sims &lt;- \n  many_sims |&gt; \n  mutate(tidied = map(model, tidy),\n         glanced = map(model, glance))\n\nFinally, we can unnest our tidied and glanced list columns so they are easier to work with:\n\nmany_sims_unnested &lt;- \n  many_sims |&gt; \n  unnest(.cols = tidied) |&gt; \n  unnest(.cols = glanced)\n\n\n\nStep #4: Use the simulated datasets to answer questions about the design\nWith our datasets and models in place, we can start interrogating the design. For example, what can we say about the standard error of the treatment?\nOur two initial models suggested that the standard error was about 4, but we can study its variation more systematically:\n\n\nCode\nmany_sims_unnested |&gt; \n  filter(term == \"conditionTreated\") |&gt; \n  select(\"std.error\") |&gt; \n  ggplot(aes(std.error)) + \n  geom_histogram(fill = \"firebrick\", color = \"white\", bins = 75) +\n  labs(x = \"Standard error\",\n       y = \"Count\")\n\n\n\n\n\nFigure 2: Histogram of 1,000 simulated standard errors of the treatment effect when n = 100\n\n\n\n\nAs Figure 2 reveals, our standard error would range between about 3.5 and 4.5. If that is enough for our purpose, then fine. But being that we expect a treatment effect of about 5, we should not set our hopes up for achieving any “statistical significance”.\nTo get statistical significance, the standard error needs to be less than half the treatment effect, i.e. &lt;2.5. To be on the safe side, say we were going for a standard error of 2. In that case, since the standard error drops with the square root of the sample size, we would need about 400 subjects.\n\n\nOpen to see workflow for n = 400\nnew_sims &lt;- \n  tibble(\n    trial = 1:1000,\n    sim_dat = map(trial, ~sim_my_data(n_subjects = 400))\n  ) |&gt; \n  mutate(model = map(sim_dat, ~lm(outcome ~ condition, data =.))) |&gt; \n  mutate(tidied = map(model, tidy)) |&gt; \n  unnest(.cols = tidied)\n\nnew_sims |&gt; \n  filter(term == \"conditionTreated\") |&gt; \n  select(\"std.error\") |&gt; \n  ggplot(aes(std.error)) + \n  geom_histogram(fill = \"firebrick\", color = \"white\", bins = 75) +\n  labs(x = \"Standard error\",\n       y = \"Count\")\n\n\nAnother quantity we might be interest in is the p-value across simulations. Recall, that we defined a treatment effect of 5, so we know there is an effect of five. How likely is it that we would arrive at this result if we define a threshold of p &lt; 0.05?\n\n\nCode\nmany_sims_unnested |&gt; \n  filter(term == \"conditionTreated\") |&gt; \n  summarize(\n    stat_power = mean(p.value &lt; 0.05)\n  )\n\n\n# A tibble: 1 × 1\n  stat_power\n       &lt;dbl&gt;\n1      0.248\n\n\nIn statistical parlance, this quantity is known as the statistical power: the likelihood of a hypothesis test detecting a true effect if there is one. With 100 subjects, we get a power of about 25 percent - much too low if our goal is to learn anything. Put another way, we incorrectly reject the true treatment effect about 75 percent of the time.\nIn case we wanted to, we could also make a plot:\n\n\nCode\nmany_sims_unnested |&gt; \n  filter(term == \"conditionTreated\") |&gt; \n  select(\"p.value\") |&gt; \n  mutate(signif = ifelse(p.value &lt;= 0.05, \"Significant\", \"Not significant\")) |&gt; \n  ggplot(aes(p.value)) + \n  geom_histogram(aes(fill = signif), \n                 color = \"white\", \n                 bins = 75) +\n  scale_fill_brewer() + \n  labs(x = expression(paste(italic(p), \"-value\")),\n       y = \"Count\",\n       fill = \"Statistical significance\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3: Simulated p-values when n = 100. There is a true treatment effect of 5. Dark blue marks p-values below the conventional threshold of statistical significance of .05. Only in about 25 percent of the cases would we correctly be able to identify the treatment effect.\n\n\n\n\nLooking at the p-values and the treatment effect standard errors gives tells us something about how well we can trust the estimated treatment effect (not much, it turns out).\nBut we could also study some properties of the overall model. For example, what is the coverage of say, the 68 and 95 percent confidence intervals?\n\n\nCode\nmany_sims_unnested &lt;-\n  many_sims_unnested |&gt;\n  mutate(\n    low_95 = estimate - (1.96 * std.error),\n    high_95 = estimate + (1.96 * std.error),\n    low_50 = estimate - (2/3 * std.error),\n    high_50 = estimate + (2/3 * std.error),\n  )\n\nmany_sims_unnested |&gt;\n  filter(term == \"conditionTreated\") |&gt; \n  summarize(\n    coverage_50 = mean(low_50 &lt;= treat_effect & high_50 &gt;= treat_effect),\n    coverage_95 = mean(low_95 &lt;= treat_effect & high_95 &gt;= treat_effect)\n  )\n\n\n# A tibble: 1 × 2\n  coverage_50 coverage_95\n        &lt;dbl&gt;       &lt;dbl&gt;\n1       0.468       0.965\n\n\nThe coverage looks right, which suggest that at least running a linear regression is a good model for our purpose. In our case of a randomized experiment, this is not all that surprising since the model only needs to fit a very basic two-group comparison data structure. Again we could make a plot:\n\n\nCode\nmany_sims_unnested |&gt;\n  filter(term == \"conditionTreated\") |&gt;\n  slice(1:100) |&gt;\n  ggplot(aes(trial)) +\n  geom_hline(yintercept = treat_effect, color = \"firebrick\") +\n  geom_point(aes(y = estimate)) +\n  geom_linerange(aes(ymin = low_95,\n                     ymax = high_95),\n                 color = \"grey50\") +\n  geom_linerange(aes(ymin = low_50,\n                     ymax = high_50),\n                 linewidth = 1) +\n  geom_point(aes(y = estimate)) +\n  labs(\n    x = \"Simulation\",\n    y = \"Treatment effect\"\n  )\n\n\n\n\n\nFigure 4: 100 estimates of the treatment effect with 50% and 95% confidence intervals calculated using simulation. If the model is correct, then about 50 percent of the 50%-intervals and 95 percent of the 95%-intervals will contain the true treatment effect (in this case 5)\n\n\n\n\nFinally, let’s also have a look at how R2 behaves across simulations:\n\n\nCode\nround(summary(many_sims_unnested$r.squared), 2)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.02    0.03    0.04    0.19 \n\n\nOverall, pretty low (as we should expect). But notice the outliers. Let’s make a final plot to summarize our simulations:\n\n\nCode\nmany_sims_unnested |&gt; \n  ggplot(aes(r.squared)) +\n  geom_histogram(fill = \"lightblue\", \n                 color = \"white\", \n                 bins = 75) +\n  scale_fill_brewer() + \n  labs(x = expression(R^2),\n       y = \"Count\") \n\n\n\n\n\nFigure 5: Distribution of r squared values from 1,000 simulations. By far, most values\n\n\n\n\n\n\nFinal thoughts: Using simulation for design analysis\nSimulation is an incredibly useful way of studying the properties of of a research design. Once the resamples are made, you can study basically any property of the design: power, standard errors, minimum detectable effect size, etc.\n[In this post, I have shown you how to study the consequences of manipulating one feature of the design at a time. In my follow-up post, I show how to simulate datasets for multiple sample sizes, potential treatment effects etc.]\n\n\nCool! Where can I learn more?\n\nGelman, Hill, and Vehtari. (2020). Regression and Other Stories. Cambridge University Press.\nAlexander, R. (2023). Telling Stories with Data: With Applications in R. CRC Press."
  },
  {
    "objectID": "blog/simulation-design-analysis-pt2/index.html",
    "href": "blog/simulation-design-analysis-pt2/index.html",
    "title": "Using simulation for design analysis part II: Iterating over multiple values",
    "section": "",
    "text": "In my previous post, I showed how you can use simulation to study the properties of different design choices.\nIn this follow-up post, we’ll expand on those ideas a bit by writing code that will allow us to study several features of the design at once. That is, we’ll simulate data; varying both the number of subjects and the treatment effect size.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\nlibrary(modelsummary)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\nWe begin with the same function as in my previous post. This will simulate one dataset based on the number of subjects and treatment effect size you provide:\n\nsim_my_data &lt;- function(n_subjects = NULL, treat_effect = NULL) {\n  \n  y_if_control &lt;- rnorm(n_subjects, 60, 20)\n  y_if_treatment &lt;- y_if_control + treat_effect\n  \n  tibble(\n    condition = sample(x = rep(c(\"Control\", \"Treated\"), n_subjects/2), \n                     size = n_subjects, \n                     replace = TRUE),\n    outcome = ifelse(condition == \"Control\", y_if_control, y_if_treatment)\n  )\n}\n\n\nIterating over multilple features at once\nNow we can iterate that function over several different input values. Let’s say we wanted to study the propteries of our design across the following:\n\nnumber of subjects: 25, 50, 100, 250, 400\ntreatment effect sizes: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10.\n\nThen, using map2() from the purrr packages, the code would look something like this:\n\nset.seed(1409)\n\ndat &lt;- \n  crossing(\n  trial = 1:500,\n  n_subjects = c(25, 50, 100, 250, 400),\n  treat_effect = seq(0, 10, by = 1)\n  ) |&gt; \n  mutate(\n    fake_dat = map2(.x = n_subjects, \n                    .y = treat_effect,\n                    .f = sim_my_data)\n  )\n\nNotice that it takes some time to run - we’re producing 55,000 datasets! If you’re short on time, one option would be to use parallel processing.\nWith our simulations, we can fit a model to each simulated dataset before using tidy() to extract the information we need:\n\ndat &lt;- \n  dat |&gt; \n    mutate(model = map(fake_dat, ~lm(outcome ~ condition, data =.)),\n           tidied = map(model, tidy))\n\nFinally, we’ll unnest the tidied results so that we can work with them:\n\ndat &lt;- \n  dat |&gt; \n  unnest(.cols = tidied)\n\n\n\nUsing the simulations to study the proporties of different design features\nNow we are ready to interrogate our design.\nLet us use our simulations to calculate the power associated with different numbers of subjects and different treatment effects sizes.\n\npower_rs &lt;- \n  dat |&gt; \n  group_by(n_subjects, treat_effect) |&gt; \n  summarize(power = mean(p.value &lt;= 0.05))\n\npower_rs \n\n# A tibble: 55 × 3\n# Groups:   n_subjects [5]\n   n_subjects treat_effect power\n        &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1         25            0 0.521\n 2         25            1 0.524\n 3         25            2 0.533\n 4         25            3 0.527\n 5         25            4 0.539\n 6         25            5 0.536\n 7         25            6 0.551\n 8         25            7 0.561\n 9         25            8 0.58 \n10         25            9 0.573\n# ℹ 45 more rows\n\n\nAnd let’s make a plot to see how things behave:\n\n\nCode\npower_rs |&gt; \n  mutate(n_subjects = as.factor(n_subjects)) |&gt; \n  ggplot(aes(treat_effect, power)) +\n  geom_line(aes(color = n_subjects)) +\n  scale_color_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent_format()) + \n  labs(x = \"Potential treatment effects\",\n       y = \"Statistical power\",\n       color = \"Number of subjects\") + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 1: Statistical power for different potential designs, varying both the treatment effect and the number of participating subjects.\n\n\n\n\nWe could also compute the standard error of the treatment effects to analyze its variation:\n\nrs &lt;-\n  dat |&gt; \n  filter(term == \"conditionTreated\") |&gt; \n  group_by(n_subjects, treat_effect) |&gt; \n  summarize(error = mean(std.error))\n\nrs\n\n# A tibble: 55 × 3\n# Groups:   n_subjects [5]\n   n_subjects treat_effect error\n        &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1         25            0  8.12\n 2         25            1  8.07\n 3         25            2  8.01\n 4         25            3  8.09\n 5         25            4  8.11\n 6         25            5  8.01\n 7         25            6  8.04\n 8         25            7  8.10\n 9         25            8  8.16\n10         25            9  8.16\n# ℹ 45 more rows\n\n\nThere is a clear advantage of increasing n but the simulation also reminds us that the standard error does not depend on the treatment effect size:\n\n\nCode\nrs |&gt; \n  mutate(n_subjects = as.factor(n_subjects)) |&gt; \n  ggplot(aes(treat_effect, error)) +\n  geom_line(aes(color = n_subjects)) +\n  scale_color_brewer(palette = \"Set1\") + \n  labs(x = \"Potential treatment effects\",\n       y = \"Standard error\",\n       color = \"Number of subjects\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2: Standard effor of the treatment effect for different potential designs, varying both the treatment effect size and the number of participating subjects.\n\n\n\n\nFinal note: When wring up this post, I first tried to simulate data for the case of only 10 subjects. This caused issues with running the code, because with only 10 subjects there is a significant chance that in some of your simulations, all subjects will be either treatment or control. This is not just an annoyance but a useful reminder and a good example of how trying to simulate data will make you aware of issues you had not even considered.\n\n\nFinal thoughts: Using simulation for design analysis\nUsing simulation is incredibly useful for many things. In this post, I have shown you how you can use simulating to study how assumptions about e.g. the treatment effect will affect your conclusions as well as the consequences of making different design choices (e.g. increasing n).\n\n\nCool! Where can I learn more?\n\nGelman, Hill, and Vehtari. (2020). Regression and Other Stories. Cambridge University Press.\nAlexander, R. (2023). Telling Stories with Data: With Applications in R. CRC Press."
  },
  {
    "objectID": "blog/internal-validation/index.html",
    "href": "blog/internal-validation/index.html",
    "title": "Posterior predictive checking: Comparing observed data to replicated datasets from the model",
    "section": "",
    "text": "What’s the problem?\nWhat are the basic assumptions of any statistical model? (measurement validity, functional form etc.)\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(MASS)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(modelsummary)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\nLet’s set up a basic model\n\n\nCode\ndata(\"Animals\")\n\nanimals &lt;- Animals |&gt; \n  mutate(specie = rownames(Animals))\n\nrm(Animals)\n\n\nFit a first (naive) model\n\n\nCode\nanimals |&gt; \n  ggplot(aes(body, brain)) +\n  geom_point() \n\n\n\n\n\n\n\nCode\nanimals |&gt; \n  ggplot(aes(body, brain)) +\n  geom_point() +\n  scale_x_log10() + \n  scale_y_log10() + \n  geom_smooth(method = \"lm\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\nanimals &lt;- \n  animals |&gt; \n  mutate(brain_log = log(brain),\n         body_log = log(body))\n\n\n\nBy hand\n\n\nCode\nmod_lm &lt;- \n  lm(brain_log ~ body_log, data = animals)\n\n\n\n\nCode\ntidy(mod_lm)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    2.55     0.413       6.18 0.00000153\n2 body_log       0.496    0.0782      6.35 0.00000102\n\n\n\n\nCode\nglance(mod_lm)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.608         0.593  1.53      40.3 0.00000102     1  -50.6  107.  111.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nCode\nlibrary(broom)\n\nanimals_aug &lt;- \n  augment(mod_lm)\n\n\n\n\nCode\nanimals_aug |&gt; \n  ggplot(aes(.fitted, .std.resid)) + \n  geom_hline(yintercept = 0, color = \"grey50\", lty = \"dashed\") + \n  geom_point() + \n  labs(x = \"Predicted values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\nThe easy way: Use rstanarm\n\n\nCode\nmod_stan &lt;-\n  stan_glm(brain_log ~ body_log, \n           prior_intercept = normal(location = 0, scale = 1, autoscale = F),\n           prior = normal(location = 0, scale = 1, autoscale = F),\n           data = animals)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000102 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.02 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.065934 seconds (Warm-up)\nChain 1:                0.068123 seconds (Sampling)\nChain 1:                0.134057 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.083251 seconds (Warm-up)\nChain 2:                0.105649 seconds (Sampling)\nChain 2:                0.1889 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.108 seconds (Warm-up)\nChain 3:                0.070644 seconds (Sampling)\nChain 3:                0.178644 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.9e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.082429 seconds (Warm-up)\nChain 4:                0.088401 seconds (Sampling)\nChain 4:                0.17083 seconds (Total)\nChain 4: \n\n\n\n\nCode\nmodelsummary(mod_stan)\n\n\nWarning: \n`modelsummary` uses the `performance` package to extract goodness-of-fit\nstatistics from models of this class. You can specify the statistics you wish\nto compute by supplying a `metrics` argument to `modelsummary`, which will then\npush it forward to `performance`. Acceptable values are: \"all\", \"common\",\n\"none\", or a character vector of metrics names. For example: `modelsummary(mod,\nmetrics = c(\"RMSE\", \"R2\")` Note that some metrics are computationally\nexpensive. See `?performance::performance` for details.\n This warning appears once per session.\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n2.178\n\n\nbody_log\n0.496\n\n\nNum.Obs.\n28\n\n\nR2\n0.573\n\n\nR2 Adj.\n0.508\n\n\nLog.Lik.\n−52.307\n\n\nELPD\n−55.1\n\n\nELPD s.e.\n3.4\n\n\nLOOIC\n110.2\n\n\nLOOIC s.e.\n6.7\n\n\nWAIC\n109.9\n\n\nRMSE\n1.52\n\n\n\n\n\n\n\n\n\nCode\ny_rep &lt;- posterior_predict(mod_stan)\n\n\nNow we can use some of the neat built-in functions to plot our replications and compare with the observed data.\nOne way would be to use histograms:\n\n\nCode\n# Change plotting theme\ntheme_set(bayesplot::theme_default(base_family = \"sans\"))\n\nppc_hist(animals$brain_log, y_rep[1:19, ])\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOr we could use density curves and make a “spaghetti plot”:\n\n\nCode\nppc_dens_overlay(animals$brain_log, y_rep[1:100, ]) + \n  scale_y_continuous(breaks=NULL)\n\n\n\n\n\nWe can also use built-in functions to compute and visualize different test statistics. For example, let’s make a histogram to plot the 25th percentile of the observed data against those of the replicated datasets:\n\n\nCode\nppc_stat(animals$brain_log, y_rep, stat = function(y) quantile(y, 0.25))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOr we could plot the standard deviation:\n\n\nCode\nppc_stat(animals$brain_log, y_rep, stat = function(y) sd(y))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 1: Standard deviation …\n\n\n\n\nIn both cases, this does not provide that much new information. The mean, mode and median of our replicated datasets are systematically lower than those in the observed data; indicating that there is some room for improvement in our model.\n\n\nFinal thoughts: Using simulation for design analysis\n\n\n\n\n\nCool! Where can I learn more?\n\nGelman, Hill, and Vehtari. (2020). Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "research/extra-role-behavior/index.html",
    "href": "research/extra-role-behavior/index.html",
    "title": "Invisible frontline work: How client characteristics cause extra-role behaviours in public service",
    "section": "",
    "text": "New article in Public Management Review.\nFrontline workers play a vital role in delivering public services, from teachers educating our children to nurses caring for patients. These dedicated individuals are entrusted with the discretion to allocate these services, and how they distribute and prioritize them can have a profound impact on citizens’ lives.\nIn my new study, I explore a new aspect of frontline workers’ roles: how citizen attributes affect frontline workers extra-role behaviors — behaviors where they go above and beyond their formal job descriptions to provide public service.\nFrontline workers frequently exhibit extra-role behaviors by using their own personal resources and time to help clients. For example, nurses may stay after their shifts to care for patients, caseworkers might use their own money to assist clients in need, and teachers make themselves available after hours to support their students. These “extra-role behaviors” are essential to study because they are a significant part of how public service delivery occurs in practice, yet they often go unnoticed.\nTo study this, I did a large-scale conjoint experimental study, surveying 1,507 Danish high school teachers. Teachers were presented with different fictive student profiles and then asked to rate their willingness to engage in various extra-role behaviors.\nI found that teachers were more responsive to students who displayed low well-being or high effort in their classwork. For example, teachers were more willing to stay after their workday was over or to allow a phone call during the weekend if the student frequently handed in assignments or seemed to be having trouble at home.\nAdding nuance to some previous studies on the topic, I also found that teachers become less willing to extend themselves the further requested behaviors are from their job expectations. That is, teachers are willing to go above and beyond for their students —doing something ‘extra’— but they are less willing when that behavior is ‘more extra’. Teachers were largely willing to stay after their workday, for instance, but much less willing to allow a phone call during the weekend.\nHere is a figure from the paper:\n\n\n\nFigure 1: Causal effect of student attributes on teachers’ extra-role behavior\n\n\nYou can download a preprint of the article here."
  },
  {
    "objectID": "blog/toolbox-iv/index.html",
    "href": "blog/toolbox-iv/index.html",
    "title": "The policy evaluator’s toolbox: Presenting your next instrumental variable analysis",
    "section": "",
    "text": "New post for my “policy evaluator’s toolbox” series! Today, let’s have a stab at an instrumental variable analysis.\nAs always, I’ll go over some of the basic intuition but the main focus will be on writing some code to produce a few useful graphs and tables. For more in-depth treatment of the topic, check out the references listed below.\nPackages used in this post\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggsci)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(estimatr)\n\ntheme_set(theme_minimal(base_size = 12))"
  },
  {
    "objectID": "blog/toolbox-iv/index.html#footnotes",
    "href": "blog/toolbox-iv/index.html#footnotes",
    "title": "The policy evaluator’s toolbox: Presenting your next instrumental variable analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCode for this example is largely adapted from Telling Stories with Data↩︎"
  },
  {
    "objectID": "blog/casestudy-tidy-funtime/index.html",
    "href": "blog/casestudy-tidy-funtime/index.html",
    "title": "Tidy Fun",
    "section": "",
    "text": "Sitting at a cafe feeling like doing some data analysis. So, let’s have a look at a recent Tidy Tuesday dataset and see if we can make some fun out of it.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(estimatr)\nlibrary(ggsci)\nlibrary(gt)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\n\nImport\n\n\nCode\ndata(\"mtcars\")\n\nmtcars &lt;- \n  mtcars |&gt; \n  mutate(cyl = as.factor(cyl))\n\n\n\n\nEDA\nMore cylinders, more horsepowers is bad for fuel efficiency.\n\n\nCode\nmtcars |&gt; \n  ggplot(aes(hp, mpg)) + \n  geom_point()\n\n\n\n\n\nCode\nmtcars |&gt; \n  ggplot(aes(as.factor(cyl), mpg)) + \n  geom_boxplot(aes(fill = as.factor(cyl))) +\n  scale_fill_futurama()\n\n\n\n\n\nCode\nmtcars |&gt; \n  ggplot(aes(as.factor(cyl), mpg)) + \n  geom_jitter(aes(color = as.factor(cyl))) +\n  scale_color_futurama()\n\n\n\n\n\n\n\nModel\n\n\nCode\nmod  &lt;- \n  lm_robust(mpg ~ hp + disp + cyl, \n            data = mtcars)\n  \nmod_fixed &lt;- \n  lm_robust(mpg ~ hp + disp, \n            fixed_effects = ~ cyl, \n            data = mtcars)\n  \n\ncoef_names &lt;- \n  c(\"hp\" = \"Gross horsepower\",\n    \"disp\" = \"Displacement\",\n    \"(Intercept)\" = \"Intercept\")\n\nmodelsummary(model = list(\n             \"Without FE\" = mod,\n             \"Fixed Effects\" = mod_fixed), \n             fmt = 3, \n             stars = TRUE, \n             gof_map = c(\"rmse\", \"r.squared\", \"nobs\"),\n             coef_map = coef_names)\n\n\n\n\n\n\nWithout FE\nFixed Effects\n\n\n\n\nGross horsepower\n−0.021+\n−0.021+\n\n\n\n(0.011)\n(0.011)\n\n\nDisplacement\n−0.026**\n−0.026**\n\n\n\n(0.009)\n(0.009)\n\n\nIntercept\n31.148***\n\n\n\n\n(1.775)\n\n\n\nRMSE\n2.65\n2.65\n\n\nR2\n0.800\n0.800\n\n\nNum.Obs.\n32\n32\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicate\n\n\nCode\npredictions(mod, newdata = datagrid(hp = c(100, 110, 120),\n                                    cyl = c(4, 6, 8)),\n            by = \"cyl\") |&gt;   \n  gt() |&gt; \n   fmt_number(\n    decimals = 1,\n    use_seps = FALSE\n  ) |&gt; \n    cols_label(\n    cyl = \"Cylinder\"\n  )\n\n\n\n\n\n\n  \n    \n    \n      Cylinder\n      estimate\n      std.error\n      statistic\n      p.value\n      s.value\n      conf.low\n      conf.high\n    \n  \n  \n    4\n22.8\n1.5\n15.1\n0.0\n169.0\n19.9\n25.8\n    6\n18.8\n0.9\n20.1\n0.0\n294.7\n16.9\n20.6\n    8\n20.4\n1.7\n12.0\n0.0\n107.8\n17.1\n23.7\n  \n  \n  \n\n\n\n\nCode\navg_comparisons(mod, variables = \"cyl\")\n\n\n\n Term Contrast Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n  cyl    6 - 4    -4.05       1.43 -2.837  0.00455 7.8 -6.84  -1.25\n  cyl    8 - 4    -2.43       2.78 -0.874  0.38212 1.4 -7.89   3.02\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nCode\navg_slopes(mod, newdata = datagrid(hp = c(100, 110, 120),\n                                    cyl = c(4, 6, 8)))\n\n\n\n Term Contrast Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 %    97.5 %\n hp      dY/dX  -0.0211    0.01122 -1.885  0.05948 4.1 -0.0431  0.000845\n disp    dY/dX  -0.0260    0.00892 -2.919  0.00351 8.2 -0.0435 -0.008554\n cyl     6 - 4  -4.0472    1.42660 -2.837  0.00455 7.8 -6.8433 -1.251102\n cyl     8 - 4  -2.4319    2.78253 -0.874  0.38212 1.4 -7.8856  3.021735\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "blog/prob-puzzle-bertrand-ballot/index.html",
    "href": "blog/prob-puzzle-bertrand-ballot/index.html",
    "title": "Summer Simulations: Exploring Bertrand’s Ballot Problem with a Simulation Study",
    "section": "",
    "text": "Packages used in this post\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(kableExtra)\ntheme_set(theme_minimal_grid())\n\n\nBertrand’s ballot problem is a fascinating challenge in probability theory, originally posed by French mathematician Joseph Bertrand. It asks: if two candidates, A and B, receive 𝑝 and 𝑞 votes respectively, what is the probability that A will stay ahead of B throughout the entire vote counting process? Traditionally, this problem uses absolute vote counts, but in this study, I’ll tweak the problem slightly and use percentage points.\nThe goal is to simulate this problem using R, exploring how different vote distributions affect the probability of A staying ahead.\nTo begin, I will simulate one vote counting process by taking advantage of the cumsum function, which calculates the cumulative sum. Let’s simulate a case where A gets 51 percent of the votes and B 49 percent. I also create a “ballot box”, which contains 1s for Part A’s votes and 0s for those of Party B.\n\np &lt;- 51 # Example of A's vote share\nq &lt;- 100 - p # B's vote share follows from p\n\nvotes &lt;- c(rep(1, p), rep(0, q)) # Ballot box w/ all votes. \n\nNow I can create my first simulation by randomly drawing from the ballot box vector (votes). The variables votes_for_a and votes_for_b keep track of how many votes each party has received.\n\n  sims &lt;- \n    tibble(ballot = 1:length(votes)) |&gt; \n    mutate(voted_a = sample(votes, length(votes))) |&gt; \n    mutate(votes_for_a = cumsum(voted_a),\n           votes_for_b = cumsum(1 - voted_a))\n\nThe resulting data looks like this:\n\nhead(sims)\n\n# A tibble: 6 × 4\n  ballot voted_a votes_for_a votes_for_b\n   &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      1       1           1           0\n2      2       0           1           1\n3      3       0           1           2\n4      4       1           2           2\n5      5       1           3           2\n6      6       0           3           3\n\n\nDid Party A stay ahead of Party B throughout the vote counting?\n\nall(sims$votes_for_a &gt; sims$votes_for_b)\n\n[1] FALSE\n\n\nNo (which makes sense given a final majority of only one percent).\n\nSimulating One Vote Count Using a Function\nLet us turn our initial code into a function to simulate the vote counting process. This function will help determine if A stays ahead throughout the count:\n\nsimulate_votes &lt;- function(p = 50, q = 100 - p) {\n  votes &lt;- c(rep(1, p), rep(0, q)) # Ballot box w/ all votes\n  \n  sims &lt;- \n    tibble(ballot = 1:length(votes)) |&gt; \n    mutate(voted_a = sample(votes, length(votes))) |&gt; \n    mutate(votes_for_a = cumsum(voted_a),\n           votes_for_b = cumsum(1 - voted_a))\n  \n  all(sims$votes_for_a &gt; sims$votes_for_b)\n}\n\n# simulate_votes(p = 60) # Make sure the function works\n\n\n\nRunning the Simulation\nNext, I run the simulation across multiple trials to obtain reliable probability estimates. I use the crossing function to create a tibble of all combinations of trials and values of 𝑝. We then apply our simulation function to each value of 𝑝 and record whether A stayed ahead:\n\nrs &lt;- \n  crossing(trial = 1:1000, \n           p = 50:100) |&gt; \n  mutate(ahead = map_lgl(p, ~simulate_votes(p = .)))\n\n\n\nAnalyzing the Results\nOne way of presenting the results is using a line graph to show the probability that A stays ahead throughout the vote count for different values of 𝑝.\n\n\nCode\nrs |&gt; \n  group_by(p) |&gt; \n  summarize(probability = mean(ahead)) |&gt; \n  ggplot(aes(p, probability)) + \n  geom_line(size = 1, color = \"firebrick\") + \n  scale_y_continuous(labels = scales::percent_format()) + \n  labs(title = \"More votes, bigger chance of leading from start to finish\",\n       subtitle = \"Probability that Party A will be strictly ahead of B throughout the count\",\n       x = \"Party A' Vote Share\",\n       y = \"Probability of Staying Ahead\") + \n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nThis plot shows that as the number of votes for A increases, the probability of A staying ahead throughout the count also increases.\nAnother approach would be to present the results in a table, focusing on specific values of 𝑝 that might be of particular interest:\n\n\nCode\nrs |&gt; \n  group_by(p) |&gt; \n  summarize(probability = mean(ahead)) |&gt; \n  filter(p %in% c(50, 75, 90, 100)) |&gt;  \n  kable(col.names = c(\"Party A's final vote share\", \"Probability\"), \n    digits = 2) \n\n\n\n\nTable 1: Probability of Party A staing ahead througout the vote counting for selected final vote shares\n\n\nParty A's final vote share\nProbability\n\n\n\n\n50\n0.00\n\n\n75\n0.50\n\n\n90\n0.81\n\n\n100\n1.00\n\n\n\n\n\n\n\n\nThis table will show the probability of party A staying ahead for selected values of 𝑝, providing a quick reference for key points in our simulation.\n\n\nConclusion\nThe above simulation study reveals that the likelihood of one party remaining ahead throughout the vote count significantly increases with a higher number of votes. This result is quite intuitive but simulation allows us to test if our intuition is correct as well as assigning specific probabilities to outcomes of interest.\nYou could complicate the simulation study by adding more parties or by allowing the probability of receiving a vote to vary throughout the vote count (so that Party A is more likely to receive its votes early on, for example).\nHappy simulating!"
  },
  {
    "objectID": "blog/prob-puzzle-marriage/index.html",
    "href": "blog/prob-puzzle-marriage/index.html",
    "title": "Summer Simulations: Solving the Marriage Problem Using Tidy Simulation",
    "section": "",
    "text": "Packages used in this post\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(kableExtra)\ntheme_set(theme_minimal_grid())\n\n\nThe marriage problem, also known as the secretary problem, is a classic probability puzzle that allows us to study optimal stopping theory and decision-making under uncertainty.\nIt goes like this: Consider a woman who has 𝑛 men willing to marry her. If all the men showed up at once, she could order them and choose the best. Unfortunately for her, the men arrive one at a time and in random order. After dating each man for a short period of time, she must decide, before moving on to the next, whether or not to marry him. If she rejects his marriage proposal, she cannot recall him at a later time and, should she decide to marry, she will have to forego meeting the remaining suitors.\nWhat strategy should she adopt if she wants to maximize the probability of marrying the best suitor?\nThis puzzle is an excellent example of a type of decision-making that requires some form of stopping rule: when should the woman “stop playing” and accept the suitor at hand? This type of puzzle extends to many other forms of decisions such as hiring employees, selling assets, and making investment choices.\n\nStopping Rule #1: Choosing at Random\nTo begin, consider a naive strategy: choosing at random. For example, knowing that she has 10 suitors, the woman rolls a fair 10-sided die and chooses the suitor corresponding to the number on the die (e.g. the fifth suitor to show up).\nHow often will that strategy lead to woman to marrying the optimal suitor?\n\nn_men &lt;- 10\nsuitors &lt;- 1:n_men\n\nmean(replicate(1000, sample(suitors, n_men)) == n_men)\n\n[1] 0.1\n\n\nOn average, this strategy results in choosing the best suitor about 10% of the time - pretty bad odds if you ask me.\nAlso, notice that this is equivalent to picking a specific position every time, such as always choosing the third suitor:\n\nmean(replicate(10000, sample(suitors, n_men)[3]) == n_men)\n\n[1] 0.1021\n\n\n\n\nStopping Rule #2: Building on Baseline Knowledge\nA more sophisticated strategy would be using a training set to first establish a baseline before making a final selection. Here’s how it works:\n\nObserve a set number of suitors (training set) without making a selection.\nSet a cutoff based on the highest quality suitor in the training set.\nChoose the first suitor in the test set who exceeds this cutoff.\n\nLet’s implement this strategy in R:\n\nbaseline_choice &lt;- function(n_men, n_train) {\n  \n  # Define suitors\n  suitors &lt;- 1:n_men\n  \n  # Shuffle the suitors\n  suitors &lt;- sample(suitors, n_men)\n  \n  # Split suitors into \"train\" and \"test\" sets\n  train &lt;- suitors[1:n_train]\n  test &lt;- suitors[(n_train + 1):n_men]\n  \n  # Define a cutoff\n  cutoff &lt;- max(train)\n  \n  # Choose the first suitor greater than the cutoff\n  choice &lt;- which(test &gt; cutoff)[1]\n  \n  # If no suitor is found that is greater than the cutoff, choose the last one\n  if (is.na(choice)) {\n    return(suitors[n_men])  # Choose the last suitor in the original list\n  } else {\n    return(test[choice])\n  }\n}\n\nTo make sure this function works, let’s run a simulation with ten suitors with the first five being set aside for the training set:\n\nresults &lt;- replicate(1000, baseline_choice(10, 5))\nmean(results == n_men)\n\n[1] 0.375\n\n\nAlready much better odds than choosing at random.\nThe next question is of course what is the optimal number of suitors to set aside for the training set? We can look this number up on Wikipedia but simulation allows us to approximate the right number without knowing a lot of math.\nLet us rerun our simulation from before but this time with 100 suitors. Using the ever-useful crossing function and map we can study how the probability of choosing the best suitor varies with the number of suitors set aside. We’ll do 10.0000 simulations:\n\nn_men &lt;- 100\n\nsims &lt;- \n  crossing(trial = 1:10000, \n         n_train = 1:99) |&gt; \n  mutate(suitor_score = map_dbl(n_train, ~baseline_choice(n_men, .))) \n\nAnd then calculate the probability:\n\n\nCode\nrs &lt;- \n  sims |&gt; \n  group_by(n_train) |&gt; \n  summarize(chose_mr_ten= mean(suitor_score == n_men)) |&gt; \n  arrange(desc(chose_mr_ten))\n\nhead(rs) |&gt; \n  kable(col.names = c(\"Number of suitors in training set\", \"Probaiblity\"), digits = 2)\n\n\n\n\nTable 1: Probability of selecting the best suitor given size of training set\n\n\nNumber of suitors in training set\nProbaiblity\n\n\n\n\n38\n0.37\n\n\n36\n0.37\n\n\n35\n0.37\n\n\n39\n0.37\n\n\n40\n0.37\n\n\n42\n0.37\n\n\n\n\n\n\n\n\nWe can also visualize the success rate as a function of the training set size:\n\n\nCode\nrs |&gt; \n  ggplot(aes(n_train, chose_mr_ten)) + \n  geom_line(color = \"firebrick\", linewidth = 1) + \n  scale_y_continuous(labels = scales::percent_format()) + \n  labs(x = 'Number of Suitors in \"Training Set\"',\n       y = \"Probability of Choosing the Best Suitor\",\n       title = str_wrap(\"Using About One-Third of Suitors for Training Will Get You The Best Suitor in The End\", 60)) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nConclusion\nThe optimal stopping rule in the marriage problem suggests that the woman should observe and reject roughly the first third of suitors, then marry the first suitor who is better than those she has previously observed.\nThis approach to solving the marriage problem showcases the power of simulation in understanding decision-making processes. Similar strategies can be applied to various real-world scenarios, such as hiring the best candidate, selling a house at the optimal price, or making investment decisions, where making the right choice is crucial and must be done with incomplete information.\nHappy simulating!\n\n\nCool! Where Can I Learn More?\n\nDavid Robinson’s blogpost on tidy simulation: http://varianceexplained.org/posts/"
  },
  {
    "objectID": "blog/prob-puzzle-birthdays-in-erewhon/index.html",
    "href": "blog/prob-puzzle-birthdays-in-erewhon/index.html",
    "title": "Summer Simulations: Optimizing the Number of Factory Workers in Erewhon",
    "section": "",
    "text": "Packages used in this post\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(kableExtra)\ntheme_set(theme_minimal_grid())\n\n\nAs summer approaches, I found the time to write up a brief post on the “factory workers in Erewhon” problem - another classic probability puzzle.\nThe puzzle is as follows: Labor laws in Erewhon require factory owners to give every worker a holiday whenever one of them has a birthday, and they must hire without discrimination based on birthdays. Except for these holidays, they work a 365-day year. The owners want to maximize the expected total number of man-days worked per year in a factory. How many workers should the factories in Erewhon employ?\n\nSimulating One Year of Birthdays in a Single Factory\nTo tackle this problem, we start by writing a function to simulate a single year. For our case in Erewhon, the function might look something like this:\n\nsimulate_one_year &lt;- function(n_workers) {\n  \n  # Calculate the number of holidays for a year with n_workers\n  # replace = TRUE because workers may share birthdays\n  n_holidays &lt;- length(unique(sample(1:365, n_workers, replace = T)))\n  \n  # Number of work days = # Days in year - # holidays \n  n_work_days &lt;- 365 - n_holidays\n  \n  # And then the man-days...\n  man_days &lt;- n_work_days * n_workers\n  \n  return(man_days)\n  \n}\n\nThis function takes the number of workers in a given factory as input, simulates the occurrence of birthdays in a year, and uses that to calculate the resulting man-days.\nLet’s ensure it works by trying it with 10 workers:\n\nsimulate_one_year(10)\n\n[1] 3550\n\n\nThe function returns 3,550 man-days. Now, let’s optimize this to find the ideal number of workers for maximizing man-days in a factory.\n\n\nRunning the Function Multiple Times\nWe’ll use crossing to create a combination of trials (simulations) and the number of workers:\n\nset.seed(25062024)\n\nsims &lt;- \n  crossing(trial = 1:1000, \n         n_workers = seq(1, 500, 1)) |&gt; \n  mutate(man_days = map_dbl(n_workers, ~ simulate_one_year(.)))\n\nThe function takes a little time to run. Afterward, we can calculate the results:\n\nsims_rs &lt;- \n  sims |&gt; \n  group_by(n_workers) |&gt; \n  summarise(avg_workdays = mean(man_days)) |&gt; \n  arrange(desc(avg_workdays))\n\n\n\nCode\nsims_rs |&gt; \n  head() |&gt; \n  kbl(col.names = c(\"Number of workers\", \"Resulting man-days\"), \n      format.args = list(big.mark = ','))\n\n\n\n\nTable 1: Resulting man-hour for selected number of factory workers in Erewhon\n\n\nNumber of workers\nResulting man-days\n\n\n\n\n376\n49,063.86\n\n\n363\n49,038.03\n\n\n358\n49,037.41\n\n\n380\n49,018.48\n\n\n351\n49,015.75\n\n\n383\n49,006.38\n\n\n\n\n\n\n\n\nOur simulation analysis suggests that the optimal number of workers is around 376, which is close to the true number of 364/365 found using calculus. Depending on the desired level of precision, our estimate might be good enough. Simulation has brought us close without needing to delve into complex probability theory.\nWe can also visualize our results:\n\n\nCode\nsims_rs |&gt; \n  ggplot(aes(n_workers, avg_workdays)) + \n  geom_line(linewidth = 1, color = \"firebrick\") +\n  scale_y_continuous(labels = scales::comma) + \n  labs(title = \"Factories in Erewhon have about 365 workers\",\n       x = \"Number of workers\",\n       y = \"Number of man-days\")\n\n\n\n\n\nAnd say we wanted to calculate confidence intervals for the number of man-days provided the 365 optimal number of factory workers\n\nset.seed(25062024)\nsims_2 &lt;- \n  tibble(\n    man_days = replicate(1000, simulate_one_year(365)) \n  )\n\nquantile(sims_2$man_days, probs = c(0.025, .5, 0.975 ))\n\n 2.5%   50% 97.5% \n44895 48910 53290 \n\n\nLikewise, we could plot the uncertainty using a histogram:\n\n\nCode\nsims_2 |&gt; \n  ggplot(aes(man_days)) + \n  geom_histogram(fill = \"firebrick\", color = \"white\") + \n  geom_vline(xintercept = mean(sims_2$man_days), lty = 2, linewidth = 1) + \n  labs(x = \"Man-Days\",\n       y = \"Frequency\") \n\n\n\n\n\nFigure 1: Distribution of Man-Days for Optimal Number of Workers (i.e. 365)\n\n\n\n\nA final note: We could also have calculated the expected number of man-hours using calculus and probability theory (you can look up the formulas online, just type “factories erewhon”). Let’s do that and compare those numbers with our simulations\n\ncalc &lt;- \n  tibble(\n    n_workers = seq(1, 500, 1), \n    calc_workers = n_workers * 365 * (364/365)^n_workers\n  )\n\n\n\nCode\nsims_rs |&gt; \n  ggplot(aes(n_workers, avg_workdays)) + \n  geom_point(data = calc, aes(n_workers, calc_workers), color = \"steelblue\", size = .05) + \n  geom_line(linewidth = 1, color = \"firebrick\", alpha = .9) +\n  scale_y_continuous(labels = scales::comma) + \n  labs(x = \"Number of workers\",\n       y = \"Number of man-days\")\n\n\n\n\n\nFigure 2: Comparing simulations (red line) to calculus (blue dots)\n\n\n\n\nAlmost impossible to distinguish!\n\n\nConclusion\nOur simulation has demonstrated that the optimal number of factory workers in Erewhon is approximately 365, balancing the trade-off between birthdays and workdays effectively. This result, derived through simulation, closely aligns with the theoretical maximum obtained using calculus. Simulation provides an accessible and intuitive approach to solving complex probability problems, making it an invaluable tool for decision-making in uncertain scenarios.\nBy employing the tidyverse suite in R, we’ve not only solved a fascinating probability puzzle but also showcased the power of simulations in optimizing real-world decisions. Whether you’re a factory owner in Erewhon or tackling a different optimization problem, simulations can provide clear insights and guide you to the best outcomes."
  }
]