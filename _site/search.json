[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paw Hansen",
    "section": "",
    "text": "Hi, I’m Paw! Good to see you.\nI’m a researcher at the The Danish National Center for Social Science Research where I work on I do experimental behavioral science research related to the public sector. Currently, I study how frontline workers make professional judgements, and how these affect clients. You can find examples of my research here.\nMy research interests also include data visualization, Bayesian modeling, and how to communicate statistics and evidence to practitioners in compelling and intuitive ways. I occasionally blog about these topics.\nWhen I’m out of office, I enjoy sourdough baking, having coffee with friends, and reading gloomy novels in the evenings. I live in Copenhagen with my wife and our boy, Jakob."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Paw Hansen",
    "section": "",
    "text": "Hi, I’m Paw! Good to see you.\nI’m a social science researcher doing experimental behavioral science research related to the public sector. Currently, I study how frontline workers make professional judgements, and how these affect clients.\nMy research interests include data visualization, Bayesian modeling, and how to communicate statistics and evidence to practitioners in compelling and intuitive ways.\nWhen I’m out of office, I enjoy sourdough baking, having coffee with friends, and reading gloomy novels in the evenings. I live in Copenhagen with my wife and our baby boy, Jakob."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Bayesian analysis of a randomized controlled trial III: Interpretation and presentation\n\n\n\n\n\n\n\nBayesian modeling\n\n\npresentation\n\n\n\n\nFinal part of my three-part series on Bayesian analysis of randomized controlled trials. Get ready to interpret and present your results!\n\n\n\n\n\n\nJul 2, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nBayesian analysis of a randomized controlled trial I: Specifying the priors\n\n\n\n\n\n\n\nBayesian modeling\n\n\n\n\nKicking off my three-part series on doing Bayesian analysis of randomized controlled trials. First up: specifying your priors.\n\n\n\n\n\n\nJul 2, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nBayesian analysis of a randomized controlled trial II: Defining and validating the model\n\n\n\n\n\n\n\nBayesian modeling\n\n\n\n\nPart two of my three-part series on Bayesian analysis of randomized controlled trials. You’ve built a model but is it any good?\n\n\n\n\n\n\nJul 2, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nDot-dot-dot: Three dotplots to stake your life on\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nCelebrating the beauty of dots and points in plotting data.\n\n\n\n\n\n\nJun 9, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nThe policy evaluator’s toolbox: Plotting your next regression discontinuity analysis\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nWhen evaluating public policies, regression discontinuity designs are impossible to live without. Here is how to do one.\n\n\n\n\n\n\nJun 1, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nThe policy evaluator’s toolbox: Presenting results from randomized experiments\n\n\n\n\n\n\n\nstatistical analysis\n\n\n\n\nSensible out-of-the-box tables and plots for your next randomized controlled trial\n\n\n\n\n\n\nMay 24, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nThe Human Scale Principle for presenting statistical results\n\n\n\n\n\n\n\nstatistical analysis\n\n\npresentation\n\n\n\n\nWhen presenting statistical results, don’t rely on the default computer output. Do this instead.\n\n\n\n\n\n\nMay 17, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nShow the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Making public service employees aware of their positive impact\n\n\n\n\n\n\n\nfrontline work\n\n\n\n\nA nagging question: How well do MTurk-validated interventions fare among real-world samples?\n\n\n\n\n\n\nFeb 14, 2023\n\n\nPaw Hansen\n\n\n\n\n\n\n  \n\n\n\n\nRule bending on the frontlines of public service delivery\n\n\n\n\n\n\n\nfrontline work\n\n\n\n\nWhen and why do caseworkers bend the rules in favor of their clients?\n\n\n\n\n\n\nNov 19, 2022\n\n\nPaw Hansen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/replication-motivation/index.html",
    "href": "research/replication-motivation/index.html",
    "title": "Making public service employees aware of their positive impact",
    "section": "",
    "text": "I have a new article out in Public Administration.\nStudies show that asking frontline workers (teachers, nurses, police officers) to reflect on the positive difference they make in citizens’ lives will enhance their motivation. This idea makes sense: People like to help each other out, and when making one’s positive impact more obvious, we should expect public servants to be happier about their jobs.\nHowever, previous studies have often relied on paid survey respondents, such as those recruited via Amazon MTurk. The result is an unfortunate mismatch between test sample and target population. Essentially, we should probably expect people who sign up to participate in a paid survey to be motivated by different outcomes than fronltine workers in general.\nTo test how well reflection tasks do among real-world public servants, I did a replicationof a recent study by Vogel and Willems. My replication study was set among 412 Danish caseworkers working with unemployed clients.\nI found that the effect sizes of reflection tasks are likely smaller when deployed “in the wild.” And so we should probably be critical when we consider how much we can get from low-cost reflection tasks.\nHere’s a graph from the paper. The control group was not asked to do any reflection task but simply answered a few questions about job satisfaction and turnover intention. The prosocial treatment group reflected on a recent impact they had made on a specific client, whereas the societal treatment group reflected on a recent impact on society more generally.\nFigure 1 is the key graph from the paper. As you can see from the graph, there is really no difference between any of the groups, and this indifference holds for both outcomes.\n\n\n\nFigure 1: Outcomes across experimental conditions\n\n\nThe published version of the article is Open Access. You can find it here."
  },
  {
    "objectID": "research/rule-bending-caseworkers/index.html",
    "href": "research/rule-bending-caseworkers/index.html",
    "title": "Rule bending on the frontlines of public service delivery",
    "section": "",
    "text": "My first article is out in the International Public Management Journal!\nHere’s what it’s about: When facing a disobedient client, caseworkers are often required to impose sanctions. But even when sanctions are explicitly required by law, research shows that caseworkers may turn a blind eye. Why so?\nThe study asks how certain clients can merit themselves to avoid sanctioning.\nDrawing from literature on deservingness, I propose that clients can avoid sanctions if they seem (1) needy, (2) hard working, or (3) resourceful.\nTo test these expectations, I ran a conjoint vignette experiment among Danish unemployment caseworkers. In the experiment, I presented each of 407 caseworkers with 3 fictive client descriptions and asked if the caseworker would santion that client. Within each client description, I randomly assigned several pieces of information concerning the three types of deserving clients above.\nImportantly, according to policy rules, all clients should have been sanctioned, yet for many of the client profiles caseworkers were unwilling to impose sanctions.\nInterestingly, caseworkers tended to favor stronger clients when bending the rules: Clients who appeared motivated, had not been sanctioned in the past, or had years of previous job experience were all less likely to be sanctioned.\nThis is a bit of paradox: Although welfare usually targets clients in need, avoiding welfare sanctions seems based on client resources. Consequently, caseworker rule-bending can have unintended distributional consequences since stronger clients are those who can get away with disobedience.\nHere is key figure from the paper:\n\n\n\nFigure 1: Causal effect of various client attributes on caseworker sanctioning\n\n\nYou can download a preprint of the article using the botton above. Or find the the published version here."
  },
  {
    "objectID": "blog/human-scale-principle/index.html",
    "href": "blog/human-scale-principle/index.html",
    "title": "The Human Scale Principle for presenting statistical results",
    "section": "",
    "text": "Computer software, such as R and Python, makes it easy to build complex statistical models. And with a few lines of code, you can get all sorts of outputs summarizing the model parameters.\nHowever, the default computer output for most statistical models is rarely the most intuitive or compelling way to present your results to stakeholders.\nIn this post, I introduce a basic principle for how to present statistical results in a way that is easy to interpret, compelling to readers, and requires no special knowledge to understand. Applying these guidelines will ensure that your next data analysis actually makes the impact on stakeholders you intend it to make.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(socviz)\nlibrary(broom)\nlibrary(scales)\nlibrary(modelsummary)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\n\nThe problem with accepting the default computer output\nConsider the following example using survey data from the General Social Survey. Using respondents’ age (in years) and vote in the 2016 election (Obama(not Obama), we want to predict support of marihuana legalization (yes/no). That is, our data looks as in Table 1.\n\n\nCode\ndata(\"gss_sm\")\n\n# Recode\ngss_support &lt;- \n  gss_sm %&gt;%\n  drop_na(grass, obama) %&gt;%\n  mutate(grass = ifelse(grass == \"Legal\", 1, 0),\n         obama = factor(ifelse(obama == 1, \"Obama\", \"Not Obama\"))) |&gt; \n  select(c(grass, obama, age))\n\nhead(gss_support) |&gt; \n  kbl()\n\n\n\n\nTable 1: A peak at the data for our example\n\n\ngrass\nobama\nage\n\n\n\n\n1\nObama\n61\n\n\n0\nNot Obama\n72\n\n\n1\nObama\n55\n\n\n1\nObama\n53\n\n\n0\nNot Obama\n71\n\n\n1\nObama\n32\n\n\n\n\n\n\n\n\nLet’s fit a basic logistic regression model predicting support for legalization from age and whether respondents voted for Obama in 2016. Table 2 shows the results.\n\n\nCode\n# Fit model\nlog_mod &lt;- \n  glm(grass ~ age + obama, data = gss_support, family = \"binomial\")\n\n# Show model\nmodelsummary(models = list(\"Support for legalization\" = log_mod),\n             stars = TRUE, fmt = 2, \n             coef_rename =c(\"age\" = \"Age (years)\",\n                            \"obamaObama\" = \"Voted Obama in 2016\") ) \n\n\n\n\nTable 2: Predicting support for marihuana legalization\n\n\n\nSupport for legalization\n\n\n\n\n(Intercept)\n1.17***\n\n\n\n(0.25)\n\n\nAge (years)\n−0.03***\n\n\n\n(0.00)\n\n\nVoted Obama in 2016\n1.10***\n\n\n\n(0.13)\n\n\nNum.Obs.\n1115\n\n\nAIC\n1364.0\n\n\nBIC\n1379.1\n\n\nLog.Lik.\n−679.014\n\n\nRMSE\n0.46\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nTable 2 contains several numbers, some of which have asterisks next to them. If this makes you happy, beware.\nBefore going further, quiz yourself and see if you can answer the following question without using any statistical jargon: what can we say about the relationship between age and support for legalization?\nIf your answer begins “There is a statistically significant relationship between…”, then you are going off track. If your answer begins “The coefficient reveals that the log(odds) of age…”, all the worse.\nBeing a reader of this blog post, you likely have some training in statistical modeling. But your stakeholders might not. If you have a hard time communicating the conclusions from the output in plain language, the default output will probably make little sense to your stakeholders.\n\n\nWhat to do instead: Make predictions on a ‘human scale’\nThe general rule of thumb I propose here is that you always try to present your results ‘on a human scale’. Thinking of someone with no formal training in statistics, what would make sense to that person? What kind of questions would someone caring about the substance (but not the methods) be interested in?\nFor example, log(odds) do not buy you anything down at the supermarket. They have no value or interpretation outside the realms of statistics. Neither do standard deviations, z-scores, or p-values. Without statistical training, these numbers convey no information (and even people with statistical training often get them wrong).\nInstead, use your model to make predictions on a human scale. When deciding on what predictions to make, list all the different questions that someone with no statistical training would want to know the answer to. In the case of our logistic regression, such questions might be:\n\nHow likely is it that an average Republican will support legalization?\n\nHow much more/less is that than the expected support from an average Democrat?\nDoes the support stay the same over the course of a lifetime, or does it drop? By how much does it change?\nHow certain are we of these conclusions?\n\n\n\nTaking a sad presentation and making it better\nFor our example, I use crossing() to set up a dataframe with the values we want to see predictions for. Specifically, I use a range of age going from 18 through 80 years and people who voted for/did not vote for Barack Obama.\n\n\nCode\nnew_dat &lt;- \n  crossing(\n    age = 18:80, \n    obama = c(\"Obama\", \"Not Obama\")\n  )\n\n\nWith that in place, we can calculate our predicted probabilities using augment() from the broom package:\n\n\nCode\npreds &lt;-\n  augment(log_mod, newdata = new_dat, type.predict = \"response\", se_fit = T) %&gt;%\n  mutate(lwr = .fitted - 1.96 * .se.fit,\n         upr = .fitted + 1.96 * .se.fit) \n\n\nNow we have a large amount of predicted probabilities of substantive interest. A good idea would be to graph them. This can be done in several ways, depending on what you want to focus your analysis on. Figure 1 is my suggestion.\n\n\nCode\npreds %&gt;%\n  ggplot(aes(age, .fitted, color = obama)) + \n  geom_line(linewidth = .6) +\n  geom_line(aes(x = age, y = lwr, group = obama), alpha = .7, linetype = \"dashed\") + \n  geom_line(aes(x = age, y = upr, group = obama),  alpha = .7, linetype = \"dashed\") + \n  scale_y_continuous(label = percent_format()) + \n  scale_color_brewer(type = \"qual\", palette = \"Set1\") + \n  labs(x = \"Age (years)\",\n       y = \"Probability of support\",\n       color = \"Vote in 2016 election\") + \n  theme(legend.position = \"top\")\n\n\n\n\n\nFigure 1: Probability of supporting marihuana legalization, by age\n\n\n\n\nWhen writing up the analysis, carefully go over the graph. What do we see?\nA useful structure might be:\n\nPresent the graph. What is on the axes? What do the lines represent?\nPresent general findings. What are some trends visible from the graph?\nUse a few of the calculated predictions to provide readers with some concrete examples.\n\nConcerning 2) and 3), one example, I often use as a template is the following quote from King, Tomz, and Wittenberg (2000, 347):\n\n“Other things being equal, an additional year of education would increase your annual income by $1,500 on average, plus or minus about $500.”\n\nAs a brief example, consider the following paragraph:\n\nFigure 1 shows the results. Solid lines illustrate the predicted probability of supporting marihuana legalization across respondents’ age for people who voted/did not for Obama in the 2016 election. Dashed lines are the 95 percent uncertainty intervals.\n\n\nFirst, we found that political support was a strong predictor of support for legalization. For example, at the median age of 49 years, the predicted probability of support among Obama voters was 74 percent (+/- about 1.7 pct. points), whereas a Republican had only a 49 percent probability of support (+/- 2.5 pct. points).\n\n\nSecond, we found that support for legalization drops with age. For example, at age 74, a Republican is only about half as likely to support legalization as is an 18-year old.\n\nSome final advice, make sure that you round your estimates instead of just throwing useless decimals at your reader. And always provide readers with a sense of the uncertainty around your estimates.\n\n\nFinal thoughts: Presenting results ‘on a human scale’\nAs a data analyst, you should not just “get the numbers right”, but also “get the right numbers”. To do so, you must work on a human scale; thinking about the questions that would be compelling to someone who cares about the substance but not the methods. Use your model to make predictions that address those questions. Then use those predictions to make a compelling graph and provide a few illustrative examples. Done.\n\n\nCool! Where can I learn more?\n\nKing, G., Tomz, M., and Wittenberg, J. (2000). Making the most of statistical analyses: Improving interpretation and presentation. American journal of political science, 347-361.\nHealy, K. (2018). Data visualization: a practical introduction. Princeton University Press.\n\n\n\n\n\n\nReferences\n\nKing, Gary, Michael Tomz, and Jason Wittenberg. 2000. “Making the Most of Statistical Analyses: Improving Interpretation and Presentation.” American Journal of Political Science 44 (2): 347. https://doi.org/10.2307/2669316."
  },
  {
    "objectID": "blog/bayesian-rct-presentation/index.html",
    "href": "blog/bayesian-rct-presentation/index.html",
    "title": "Bayesian analysis of a randomized controlled trial III: Interpretation and presentation",
    "section": "",
    "text": "Welcome to the last part of my three-part series on Bayesian analysis of randomized controlled trial. So far, we have specified priors to take previous knowledge into account and we have fit and validated two different models.\nIn this final post, we’ll look at how we could go about presenting and interpreting our results. In a report or scientific article, this is the part that should go into the Results section.\n\n\nPackages used in this post\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\nIn my view, presenting results is where Bayesian analysis really gets to shine. As I hope you will see, taking a Bayesian approach makes it easy to present meaningful and intuitive quantities of interest and gives us incredible flexibility when it comes to answering question of real-world importance.\nRecall that in our hypothetical experiment, students were randomly assigned to either a treatment or a control group. The outcome is dichotomous: did the students fail the reading test (indicating dyslexia) or not.\nBefore we go into any calculations, let’s look at four core quantities of interest commonly used for controlled trial settings:\n\nabsolute risk\nabsolute risk reduction\nrelative risk\nnumbers needed to treat\n\nWhat do these numbers mean? The absolute risk is simply the risk of an event happening given the exposure.\n\nabs_risk &lt;- \n  fake |&gt; \n  summarize(abs_risk = mean(fail_test, na.rm = T), \n            .by = condition)\n\nabs_risk |&gt; \n  mutate(across(where(is.numeric), ~round(., 2))) |&gt; \n  kbl()\n\n\n\n\ncondition\nabs_risk\n\n\n\n\nControl\n0.52\n\n\nTreatment\n0.41\n\n\n\n\n\n\n\nAt first sight, it seems the treatment worked! The absolute risk of failing the reading test was 0.52 in the control group but only 0.41 in the treatment group.\nSecond, the absolute risk reduction is the difference between the observed risks the treatment and the control group:\n\nround(abs_risk$abs_risk[abs_risk$condition == \"Control\"] \n      - abs_risk$abs_risk[abs_risk$condition == \"Treatment\"], 2)\n\n[1] 0.11\n\n\nFor an individual, the risk reduction expresses the estimated difference in the probability of experiencing the event. In our case the reduction is 11 percent.\nThird, relative risk is the risk of an event occurring in the exposed group divided by the risk of the event occurring in the non-exposed group:\n\nround(abs_risk$abs_risk[abs_risk$condition == \"Treatment\"] \n      / abs_risk$abs_risk[abs_risk$condition == \"Control\"], 2)\n\n[1] 0.78\n\n\nThus, the relative risk of failing the reading test for students in the treatment group was only 0.78 as compared to students in the control group.\nNotice that the relative risk is not the same as the odds, since the odds are the risk of an event occurring in any group divided by the risk of the event not occuring.\nFinally, the number needed to treat is the number of individuals you need to treat to prevent one additional bad outcome:\n\nround(1/ (abs_risk$abs_risk[abs_risk$condition == \"Control\"] \n          - abs_risk$abs_risk[abs_risk$condition == \"Treatment\"]), 0)\n\n[1] 9\n\n\nFinally, for calculating the number needed to treat, we simply take 1 and divide it by the absolute risk reduction. Our results suggests that for every 9 students placed in the program, we could prevent 1 student from failing the reading test.\nHopefully, calculating each of these quantities “by hand”, has given us some intuition about what each number conveys. In practice, though, we will never want to do it this way ever again. For one thing, the above approach did not give us any uncertainty estimates, which we should always include as part of any interpretation. Moreover, recalling formulas can be tedious and for more complicated models, many of these will break down anyway. Instead, we will use our posterior simulations.\n\nAnalysis using rstanarm\nThe core principle for presenting results is that we don’t want to just show a list of model parameters. These have little interest in and off themselves. Instead, we’ll use the model to make predictions, i.e. show the implications of the model..\nTo this end, we set up a data frame containing the values of the predictors we intend to predict from (aka a “predictor matrix”). In our context of a controlled trial, these are simply the treatment and the control group:\n\npred_dat &lt;- \n  crossing(\n    condition = c(\"Control\", \"Treatment\")\n    )\n\nWe can then use this data frame to estimate the absolute risk for each group:\n\nmodel_preds &lt;- \n  add_epred_draws(newdata = pred_dat, mod_rstan)\n\nWith our predictions in place, we can plot distributions of each of the four quantities of interest, for example for including in a presentation. The following code chunk shows how to plot each of the four quantities from before:\n\n\nCode\n# abs risk of passing reading test\nmodel_preds |&gt; \n  ggplot(aes(.epred, fill = condition)) + \n  geom_density(alpha = .4) + \n  annotate(geom = \"text\", x = .41, y = 2, label =\"Treatment\") + \n  annotate(geom = \"text\", x = .52, y = 2, label =\"Control\") + \n  scale_x_continuous(labels = scales::label_percent()) + \n  scale_y_continuous(labels = NULL) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Absolute risk\",\n       y = NULL, \n       fill = \"Condition\") + \n  theme(legend.position = \"none\")\n\n# rel risk of failing\nrel_risk &lt;- \n  as_tibble(posterior_epred(mod_rstan, newdata = pred_dat)) |&gt; \n  mutate(control = `1`, treatment = `2`) |&gt; \n  mutate(relrisk = treatment / control) \n\nrel_risk |&gt; \n  ggplot(aes(relrisk)) + \n  geom_histogram(fill = \"lightblue\", color = \"white\") + \n  geom_vline(xintercept = median(rel_risk$relrisk), \n             lty = 2,\n             color = \"firebrick\") + \n  scale_y_continuous(labels = NULL) +\n  labs(x = \"Relative risk\", \n       y = NULL)\n\n# abs risk reduction \nabs_rr &lt;- \n  as_tibble(posterior_epred(mod_rstan, newdata = pred_dat)) |&gt; \n  mutate(control = `1`, treatment = `2`) |&gt; \n  mutate(abs_rr = control - treatment) \n\nabs_rr |&gt; \n  ggplot(aes(abs_rr)) + \n  geom_histogram(fill = \"lightblue\", color = \"white\") +\n  geom_vline(xintercept = median(abs_rr$abs_rr), \n             lty = 2,\n             color = \"firebrick\") + \n  scale_y_continuous(labels = NULL) +\n  labs(x = \"Absoulte risk reduction\",\n       y = NULL)\n\n# Numbers needed to treat \nnntt &lt;- \n  abs_rr |&gt; \n  mutate(nn_treat = 1/abs_rr) \n\nnntt |&gt; \n  ggplot(aes(nn_treat)) +\n  geom_histogram(fill = \"lightblue\", color = \"white\") +\n  geom_vline(xintercept = median(nntt$nn_treat), \n             lty = 2,\n             color = \"firebrick\") + \n  scale_y_continuous(labels = NULL) +\n  xlim(0, 25) + \n  labs(x = \"Numbers needed to treat\", \n       y = NULL)\n\n\n\n\n\n\n\n\n(a) Absolute risk of failing reading test.\n\n\n\n\n\n\n\n(b) Relative risk of failing reading test when given treatment as opposed to control.\n\n\n\n\n\n\n\n\n\n(c) Absolute risk reduction in failing reading test when given treatment\n\n\n\n\n\n\n\n(d) Numbers needed to treat: How many students should be placed in the program to prevent one student from failing the reading test?\n\n\n\n\nFigure 1: Quantities of interest with uncertainties, calculated using rstanarm.\n\n\n\nWhen writing up our analysis, we could include one or more of these plots and then use the predictions to make meaningful statements about the model implications and uncertainties. Taking the number needed to treat as an example, we might want to calculate the 80 percent prediction interval:\n\n\nCode\nround(quantile(nntt$nn_treat, c(.1, .5, .9)))\n\n\n10% 50% 90% \n  6   9  14 \n\n\nFrom that, we could write something like: “Using our Bayesian model, we estimated the number needed to treat. We found that the intervention should be applied to 9 students to prevent one case of dyslexia, plus or minus about 4 students.”\n\n\nTaking the posterior out for a spin: probability of a practically significant effect\nSo far we have covered most the standard quantities of interest for analyzing randomized controlled trials. But Bayesian analysis has more to offer.\nWhen working within a Bayesian framework, a lot of the hard work lies in specifying a sensible prior and then fitting a proper model. But once we have our posterior distribution(s) at hand, we can calculate any quantity that might be of interest.\nTo give a brief example, suppose a policy maker is only interested in scaling up the intervention if the absolute risk reduction is at least 15 percent (perhaps out of political or cost-effectiveness considerations).\nLooking at the histogram in Figure 1 (c), we see that some of the distribution is above .15. It is not impossible that the treatment effect could be .15 or more. But how likely is that? Put another way, given what we know (model, data), what is the probability that the absolute risk reduction is above 15 percent?\nWe can approach this graphically by drawing the empirical cumulative distribution, which shows the share of observations being lower than the specified value on the x-axis.\nFigure 2 (a) shows a distribution of plausible risk reductions given the data and model. The shaded region starting at 15 percent is what we care about. This region is an incredibly powerful concept because it allows us to think about effect sizes in terms of what _matters rather than what is “statistically significant”.\nWhat is the probability that our risk reduction lies within the ROPE? Figure Figure 2 (b), shows a so-called quantile function, plotting the potential risk reductions on the x-axis and the inverse cumulative probability on the y-axis. Substantially, the inverse cumulative probability describes the probability of the risk reduction being greater than the number on the x-axis.\nLooking at the figure, we see that the probability is about 13. Is this a lot or a little? Well, that depends…\nOf course 13 percent, or about 1-in-8, does not sound like a lot. But suppose that running the intervention is really cheap. Then, policy makers may want to “roll the dice” and see what happens. After all, the probability that the effect size is greater than 15 percent is small, but the probability that the effect size is at least 5 percent is 98 percent!\nThe point is that when doing Bayesian analysis, we do not bake decision-making into the analysis, such as saying that the treatment “worked” because p&lt;0.05). Instead, we live with the uncertainty; estimating the effect size and uncertainties the best we can, and then leave the rest up to stakeholders.\n\n\nCode\n# Density curve \nabs_rr |&gt; \n  ggplot(aes(abs_rr)) + \n  geom_density() + \n  geom_vline(xintercept = .15, \n             linetype = \"dashed\", \n             color = \"steelblue\") + \n  geom_area(data = abs_rr_density_tibble, \n            aes(x = abs_rr, y = density), \n            fill = \"lightblue\",\n            alpha = .4) + \n  scale_x_continuous(labels = scales::label_percent()) + \n  scale_y_continuous(labels = NULL) + \n  labs(x = \"Absolute risk reduction\",\n       y = NULL)\n\n# ECDF\nabs_rr |&gt; \n  ggplot(aes(abs_rr)) +\n  geom_vline(xintercept = .15, color = \"steelblue\", linetype = \"dashed\") + \n  stat_ecdf(aes(y = 1 - ..y..)) + \n  geom_point(data = secret_dot, aes(abs_rr, ecdf), color = \"steelblue\") + \n  annotate(geom = \"text\", x = .21, y = .14, label = str_c(\"Pr(ARR &gt; 15 pct.): \\n\", secret_prob, \" pct.\"), color = \"steelblue\", size = 4) + \n  scale_x_continuous(labels = scales::label_percent(),  limits = c(0, .25)) + \n  scale_y_continuous(labels = scales::label_percent()) + \n  labs(x = \"Absolute risk reduction\", \n       y = \"Inverse cumulative probability\")\n\n\n\n\n\n\n\n\n(a) Distribution of the plausable risk reductions given the data and model. Policy maker only cares about the blue area.\n\n\n\n\n\n\n\n(b) Quantile fundction showing the distribution of plausable risk reductions. The graph shows the probability of the absoulute risk reduction being lower than the specified value on the x-axis.\n\n\n\n\nFigure 2: What is the probability that the risk reduction is above 15 percent?\n\n\n\nOf course, in addition to eyeballing the graph, we can ask R to extract the exact probability for us:\n\n\nCode\nmean(abs_rr$abs_rr &gt; .15)\n\n\n[1] 0.12675\n\n\nAs a final tip, interpreting a quantile function may be tricky to non-technical audiences. If I were advising, say, a politician, I might prefer to present some meaningful risk reductions and then simply present these in a small table:\n\n\n?(caption)\n\n\n\n\n\n\nMinimum risk reduction\nProbability\n\n\n\n\n0.05\n0.98\n\n\n0.10\n0.68\n\n\n0.15\n0.13\n\n\n0.20\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal thoughts: Bayesian analysis of a randomized controlled trial - interpretation and presentation\nBayesian analysis has a lot to offer when conducting and interpreting statistical analyses of randomized controlled trials. It allows analysts to interpret results in a way that is usually far more intuitive and useful to policy makers than standard reports relying on statistical significance tests. Rather than discretizing results (the treatment either “worked” or “failed”), Bayesian analysis makes it possible to make more nuanced statements (“the probability that the treatment effect is above the desired 15 percentage points is 97 percent”). Taken together, I hope this post and the two previous ones in the three-part series has encouraged you to go and do your first Bayesian analysis!\n\n\nCool! Where can I learn more?\n\nJohnson, A. A., Ott, M. Q., & Dogucu, M. (2022). Bayes rules!: An introduction to applied Bayesian modeling. CRC Press.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.\nRyan, E. G., Harrison, E. M., Pearse, R. M., & Gates, S. (2019). Perioperative haemodynamic therapy for major gastrointestinal surgery: the effect of a Bayesian approach to interpreting the findings of a randomised controlled trial. BMJ open, 9(3).\nGoodrich, B., Gabry, J., Ali, I., & Brilleman, S. (2020). rstanarm: Bayesian applied regression modeling via Stan. Find many of their useful vignettes here."
  },
  {
    "objectID": "blog/toolbox-rdd/index.html",
    "href": "blog/toolbox-rdd/index.html",
    "title": "The policy evaluator’s toolbox: Plotting your next regression discontinuity analysis",
    "section": "",
    "text": "In the previous post of my “policy evaluator’s toolbox” series, I showed you two basic ways of plotting results from randomized experiments. A randomized experiment should be the design you are going for whenever possible since it allows for easy estimation and clean causal inference.\nIn practice, though, randomized experiments may not be feasible. Often policymakers are unwilling to assign policies at random, so the question becomes what we can do in those cases if we still want to evaluate if a policy had an effect.\nToday, we’ll have a look at one widely-used approach: the regression discontinuity design. Here is the definition I got when asking ChatGPT:\n\n“A regression discontinuity design (RDD) is a quasi-experimental research design used to estimate causal effects by taking advantage of a discontinuity in the treatment assignment process. The basic logic behind RDD is that when a certain threshold or cutoff point is used to determine whether an individual receives a treatment or not, individuals on either side of the cutoff are expected to be similar in all other relevant aspects, except for the treatment itself. By comparing the outcomes of individuals just above and just below the cutoff, researchers can attribute any differences to the treatment rather than other confounding factors.”\n\nThis is not a bad definition but I do want to emphasize one core assumptions: the assignment variable should be continuous or “smooth” around the cutoff.\nIn my field of public welfare, many such cutoff variables exist:\n\nAge. For example, everybody younger than 21 years old might receive free dental care\nIncome. For example, everybody earning above some level might lose eligibility for housing benefits\nTime. For example, a new tutoring program may be implemented on January 1st making it possible to estimate its effects by comparing units just before and just after the cutoff date\n\nFinally, a popular threshold is test scores, which we will work with today. Specifically, we’ll use the Chile Schools data taken from (the highly recommendable) Regression and Other Stories. Let’s get our packages and load up the data:\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggsci)\nlibrary(readr)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\nThe Chile Schools data is a good example of something you will often encounter when doing policy evaluations. In 1992, the Chilean government wanted to improve poor-performing schools. To do so, the government assigned extra resources (teacher training and after-school tutoring) to all schools scoring lower than a predefined cutoff on the average fourth-grade test score. This assignment is clearly not random. In fact, we can be quite certain that the schools that received the extra resources are different from those that did not (that is why those schools received extra help!).\nHowever, as we move closer to the cutoff, it seems more and more likely that the schools falling on either side are not that different. For example, say the cutoff was 60 points on a 100-point scale. Two schools with an average score of 30 and 70 are clearly different, and it makes no sense to compare those. But what about two schools scoring 58 and 62? Do we really believe that those schools differ fundamentally from each other? The key assumption we make in a regression discontinuity design is that the units we look at on either side of the cutoff are as-if random.\n\nPlot 1: Showing the raw data\nA good place to start is to show the raw data on either side of the cutoff. In the plot below, all units above the threshold (here scaled to zero) received extra help, while those below did not.\n\n\nShow the code\n# Make a variable saying if schools received the treatment or not\nchile &lt;-\n  chile %&gt;% \n  mutate(condition = ifelse(rule2 &lt; 0, \"Treatment\", \"Control\")) \n\n\nchile %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nReading test scores in 1992 by assignment variable. The assignment variable is centered reading test scores in 1988. Having a score less than 0 meant that schools would be included in the program.\n\n\n\n\nHowever, since we wanted schools to be “alike” (in terms of potential outcomes), we should probably focus the graph on those units that are close to the threshold. Let’s define a cutoff range from -5 through 5 and make a new plot:\n\n\nShow the code\nchile_sub &lt;- \n  chile %&gt;% \n  filter(rule2 &lt; 5, rule2 &gt; -5) \n\nchile_sub %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  geom_point(aes(color = condition)) + \n scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nReading test scores in 1992 by assignment variable. In this version of the graph, only schools close to the cutoff (-5 through 5) are plotted.\n\n\n\n\nAnd we can even make this plot a bit clearer if we use binned averages instead of showing all data points:\n\n\nShow the code\nchile_sub %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, \n             size = 1, \n             color = \"grey50\", \n             linetype = \"dashed\") + \n  stat_summary_bin(aes(color = condition), \n                   fun = \"mean\", \n                   size = 2, \n                   geom = \"point\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nReading test scores in 1992 by assignment variable. In this version of the graph, schools are plotted using binned averages on the assignment variable.\n\n\n\n\n\n\nPlot 2: Adding the model\nShowing the data (to others but also yourself) is always a good place to start and a key part of of exploratory data analysis. However, to answer the question of interest (did assigning extra help to schools improve reading?), we need a model.\nLuckily, the modeling part of a regression discontinuity analysis is straightforward: We simply include the condition and the assignment variable as predictors in our regression model:\n\n\nShow the code\nchile_rs &lt;- \n  lm(read92 ~ condition + rule2, data = chile_sub) \n\nchile_rs %&gt;% \n  tidy(conf.int=T,) %&gt;% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %&gt;% \n  mutate(term = c(\"Intercept\", \"In program\", \"Assignment variable\")) %&gt;% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      caption = \"Effect of being in policy program\",\n      booktabs = T,\n      digits = 2) %&gt;% \n  kable_styling(full_width = F)\n\n\n\nEffect of being in policy program\n\n\nTerm\nEstimate\nLower bound\nUpper bound\n\n\n\n\nIntercept\n59.83\n58.80\n60.85\n\n\nIn program\n2.15\n0.27\n4.04\n\n\nAssignment variable\n0.95\n0.61\n1.28\n\n\n\n\n\n\n\nThus, the estimated effect of being in the program is about 2.15 points.\nTo understand this result better, and to present it in an intuitive way to stakeholders, we can plot the model on either side of the cutoff (i.e. the regression line):\n\n\nShow the code\nchile_sub %&gt;% \n  ggplot(aes(rule2, read92)) + \n  geom_vline(xintercept = 0, size = 1, color = \"grey50\", linetype = \"dashed\") + \n  geom_point(aes(color = condition), alpha = .6) + \n  geom_smooth(\n    data = chile_sub %&gt;%  filter(rule2 &lt; 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  geom_smooth(\n    data = chile_sub %&gt;%  filter(rule2 &gt; 0),\n    method = \"lm\",\n    color = \"black\",\n    se = F) +\n  annotate(geom=\"text\", \n           x  = -2.5, \n           y = 33, \n           color = \"#DF8F44FF\", \n           label = \"Included in program\") + \n    annotate(geom=\"text\", \n           x  = 2.5, \n           y = 33, \n           color = \"#374E55FF\", \n           label = \"Not included in program\") + \n  scale_color_jama() + \n  labs(x = \"Assignment variable\",\n       y = \"Reading test score\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nReading test scores in 1992 by assignment variable. In this version of the graph, the fitted model on either side of the threshold is overlaid on the raw data.\n\n\n\n\nThe estimated effect of 2.15 points corresponds to the difference (the “kink”) between the two lines.\n\n\nBuilding the final model\nFinally, in most real-world applications, it makes sense to include pre-treatment variables that strongly predict the outcome since this can make our model estimates more precise. In our example, we have schoolchildren’s math and reading scores in 1988, which is probably useful when trying to predict reading scores in 1992. Let’s include those:\n\n\nShow the code\nchile_rs_final &lt;- \n  lm(read92 ~ condition + rule2 + read88 + math88, data = chile_sub) \n\nchile_rs_final %&gt;% \n  tidy(conf.int=T,) %&gt;% \n  select(-c(\"statistic\", \"std.error\", \"p.value\")) %&gt;% \n  mutate(term = c(\"Intercept\", \n                  \"In program\", \n                  \"Assignment variable\",\n                  \"Reading score in 1988\",\n                  \"Math score in 1988\")) %&gt;% \n  kbl(col.names = c(\"Term\", \"Estimate\", \"Lower bound\", \"Upper bound\"),\n      caption = \"Effect of being in policy program, covariate adjusted\",\n      booktabs = T,\n      digits = 2) %&gt;% \n  kable_styling(full_width = F)\n\n\n\nEffect of being in policy program, covariate adjusted\n\n\nTerm\nEstimate\nLower bound\nUpper bound\n\n\n\n\nIntercept\n23.35\n14.84\n31.85\n\n\nIn program\n2.08\n0.28\n3.88\n\n\nAssignment variable\n0.14\n-0.22\n0.51\n\n\nReading score in 1988\n0.61\n0.46\n0.75\n\n\nMath score in 1988\n0.16\n-0.01\n0.32\n\n\n\n\n\n\n\nAs can be seen, our confidence intervals are now slightly narrower, reflecting the extra information we have included in our analysis.\n\n\nFinal thoughts: plotting your next regression discontinuity analysis\nRegression discontinuity designs (RDD) are a popular way of evaluating policies when randomization b y the researcher is not an option. When doing RDD, we look for naturally occurring cutoffs that split units into a treatment and a control group.\nCutoff variables should be continuous. Finally, be aware that RDD is quasi-experimental. It is not a randomized experiment, and it is on you to prove that the assumptions hold.\n\n\nCool! Where can I learn more?\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press."
  },
  {
    "objectID": "blog/toolbox-rct/index.html",
    "href": "blog/toolbox-rct/index.html",
    "title": "The policy evaluator’s toolbox: Presenting results from randomized experiments",
    "section": "",
    "text": "In this series of blog posts, I go over some of the most common experimental designs, including natural experiments such as difference-in-difference, regression discontinuity designs, and so on.\nThe goal is to provide you with some sensible default plots and tables you can use when writing up your next experimental analysis. For more detail on the technicalities of the individual designs, check out the sources I link to at the bottom of this post.\n\n\nPackages used in this post\n# Packages we'll be using today\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(kableExtra)\nlibrary(ggsci)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\nToday: RCTs. What better way to kick off a series on experimental designs than with a truly randomized experiment? To get started, I’ll simulate some fake data we can work with. Table 1 shows a peak of the simulated data.\n\n\nCode\nset.seed(2707)\n\nnum_people &lt;- 1000\ntreat_effect &lt;- 4.5\n\ndat &lt;- \n  tibble(\n  age = sample(18:75, num_people, replace = T), \n  female = sample(c(0, 1), num_people, replace = T), \n  non_western = sample(c(0, 1), num_people, replace = T, prob = c(.85, .15)), \n  mu = 25 + .5 * age + 8 * female -10 * non_western\n)\n\n\ndat &lt;- \n  dat |&gt; \n  mutate(condition = sample(c(\"Control\", \"Treatment\"), size = num_people, replace = TRUE), \n         outcome = case_when(\n           condition == \"Control\" ~ rnorm(num_people, mean = mu, sd = 1),\n           condition == \"Treatment\" ~ rnorm(num_people, mean = mu + treat_effect, sd = 1))\n         ) |&gt; \n  select(-mu) |&gt; \n  mutate(female = case_when(\n    female == 1 ~ \"Female\",\n    female == 0 ~ \"Male\", \n    TRUE ~ \"Trouble\")) |&gt;\n  mutate(non_western = case_when(\n    non_western == 1 ~ \"Non-western\",\n    non_western == 0 ~ \"Western\",\n    TRUE ~ \"Trouble\"))\n\nhead(dat) |&gt;\n  kbl()\n\n\n\n\nTable 1: A peak at the simulated data\n\n\nage\nfemale\nnon_western\ncondition\noutcome\n\n\n\n\n47\nFemale\nWestern\nControl\n57.33940\n\n\n47\nMale\nWestern\nControl\n48.15630\n\n\n44\nFemale\nWestern\nTreatment\n62.29194\n\n\n44\nFemale\nWestern\nControl\n55.12165\n\n\n63\nMale\nWestern\nControl\n54.08326\n\n\n51\nFemale\nWestern\nTreatment\n62.40755\n\n\n\n\n\n\n\n\n\nPresenting descriptives\nFirst, let’s make a table to see if our two groups are balanced on average. See Table 2. This would be your typical “Table 1” if you were writing a research paper.\n\n\nCode\ndat_summary &lt;- \n  dat |&gt; \n  select(\n    Age = age,\n    Female = female,\n    'Non-Western' = non_western,\n    condition,\n    -outcome,\n  )\n  \ndatasummary_balance(~condition, \n                    data = dat_summary)\n\n\n\n\nTable 2: Means of covariates across experimental conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl (N=485)\n\n\nTreatment (N=515)\n\n\n\n\n\n\nMean\nStd. Dev.\nMean\nStd. Dev.\nDiff. in Means\nStd. Error\n\n\n\n\nAge\n\n46.6\n16.7\n47.1\n16.6\n0.6\n1.1\n\n\n\n\nN\nPct.\nN\nPct.\n\n\n\n\nFemale\nFemale\n237\n48.9\n248\n48.2\n\n\n\n\n\nMale\n248\n51.1\n267\n51.8\n\n\n\n\nNon-Western\nNon-western\n72\n14.8\n83\n16.1\n\n\n\n\n\nWestern\n413\n85.2\n432\n83.9\n\n\n\n\n\n\n\n\n\n\nAlternatively, you could reserve that table for an appendix and instead present a table showing descriptives of the full sample. Table 3 shows an examples.\n\n\nCode\ndat_summary &lt;- \n  dat |&gt; \n  select(\n    Age = age,\n    Female = female,\n    'Non-Western' = non_western,\n    Condition = condition,\n    -outcome,\n  )\n\ndatasummary_balance(~1, \n                    data = dat_summary)\n\n\n\n\nTable 3: Means of covariates for the full sample\n\n\n\n\nMean\nStd. Dev.\n\n\n\n\nAge\n\n46.9\n16.6\n\n\n\n\nN\nPct.\n\n\nFemale\nFemale\n485\n48.5\n\n\n\nMale\n515\n51.5\n\n\nNon-Western\nNon-western\n155\n15.5\n\n\n\nWestern\n845\n84.5\n\n\nCondition\nControl\n485\n48.5\n\n\n\nTreatment\n515\n51.5\n\n\n\n\n\n\n\n\n\n\nPresenting results\nAfter presenting evidence that your randomization was successful, you should move on to present your results. This could be either as a regression table (Table 4) or as a figure (Figure 1).\n\n\nCode\nmodel_basic &lt;-\n  lm(outcome ~ condition, data = dat)\n\nmodel_cov_adj &lt;-\n  lm(outcome ~ ., data = dat)\n\nmodelsummary(models = list(\n  \"Unadjusted\" = model_basic,\n  \"Covariate adjusted\" = model_cov_adj),\n  coef_rename = c(\n    \"age\" = \"Age (in years)\",\n    \"femaleMale\" = \"Sex: Male\",\n    \"conditionTreatment\" = \"Condition: Treatment\",\n    \"non_westernWestern\" = \"Origin: Western\")\n  )\n\n\n\n\nTable 4: Outcome across experimental conditions (OLS estimates)\n\n\n\nUnadjusted\nCovariate adjusted\n\n\n\n\n(Intercept)\n50.683\n23.114\n\n\n\n(0.454)\n(0.130)\n\n\nCondition: Treatment\n4.536\n4.439\n\n\n\n(0.632)\n(0.063)\n\n\nAge (in years)\n\n0.500\n\n\n\n\n(0.002)\n\n\nSex: Male\n\n−8.068\n\n\n\n\n(0.063)\n\n\nOrigin: Western\n\n9.895\n\n\n\n\n(0.087)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.049\n0.991\n\n\nR2 Adj.\n0.048\n0.991\n\n\nAIC\n7445.9\n2841.4\n\n\nBIC\n7460.6\n2870.8\n\n\nLog.Lik.\n−3719.933\n−1414.685\n\n\nRMSE\n9.98\n1.00\n\n\n\n\n\n\n\n\nI personally prefer Figure 1 because in addition to showing the model (i.e. the two group averages), it also conveys information about the full distribution of data for both groups.\n\n\nCode\nmeans &lt;-\n  dat %&gt;%\n  group_by(condition) %&gt;%\n  summarise(avg = mean(outcome),\n            sd = sd(outcome)) \n\navg_control &lt;-\n  means %&gt;%\n  filter(condition == \"Control\") %&gt;%\n  select(avg)\n\ndat %&gt;%\n  ggplot(aes(condition, outcome)) +\n  geom_jitter(aes(color = condition), alpha = .4) +\n  geom_hline(yintercept = avg_control$avg, linetype = \"dashed\", color = \"grey50\") +\n  geom_pointrange(data = means, aes(x = condition, \n                                    y = avg, \n                                    ymin = avg + sd, \n                                    ymax = avg - sd)) +\n  labs(color = \"Condition\", \n       x = NULL, \n       y = \"Outcome\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 1: Outcome across experimental conditions\n\n\n\n\n\n\nFinal thoughts: Presenting results from randomized experiments\n\n\nCool! Where can I learn more?\n\nCoppock, A. (2021). Visualize as you randomize. Advances in experimental political science. ."
  },
  {
    "objectID": "blog/bayesian-rct-spec-priors/index.html",
    "href": "blog/bayesian-rct-spec-priors/index.html",
    "title": "Bayesian analysis of a randomized controlled trial I: Specifying the priors",
    "section": "",
    "text": "Welcome to the first part of my three-part series on how to analyze randomized controlled trials (RCTs) using Bayesian thinking and methods. For this first post, I assume you have some basic understanding about what it means to be “working like a Bayesian”. If you are entirely new to Bayesian methods, check out the references at the bottom of the post.\nThe packages we’ll be using today:\nShow the code\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(modelsummary)\nlibrary(kableExtra)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(janitor)"
  },
  {
    "objectID": "blog/bayesian-rct-spec-priors/index.html#footnotes",
    "href": "blog/bayesian-rct-spec-priors/index.html#footnotes",
    "title": "Bayesian analysis of a randomized controlled trial I: Specifying the priors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice that although this is well-suited for our purpose, it actually represents a sub-optimal research design because we have discretized a variable for which it is fair to assume we would have a continuous measure, for example a test score for each student. By working with a dichotomous outcome, we are throwing away valuable information. Table↩︎"
  }
]