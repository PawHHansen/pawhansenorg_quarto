{
  "hash": "ca240004ef72106a7fcf5b9e3c3525f7",
  "result": {
    "markdown": "---\ntitle: \"Using simulation for design analysis: A field guide\"\ndescription: \"How simulation can help you make better design decisions ahead of collecting data.\"\nauthor: \"Paw Hansen\"\ndate: \"8/26/2023\"\nimage: \"featured.jpg\"\ncategories: [statistical analysis]\neditor_options: \n  chunk_output_type: inline\n---\n\n\nIf you've ever done a research project, at some point someone has likely asked you the following question: Is your n large enough? What this person is asking concerns *design analysis*: analyzing how different properties of your design will affect your potential conclusions.\n\nNot doing design analysis ahead of the actual data collection will cause you problems. For example, you might run an intervention study with no chance of detecting the treatment effect.\n\nIn this post, I'll show you a general way to approach design analysis using simulation. As it turns out, using simulation is a rigorous way to think about your research design, and it is far better than what is usually done such as throwing your proposed n into some online 'power calculator'.\n\nWith simulation, you can study all relevant properties of your design; including n, minimum detectable treatment effect size, standard error of the treatment effect, statistical power of the study, and anything else you might think would be nice to know before running your study.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Packages used in this post\"}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\n\ntheme_set(theme_minimal(base_size = 14))\n```\n:::\n\n\n### How to use simulation for design analysis: four steps\n\nThe workflow I propose goes something like this:\n\n1.  Carefully consider any prior assumptions about the future data (sample size, treatment effect, etc.),\n\n2.  simulate *one* dataset based on those assumptions,\n\n3.  turn your simulation into a *function* and then *repeat it* a bunch of times to simulate many dataset,\n\n4.  study the properties of those datasets using summary statistics and/or graphs.\n\nSimple as that.\n\nLet us work our way through a running example. Suppose we were interested in running a randomized experiment with the aim of improving student test scores.\n\n### Step #1: Specify prior assumptions\n\nBased on previous studies, we think the treatment effect could be about five. Previous studies have used samples of about 100 students, so we think this could be a good place for us to start. Also, students in general usually have had test scores with a mean of 60 plus or minus about 20 points.\n\n### **Step #2: Simulate one dataset**\n\nLet us simulate one dataset based on those assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(2608)\n\nn_subjects <- 100\ntreat_effect <- 5\n\ny_if_control <- rnorm(n_subjects, 60, 20)\ny_if_treatment <- y_if_control + treat_effect\n\ndat <- tibble(\n  condition = sample(x = rep(c(\"Control\", \"Treated\"), n_subjects/2), \n                     size = n_subjects, \n                     replace = TRUE),\n  outcome = ifelse(condition == \"Control\", y_if_control, y_if_treatment)\n  )\n```\n:::\n\n\nWe can fit an initial model to our simulated data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_1 <- \n  summary(lm(outcome ~ condition, data = dat))\n\nmodelsummary(models = list(\n  \"Model 1\" = mod_1),\n  fmt = fmt_significant(2),\n  stars = TRUE,\n  title = \"Estimated treatment effect from a randomized controlled trial with 100 subjects, take 1\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"NAborder-bottom: 0; width: auto !important; margin-left: auto; margin-right: auto;\" class=\"table\">\n<caption>Estimated treatment effect from a randomized controlled trial with 100 subjects, take 1</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> Model 1 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 62*** </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (3) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> conditionTreated </td>\n   <td style=\"text-align:center;\"> 1.4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1.5px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\"> (4.1) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 100 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 </td>\n   <td style=\"text-align:center;\"> 0.001 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Adj. </td>\n   <td style=\"text-align:center;\"> −0.009 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RMSE </td>\n   <td style=\"text-align:center;\"> 20.26 </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001</td></tr></tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nLooking at the results, 100 subjects for the experiment should make us worry. For one thing, the estimated treatment effect is much smaller than the true treatment effect, which we set to 5. In addition, looking at the standard error of 4.1, is seems impossible that we could ever hope to detect a treatment effect of 5.\n\nWhat would happen if we were to run the simulation again, only this time setting a different seed?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(3009)\n\ny_if_control <- rnorm(n_subjects, 60, 20)\ny_if_treatment <- y_if_control + treat_effect\n\ndat_2 <- \n  tibble(\n  condition = sample(x = rep(c(\"Control\", \"Treated\"), n_subjects/2), \n                     size = n_subjects, \n                     replace = TRUE),\n  outcome = ifelse(condition == \"Control\", y_if_control, y_if_treatment)\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_2 <- \n  summary(lm(outcome ~ condition, data = dat_2))\n\nmodelsummary(models = list(\n  \"Model 1\" = mod_1,\n  \"Model 2\" = mod_2),\n  fmt = fmt_significant(2),\n  stars = TRUE,\n  title = \"Estimated treatment effect from a randomized controlled trial with 100 subjects, take 2\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"NAborder-bottom: 0; width: auto !important; margin-left: auto; margin-right: auto;\" class=\"table\">\n<caption>Estimated treatment effect from a randomized controlled trial with 100 subjects, take 2</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> Model 1 </th>\n   <th style=\"text-align:center;\">  Model 2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 62*** </td>\n   <td style=\"text-align:center;\"> 59.9*** </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (3) </td>\n   <td style=\"text-align:center;\"> (3.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> conditionTreated </td>\n   <td style=\"text-align:center;\"> 1.4 </td>\n   <td style=\"text-align:center;\"> −1.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1.5px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\"> (4.1) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\"> (4.7) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 100 </td>\n   <td style=\"text-align:center;\"> 100 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 </td>\n   <td style=\"text-align:center;\"> 0.001 </td>\n   <td style=\"text-align:center;\"> 0.001 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Adj. </td>\n   <td style=\"text-align:center;\"> −0.009 </td>\n   <td style=\"text-align:center;\"> −0.010 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RMSE </td>\n   <td style=\"text-align:center;\"> 20.26 </td>\n   <td style=\"text-align:center;\"> 23.50 </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001</td></tr></tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nThis time we get a very different estimate of the treatment effect. Clearly, our the results from a 100 subject experiment is not very trustworthy, given our assumptions.\n\n### Step #3: Turn the simulation into a function and repeat it\n\nRather than keep making new datasets \"by hand\", we can write a function to systematically study the variation across resamples.\n\nTo set up the simulation for replication, we first turn our initial simulation into a function:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsim_my_data <- function(n_subjects = 100, treat_effect = 5) {\n  \n  y_if_control <- rnorm(n_subjects, 60, 20)\n  y_if_treatment <- y_if_control + treat_effect\n  \n  tibble(\n    condition = sample(x = rep(c(\"Control\", \"Treated\"), n_subjects/2), \n                     size = n_subjects, \n                     replace = TRUE),\n    outcome = ifelse(condition == \"Control\", y_if_control, y_if_treatment)\n  )\n}\n```\n:::\n\n\nLet's make sure that our function does what we intend it to do:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_my_data() |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  condition outcome\n  <chr>       <dbl>\n1 Control     107. \n2 Treated      52.2\n3 Treated      84.5\n4 Control      85.5\n5 Control      38.8\n6 Treated      64.5\n```\n:::\n:::\n\n\nLooks right. Now we can use `map()` to repeat our simulation 1,000 times:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmany_sims <- \n  tibble(\n    trial = 1:1000,\n    sim_dat = map(trial, ~sim_my_data())\n  )\n```\n:::\n\n\nAnd instead of fitting the model to just *one* simulated dataset, we can fit *1,000 models* - one for each dataset. This will help us study the variation in the quantities we are interested in:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmany_sims <- \n  many_sims |> \n  mutate(model = map(sim_dat, ~lm(outcome ~ condition, data =.)))\n```\n:::\n\n\nNow we have a list of simulated datasets with a model fit to each.\n\n![1,000 dataatses and 1,000 models](screenshot_sims.png){#fig-sims}\n\nAs a final step, let's make two new list colums:\n\n1.  one using `tidy()` which will store our model estimates, and\n\n2.  one using `glance` which will store information about each model, such as R\\^2 etc.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmany_sims <- \n  many_sims |> \n  mutate(tidied = map(model, tidy),\n         glanced = map(model, glance))\n```\n:::\n\n\nFinally, we can unnest our tidied and glanced list columns so they are easier to work with:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmany_sims_unnested <- \n  many_sims |> \n  unnest(.cols = tidied) |> \n  unnest(.cols = glanced)\n```\n:::\n\n\n### Step #4: Use the simulated datasets to answer questions about the design\n\nWith our datasets and models in place, we can start interrogating the design. For example, what can we say about the **standard error of the treatment**?\n\nOur two initial models suggested that the standard error was about 4, but we can study its variation more systematically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany_sims_unnested |> \n  filter(term == \"conditionTreated\") |> \n  select(\"std.error\") |> \n  ggplot(aes(std.error)) + \n  geom_histogram(fill = \"firebrick\", color = \"white\", bins = 75) +\n  labs(x = \"Standard error\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![Histogram of 1,000 simulated standard errors of the treatment effect when n = 100](index_files/figure-html/fig-stderror-hist-1.png){#fig-stderror-hist width=672}\n:::\n:::\n\n\nAs @fig-stderror-hist reveals, our standard error would range between about 3.5 and 4.5. If that is enough for our purpose, then fine. But being that we expect a treatment effect of about 5, we should not set our hopes up for achieving any \"statistical significance\".\n\nTo get statistical significance, the standard error needs to be less than half the treatment effect, i.e. \\<2.5. To be on the safe side, say we were going for a standard error of 2. In that case, since the standard error drops with the square root of the sample size, we would need about 400 subjects.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Open to see workflow for n = 400\"}\nnew_sims <- \n  tibble(\n    trial = 1:1000,\n    sim_dat = map(trial, ~sim_my_data(n_subjects = 400))\n  ) |> \n  mutate(model = map(sim_dat, ~lm(outcome ~ condition, data =.))) |> \n  mutate(tidied = map(model, tidy)) |> \n  unnest(.cols = tidied)\n\nnew_sims |> \n  filter(term == \"conditionTreated\") |> \n  select(\"std.error\") |> \n  ggplot(aes(std.error)) + \n  geom_histogram(fill = \"firebrick\", color = \"white\", bins = 75) +\n  labs(x = \"Standard error\",\n       y = \"Count\")\n```\n:::\n\n\nAnother quantity we might be interest in is the p-value across simulations. Recall, that we defined a treatment effect of 5, so we *know* there is an effect of five. How likely is it that we would arrive at this result if we define a threshold of *p* \\< 0.05?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany_sims_unnested |> \n  filter(term == \"conditionTreated\") |> \n  summarize(\n    stat_power = mean(p.value < 0.05)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  stat_power\n       <dbl>\n1      0.248\n```\n:::\n:::\n\n\nIn statistical parlance, this quantity is known as the **statistical power**: the likelihood of a hypothesis test detecting a true effect if there is one. With 100 subjects, we get a power of about 25 percent - much too low if our goal is to learn anything. Put another way, we incorrectly reject the true treatment effect about 75 percent of the time.\n\nIn case we wanted to, we could also make a plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany_sims_unnested |> \n  filter(term == \"conditionTreated\") |> \n  select(\"p.value\") |> \n  mutate(signif = ifelse(p.value <= 0.05, \"Significant\", \"Not significant\")) |> \n  ggplot(aes(p.value)) + \n  geom_histogram(aes(fill = signif), \n                 color = \"white\", \n                 bins = 75) +\n  scale_fill_brewer() + \n  labs(x = expression(paste(italic(p), \"-value\")),\n       y = \"Count\",\n       fill = \"Statistical significance\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Simulated p-values when n = 100. There is a true treatment effect of 5. Dark blue marks p-values below the conventional threshold of statistical significance of .05. Only in about 25 percent of the cases would we correctly be able to identify the treatment effect. ](index_files/figure-html/fig-pvalues-1.png){#fig-pvalues width=672}\n:::\n:::\n\n\nLooking at the *p*-values and the treatment effect standard errors gives tells us something about how well we can trust the estimated treatment effect (not much, it turns out).\n\nBut we could also study some properties of the overall model. For example, what is the **coverage** of say, the 68 and 95 percent confidence intervals?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany_sims_unnested <-\n  many_sims_unnested |>\n  mutate(\n    low_95 = estimate - (1.96 * std.error),\n    high_95 = estimate + (1.96 * std.error),\n    low_50 = estimate - (2/3 * std.error),\n    high_50 = estimate + (2/3 * std.error),\n  )\n\nmany_sims_unnested |>\n  filter(term == \"conditionTreated\") |> \n  summarize(\n    coverage_50 = mean(low_50 <= treat_effect & high_50 >= treat_effect),\n    coverage_95 = mean(low_95 <= treat_effect & high_95 >= treat_effect)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  coverage_50 coverage_95\n        <dbl>       <dbl>\n1       0.468       0.965\n```\n:::\n:::\n\n\nThe coverage looks right, which suggest that at least running a linear regression is a good model for our purpose. In our case of a randomized experiment, this is not all that surprising since the model only needs to fit a very basic two-group comparison data structure. Again we could make a plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany_sims_unnested |>\n  filter(term == \"conditionTreated\") |>\n  slice(1:100) |>\n  ggplot(aes(trial)) +\n  geom_hline(yintercept = treat_effect, color = \"firebrick\") +\n  geom_point(aes(y = estimate)) +\n  geom_linerange(aes(ymin = low_95,\n                     ymax = high_95),\n                 color = \"grey50\") +\n  geom_linerange(aes(ymin = low_50,\n                     ymax = high_50),\n                 linewidth = 1) +\n  geom_point(aes(y = estimate)) +\n  labs(\n    x = \"Simulation\",\n    y = \"Treatment effect\"\n  )\n```\n\n::: {.cell-output-display}\n![100 estimates of the treatment effect with 50% and 95% confidence intervals calculated using simulation. If the model is correct, then about 50 percent of the 50%-intervals and 95 percent of the 95%-intervals will contain the true treatment effect (in this case 5)](index_files/figure-html/fig-coverage-1.png){#fig-coverage width=672}\n:::\n:::\n\n\nFinally, let's also have a look at how **R^2^** behaves across simulations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(summary(many_sims_unnested$r.squared), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.02    0.03    0.04    0.19 \n```\n:::\n:::\n\n\nOverall, pretty low (as we should expect). But notice the outliers. Let's make a final plot to summarize our simulations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany_sims_unnested |> \n  ggplot(aes(r.squared)) +\n  geom_histogram(fill = \"lightblue\", \n                 color = \"white\", \n                 bins = 75) +\n  scale_fill_brewer() + \n  labs(x = expression(R^2),\n       y = \"Count\") \n```\n\n::: {.cell-output-display}\n![Distribution of r squared values from 1,000 simulations. By far, most values ](index_files/figure-html/fig-rsquared-1.png){#fig-rsquared width=672}\n:::\n:::\n\n\n### Final thoughts: Using simulation for design analysis\n\nSimulation is an incredibly useful way of studying the properties of of a research design. Once the resamples are made, you can study basically *any* property of the design: power, standard errors, minimum detectable effect size, etc.\n\n### Cool! Where can I learn more?\n\n-   Gelman, Hill, and Vehtari. (2020). *Regression and Other Stories*. Cambridge University Press.\n-   Alexander, R. (2023). *Telling Stories with Data: With Applications in R*. CRC Press.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}