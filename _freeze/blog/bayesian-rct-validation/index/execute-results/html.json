{
  "hash": "f253633ef71f9c815d114dc219280836",
  "result": {
    "markdown": "---\ntitle: \"Bayesian analysis of a randomized controlled trial II: Defining and validating\n  the model\"\nauthor: 'Paw Hansen'\ndate: '2023-07-05'\ndescription: \"Part two of my three-part series on Bayesian analysis of randomized controlled trials. You've built a model but is it any good?\"\nimage: \"featured.jpg\"\ncategories: [Bayesian modeling]\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n\n\nWelcome to the second post in my brief series on getting started with Bayesian modeling in R. In [my last post](https://www.pawhansen.org/blog/bayesian-rct-spec-priors/), we covered specifying the priors to take into account any prior knowledge.\n\nToday, we'll try to *validate* the models we built. As we are working with logistic regression, we'll focus on two questions @johnson2022c:\n\n1. Have our simulation _stabilized_?   \n2. How _wrong_ is the model?\n3. How _accurate_ are the model's posterior **classifications**?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Packages used in this post\"}\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesrules)\nlibrary(tidybayes)\nlibrary(bayesplot)\nlibrary(kableExtra)\nlibrary(patchwork)\n\ntheme_set(theme_minimal(base_size = 12))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nRecall our two models from my previous post:\n\n-   In one model, I used a *weakly informative prior*\n-   In another, model I used an *evidence-based prior*\n\n### Check #1: Have our simulations stabilized? \nBefore moving on, we should check the stability of our simulations.\n\nThis is easy to do using `mcmc_trace` from the `bayesrules` packages. \n\nLet's first check the model using a weakly informative prior: \n\n\n::: {#fig-sim-weak .cell layout-ncol=\"1\"}\n\n```{.r .cell-code}\n# MCMC trace, density, & autocorrelation plots - weakly informative prior \nmcmc_trace(fail_model_weakinf) + scale_x_continuous(breaks = c(0, 5000))\nmcmc_dens_overlay(fail_model_weakinf)\nmcmc_acf(fail_model_weakinf)\n```\n\n::: {.cell-output-display}\n![Traceplot](index_files/figure-html/fig-sim-weak-1.png){#fig-sim-weak-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Density of posterior simulated coefficients](index_files/figure-html/fig-sim-weak-2.png){#fig-sim-weak-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Autocorrelation](index_files/figure-html/fig-sim-weak-3.png){#fig-sim-weak-3 width=672}\n:::\n\nDiagnostic plots for the stability of our simulation results concerning the model using weakly informative priors. All looks good.\n:::\n\n\n\n\nAll looks good. For the evidence based model...\n\n\n::: {#fig-sim-evidence .cell layout-ncol=\"1\"}\n\n```{.r .cell-code}\n# MCMC trace, density, & autocorrelation plots - evidence based model\nmcmc_trace(fail_model_evidence) + scale_x_continuous(breaks = c(0, 5000))\nmcmc_dens_overlay(fail_model_evidence)\nmcmc_acf(fail_model_evidence)\n```\n\n::: {.cell-output-display}\n![Traceplot](index_files/figure-html/fig-sim-evidence-1.png){#fig-sim-evidence-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Density of posterior simulated coefficients](index_files/figure-html/fig-sim-evidence-2.png){#fig-sim-evidence-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Autocorrelation](index_files/figure-html/fig-sim-evidence-3.png){#fig-sim-evidence-3 width=672}\n:::\n\nDiagnostic plots for the stability of our simulation results concerning the model using evidencebased priors. All looks good.\n:::\n\n\nsame.\n\nAll set, let's get on with model validation!\n\n### Check #2: How well does the model fit the data?\n\nTo see this, we simulate 100 data sets from our posterior distribution. For each data set, we then calculate the number of failed tests to see if this matches up with that of the original data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalc_prop_fail <- function(x) {mean(x == 1)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check_model_weakinf <- \n  pp_check(fail_model_weakinf, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_evidence <-\n  pp_check(fail_model_evidence, \n           plotfun = \"stat\", \n           stat = \"calc_prop_fail\",\n           seed = 2307) + \n  xlab(\"Share of failed reading tests\") +\n  xlim(0,1) + \n  theme(legend.position = \"none\")\n\npp_check_model_weakinf / pp_check_model_evidence + plot_annotation(tag_levels = 'A')\n```\n\n::: {.cell-output-display}\n![Posterior predictve checks of the two models. Histograms show the proportion of failed students in a number of simulated datasets based on each model, whereas the dark blue lines mark the real proportion of failed students in the data. (a) With a weakly informative prior, the simulated data sets are unwilling to say much about the share of failed reading tests. (b) In contrast, using an evidence-based prior results in posterior simulations that resemble the original data well.](index_files/figure-html/pp-checks-1.png){width=672}\n:::\n:::\n\n\n### Check #3: How well does the model fit *new* data?\n\nBecause we are working with a categorical outcome, we can be *either* right or wrong. The question is how often are we right?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0407)\n\nclass_sum_weakinf <- \n  classification_summary(model = fail_model_weakinf, data = fake, cutoff = 0.5) \n\nclass_sum_evidence <- \n  classification_summary(model = fail_model_evidence, data = fake, cutoff = 0.5) \n```\n:::\n\n\n-   **overall accuracy** captures the proportion of all Y observations that are accurately classified\n-   **sensitivity (true positive rate)** captures the proportion of Y = 1 observations that are accurately classified\n-   **specificity (true negative rate)** the proportion of Y = 0 observations that are accurately classified:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>How well do the two models fit the data?</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Measure </th>\n   <th style=\"text-align:right;\"> Weakly informative priors </th>\n   <th style=\"text-align:right;\"> Evidence-based priors </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Sensitivity (true positive rate) </td>\n   <td style=\"text-align:right;\"> 0.57 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Specificity (true negative rate) </td>\n   <td style=\"text-align:right;\"> 0.43 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Overall accuracy </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}