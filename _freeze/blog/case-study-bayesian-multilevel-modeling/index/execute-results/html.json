{
  "hash": "1db43f6b1f1be92034a0673bff6d90f5",
  "result": {
    "markdown": "---\ntitle: \"Case study: A Bayesian multilevel analysis of xxx\"\ndescription: \"Playing around with xxx\"\nauthor: \"Paw Hansen\"\ndate: \"10/2/2023\"\nimage: \"featured.jpg\"\ncategories: [statistical analysis]\neditor_options: \n  chunk_output_type: console\ndraft: true\n---\n\n\nI was reading up on (Bayesian multilevel modeling)[https://www.bayesrulesbook.com/chapter-17] the other day, and I thought I would do a quick post showing the whole game of how to apply Bayesian modeling to hierarchical data. Specifically, I'll work my way through the following exercise from _Bayes Rules!_: \n\n> \"Does one’s voice pitch change depending on attitude? To address this question, Winter and Grawunder (2012) conducted a study in which each subject participated in various role-playing dialogs. These dialogs spanned different contexts (e.g., asking for a favor) and were approached with different attitudes (polite vs informal).\"\n\nSounds fun!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Packages used in this post\"}\n# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\ntheme_set(theme_minimal(base_size = 14))\n\n# Load data\ndata(\"voices\", package = \"bayesrules\")\n```\n:::\n\n\n### Exercise 17.11 (Voices: check out some data) \n\nLet's begin by checking out the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(voices)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 84\nColumns: 4\n$ subject  <fct> A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B, B, B…\n$ scenario <fct> A, A, B, B, C, C, D, D, E, E, F, F, G, G, A, A, B, B, C, C, D…\n$ attitude <fct> polite, informal, polite, informal, polite, informal, polite,…\n$ pitch    <dbl> 229.7, 237.3, 236.8, 251.0, 267.0, 266.0, 275.4, 306.8, 232.6…\n```\n:::\n\n```{.r .cell-code}\nvoices |> \n  group_by(subject) |> \n  summarise((n_scenarios = n()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  subject `(n_scenarios = n())`\n  <fct>                   <int>\n1 A                          14\n2 B                          14\n3 C                          14\n4 D                          14\n5 E                          14\n6 F                          14\n```\n:::\n:::\n\n\nSo we have a total of 84 observations, and each subject participated in 14 scenarios (\"dialogues\").  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoices |> \n  ggplot(aes(pitch, subject)) + \n  geom_boxplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nSeems like we have some variation both within and across the five subjects. \n\n### Exercise 17.12 (Voices: simulating the model)\n\nWe'll assume that baseline voice pitch differs from subject to subject, but that the impact of attitude on voice pitch is similar among all subjects. That is, intercepts will vary but slopes will not. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoice_model_1 <- \n  stan_glmer(\n  pitch ~ attitude + (1 | subject), \n  data = voices, family = gaussian,\n  prior_intercept = normal(200, 10, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735, \n  prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000138 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.38 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 5.16542 seconds (Warm-up)\nChain 1:                4.71852 seconds (Sampling)\nChain 1:                9.88394 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.7e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 4.71713 seconds (Warm-up)\nChain 2:                4.09666 seconds (Sampling)\nChain 2:                8.81379 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3.7e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 4.53197 seconds (Warm-up)\nChain 3:                3.75235 seconds (Sampling)\nChain 3:                8.28432 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 4.90927 seconds (Warm-up)\nChain 4:                4.21388 seconds (Sampling)\nChain 4:                9.12315 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# Confirm the prior model specifications\nprior_summary(voice_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'voice_model_1' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 200, scale = 10)\n  Adjusted prior:\n    ~ normal(location = 200, scale = 655)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 326)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.015)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Markov chain diagnostics\nmcmc_trace(voice_model_1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(voice_model_1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(voice_model_1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n```{.r .cell-code}\nneff_ratio(voice_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           (Intercept)                         attitudepolite \n                               0.27240                                0.82930 \n              b[(Intercept) subject:A]               b[(Intercept) subject:B] \n                               0.28080                                0.28745 \n              b[(Intercept) subject:C]               b[(Intercept) subject:D] \n                               0.28435                                0.28210 \n              b[(Intercept) subject:E]               b[(Intercept) subject:F] \n                               0.28300                                0.28020 \n                                 sigma Sigma[subject:(Intercept),(Intercept)] \n                               0.74070                                0.37680 \n```\n:::\n\n```{.r .cell-code}\nrhat(voice_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           (Intercept)                         attitudepolite \n                             1.0004974                              0.9999614 \n              b[(Intercept) subject:A]               b[(Intercept) subject:B] \n                             1.0004769                              1.0006120 \n              b[(Intercept) subject:C]               b[(Intercept) subject:D] \n                             1.0005675                              1.0004639 \n              b[(Intercept) subject:E]               b[(Intercept) subject:F] \n                             1.0004945                              1.0006028 \n                                 sigma Sigma[subject:(Intercept),(Intercept)] \n                             1.0000628                              1.0003043 \n```\n:::\n\n```{.r .cell-code}\n# Overlaid densities \npp_check(voice_model_1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-4.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_summary_1 <- \n  tidy(voice_model_1, effects = \"fixed\",\n                       conf.int = TRUE, conf.level = 0.95)\n\ntidy_summary_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term           estimate std.error conf.low conf.high\n  <chr>             <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)       203.      24.6     149.     258.  \n2 attitudepolite    -19.3      6.46    -32.2     -6.55\n```\n:::\n:::\n\n\n_Interpretation:_ \n\n### Exercise 17.13 (Voices: focusing on the individual) \n\nSee 17.3. \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\n\n# voices %>% \n#   add_linpred_draws(object = voice_model_1)\n```\n:::\n\n\n\nUsing `posterior_predict()`, simulate posterior predictive models of voice pitch in a new polite dialog for three different subjects: A, F, and you. Illustrate your simulation results using mcmc_areas() and discuss your findings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate posterior predictive models for the 3 runners\nset.seed(84735)\npredict_voices <- posterior_predict(\n  voice_model_1, \n  newdata = data.frame(subject = c(\"A\", \"F\", \"Me\"),\n                       attitude = c(\"polite\", \"polite\", \"polite\")))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior predictive model plots\nmcmc_areas(predict_voices, prob = 0.8) +\n  ggplot2::scale_y_discrete(labels = c(\"A\", \"F\", \"Me\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}