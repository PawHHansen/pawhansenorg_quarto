{
  "hash": "35248427d77d86bb50cc4d904293accc",
  "result": {
    "markdown": "---\ntitle: \"We should probably test that: writing tests to varify your data is what you think it is\"\ndescription: \"Writing tests to varify your data is what you think it is\"\nauthor: \"Paw Hansen\"\ndate: \"5/22/2021\"\nimage: \"featured.jpg\"\ncategories: [statistical analysis]\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\nI am working my through the so-far-wonderful book [*Telling stories with data*](https://tellingstorieswithdata.com) by Rohan Alexander. One thing I really like about the book is the heavy emphasis on simulation and all the things you can do with that: present results, validate models, plan research designs, and many, many other common tasks when doing research. \n\nAnother part to like is the discussion on how you should write tests throughout a data science project. You want to make sure your data *is* what you *think* it is. For instance, you might *think* that all your key variables are of the right class - but you should probably test that!\n\nThis is one thing I would have liked to know more about when I started out doing statistical analysis. Back then, I would often run into the problem that my code would not run[^1]. Often, this was due to tiny issues in the data: a variable that should have been a factor was perhaps coded as a character. Or, in the process of joining multiple datasets, I had lost some observations without noticing.\n\n[^1]: That still happens! But at least not as often, and now I know better what to do about it. Also, occasionally my code *would* run - but give me wrong results.\n\nThese 'little things', make up a huge part of what makes data science occasionally frustrating. And they can undermine an otherwise well-thought and thoroughly-carried-out analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Packages used in this post\"}\nlibrary(tidyverse)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 14))\n```\n:::\n\n\n### So what tests should I write?\n\nWriting test sounds harder than it is. Of course, while some tests can be quite long to write and hard to get your head around, most takes only a few seconds to write-and-run, and doing so will save you considerable time. *Trust me*.\n\nRohan recommends the following suite of minimum things to test for:\n\n1.  **Internal validity**: class of the variables, their unique values, and the number of observations.\n2.  **Internal consistency**: if, for example, there is total_score variable, you want to make sure it's actually the sum of its parts\n3.  **External validity**: compare with data form outside. For instance, if you have data on the GDP in Europe in your dataset, make sure that those values match those of, say, what you can get from the World Bank or other trusworthy sources.\n\nInternal consistency and external validity are quite project-specific, so I'll focus on internal validity.\n\nFollowing Alexander's advice, you should pay attention to the following:\n\n-   boundary conditions,\n-   classes,\n-   missing data,\n-   the number of observations and variables,\n-   duplicates, and\n-   regression results.\n\nTwo quick ways of testing are (1) make a graph and (2) look at counts. A final and (3) is to write out formal test, resulting in a true/false. Importantly, you don't have run a whole bunch of tests. Write a few, perhaps targeting specific areas of concern.\n\n### Examples \nLet's have look at a few examples. Say we intended to do a survey on adult people's preferences for desert. Specifically, we want to know if people prefer ice cream or pancakes, and how these preferences relate to respondents' age and sex. \n\nOne cool aspect about testing is that we can simulate data ahead of running the survey, write tests for the simulated data - and then use those exact same tests for when we have acquired the actual data!  \n\nLet's simulate some data, which will contain several problems: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2208)\n\nn_people <- 1000\n\nsim_dat <- \n  tibble(\n    age = runif(n_people, 16, 75) |> round(),\n    sex = sample(c(\"male\", \"female\", NA), \n                 n_people, \n                 replace = T,\n                 prob = c(.4, .4, .2)),\n    preference = sample(c(\"ice cream\", \"pancakes\", \"broccoli\"), \n                     n_people, \n                     replace = T,\n                     prob = c(.49, .49, .2))\n  )\n```\n:::\n\n\n*Boundary conditions*. Working our way through the list of attention points from above, let's begin by testing the boundary conditions. We have one numeric variable, age. Say, we only care about people between 18 and 90 years. We could make a basic histogram:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_dat |> \n  ggplot(aes(age)) + \n  geom_histogram(binwidth = 1, \n                 color = \"white\", \n                 fill = \"lightblue\") +\n  geom_vline(xintercept = 18, color = \"firebrick\", lty = \"dashed\") + \n  geom_vline(xintercept = 90, color = \"firebrick\", lty = \"dashed\") + \n   labs(\n    x = \"Age of respondent\",\n    y = \"Number of respondents\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe histogram reveals that we have people in our sample who a under 18 and perhaps should be excluded. Another way of doing the same test but using code could be: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_c(mean(sim_dat$age >= 18 & sim_dat$age< 90) * 100, \n      \" percent of observations passes the test\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"97.4 percent of observations passes the test\"\n```\n:::\n:::\n\n\nFor the two categorical variables, sex and preference, we could do a basic count:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_dat |> \n  count(sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  sex        n\n  <chr>  <int>\n1 female   387\n2 male     408\n3 <NA>     205\n```\n:::\n\n```{.r .cell-code}\nsim_dat |> \n  count(preference)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  preference     n\n  <chr>      <int>\n1 broccoli     180\n2 ice cream    414\n3 pancakes     406\n```\n:::\n:::\n\n\nFor sex, we have several missing values that we would have to consider. Again, we could write a test to tell exactly how many respondents had missing values on the sex variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_c(mean(is.na(sim_dat$sex)) * 100, \" percent of respondents have missing values on the sex variable\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"20.5 percent of respondents have missing values on the sex variable\"\n```\n:::\n:::\n\n\nWe could also just ask, if the variable had no missing values and hence could pass a true/false test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(is.na(sim_dat$sex)) == 0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\nFor the outcome variable, several respondents prefer broccoli, which wasn't one of the two options we intended to study. This should warn us that the survey was set up wrong[^2]. \n\n[^2]: In a real-life study, this type of error should probably make us consider if our survey had been implemented in such a flawed way that we could not use the data at all.  \n\n*Classes*. A quick and informal way of looking at the class is using `glimpse()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_dat |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,000\nColumns: 3\n$ age        <dbl> 50, 36, 49, 39, 67, 18, 20, 70, 35, 31, 60, 17, 28, 60, 48,…\n$ sex        <chr> \"female\", \"female\", \"male\", \"male\", \"female\", \"female\", NA,…\n$ preference <chr> \"ice cream\", \"ice cream\", \"pancakes\", \"ice cream\", \"pancake…\n```\n:::\n:::\n\n\nBoth sex and preference are coded as characters but they should probably be factors. We can easily change that, though. Let's also clean up the other issues: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_dat_cleaned <- \n  sim_dat |> \n  filter(age >= 18) |> # drop respondents under 18 \n  drop_na(sex) |>  # drop respondents with NAs on the sex variable - requires some consideration\n  filter(preference %in% c(\"ice cream\", \"pancakes\")) |> \n  mutate(sex = as.factor(sex),\n         preference = as.factor(preference))\n```\n:::\n\n\nAnd now we have cleaned dataset, which would pass our tests. Just one example: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_dat_cleaned |> \n  ggplot(aes(age)) + \n  geom_histogram(binwidth = 1, \n                 color = \"white\", \n                 fill = \"lightblue\") +\n  geom_vline(xintercept = 18, color = \"firebrick\", lty = \"dashed\") + \n  geom_vline(xintercept = 90, color = \"firebrick\", lty = \"dashed\") + \n   labs(\n    x = \"Age of respondent\",\n    y = \"Number of respondents\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n### Final thought: writing tests\n\nWriting test prior to data analysis can save you time spent debugging code and also increase confidence in your results because your model is built with the data you think it is. In this post, I have mentioned three basic ways of testing your data: graphing, counting, and writing formal tests. Now, go on and test your data!  \n\n### Cool! Where can I learn more?\n\n-   Alexander, R. (2023). *Telling Stories with Data: With Applications in R*. CRC Press.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}